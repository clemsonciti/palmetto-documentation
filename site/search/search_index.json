{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Covid-19 update In response to recent events and effective Monday, March 16, the CITI ACDS team is now delivering Palmetto office-hours, consultations, and training online via web-conferencing . In particular, the weekly Wednesday office-hours and Friday on-boarding will be hosted online via Zoom. Furthermore, all training will also be delivered online through Zoom. These changes will remain in place until the university provides further direction. Please follow our guidelines outlined below: Sign-in to https://clemson.zoom.us and use your Clemson username and password to activate your account. This only needs to be done once to activate your Zoom account. You will receive an email from Zoom that confirms your registration and contains the activation link. If you do not receive an email within an hour, please notify us at ithelp@clemson.edu . Once you have access to Zoom, please use the following link to go to our Zoom channel and register for the date/time that works for you: Wednesday Office-hours 8.30am \u2013 11.30am: http://bit.ly/Palmetto_Wed_office_hour Friday Onboarding 9am-10am: http://bit.ly/Palmetto_Fri_onboarding Research Support Office Hours (in-person support) Palmetto Office Hours are held every Wednesday 8-11 AM, in 412 Cooper Library . Just drop by - no appointment necessary. Email Support Email ithelp@clemson.edu and include the word \"Palmetto\" in the subject line. The Palmetto staff will automatically receive your message. Please provide us with as much information as possible, including: The exact commands that you run immediately after logging in, which will reproduce the problem The exact output from each command The locations of the files/folders involved, especially your PBS batch script Other details which may be relevant (OS, SSH client, etc.,) Information for New Users Obtaining an account An account is required to access and use the Palmetto cluster. All Clemson University students, faculty and staff can request an account following the procedure detailed below: Access the new ITHelp Portal at https://clemson.cherwellondemand.com/CherwellPortal After Logging in Select \"Browse Our Services\" Select Research > Research Computing > High Perf Computing and Storage Under High Perf Computing and Storage, select \"Request Account\" Enter the information requested. Once the form is complete, press the \"Submit\" button. Account requests are generally processed in a few hours, but may take up to a few days. On-boarding sessions For new users of Palmetto, we offer on-boarding sessions every Friday 9-10 AM, at Barre Hall 2108 . Please drop by to discuss how Palmetto can help you realize your research goals, and get help with getting started on Palmetto cluster.","title":"Home"},{"location":"#covid-19-update","text":"In response to recent events and effective Monday, March 16, the CITI ACDS team is now delivering Palmetto office-hours, consultations, and training online via web-conferencing . In particular, the weekly Wednesday office-hours and Friday on-boarding will be hosted online via Zoom. Furthermore, all training will also be delivered online through Zoom. These changes will remain in place until the university provides further direction. Please follow our guidelines outlined below: Sign-in to https://clemson.zoom.us and use your Clemson username and password to activate your account. This only needs to be done once to activate your Zoom account. You will receive an email from Zoom that confirms your registration and contains the activation link. If you do not receive an email within an hour, please notify us at ithelp@clemson.edu . Once you have access to Zoom, please use the following link to go to our Zoom channel and register for the date/time that works for you: Wednesday Office-hours 8.30am \u2013 11.30am: http://bit.ly/Palmetto_Wed_office_hour Friday Onboarding 9am-10am: http://bit.ly/Palmetto_Fri_onboarding","title":"Covid-19 update"},{"location":"#research-support","text":"","title":"Research Support"},{"location":"#office-hours-in-person-support","text":"Palmetto Office Hours are held every Wednesday 8-11 AM, in 412 Cooper Library . Just drop by - no appointment necessary.","title":"Office Hours (in-person support)"},{"location":"#email-support","text":"Email ithelp@clemson.edu and include the word \"Palmetto\" in the subject line. The Palmetto staff will automatically receive your message. Please provide us with as much information as possible, including: The exact commands that you run immediately after logging in, which will reproduce the problem The exact output from each command The locations of the files/folders involved, especially your PBS batch script Other details which may be relevant (OS, SSH client, etc.,)","title":"Email Support"},{"location":"#information-for-new-users","text":"","title":"Information for New Users"},{"location":"#obtaining-an-account","text":"An account is required to access and use the Palmetto cluster. All Clemson University students, faculty and staff can request an account following the procedure detailed below: Access the new ITHelp Portal at https://clemson.cherwellondemand.com/CherwellPortal After Logging in Select \"Browse Our Services\" Select Research > Research Computing > High Perf Computing and Storage Under High Perf Computing and Storage, select \"Request Account\" Enter the information requested. Once the form is complete, press the \"Submit\" button. Account requests are generally processed in a few hours, but may take up to a few days.","title":"Obtaining an account"},{"location":"#on-boarding-sessions","text":"For new users of Palmetto, we offer on-boarding sessions every Friday 9-10 AM, at Barre Hall 2108 . Please drop by to discuss how Palmetto can help you realize your research goals, and get help with getting started on Palmetto cluster.","title":"On-boarding sessions"},{"location":"about/","text":"Overview Palmetto is Clemson University's primary high-performance computing (HPC) resource; heavily utilized by researchers, students, faculty, and staff from a broad range of disciplines. Currently, Palmetto is comprised of 2021 compute nodes (totalling 23072 CPU cores), and features: 2079 compute nodes, totalling 28832 cores 595 nodes are equipped with 2x NVIDIA Tesla GPUs, with a total of 1194 GPUs in the cluster; 103 nodes each have 2x NVIDIA Tesla V100 GPUs 4 nodes with Intel Phi co-processors (2 per node) 17 large-memory nodes (with 0.5 TB - 2.0 TB of memory); in addition, 480 nodes have at least 128 GB of RAM 100 GB of personal space (backed up daily for 42 days) for each user 726 TB of scratch storage space for computing and a burst-buffer 10 and 25 Gbps Ethernet, and 56 and 100 Gbps Infiniband networks ranked 9th among the public academic institutions in the US (and 392nd overall among all world-wide supercomputers) on Top500 list, as of November 2019 benchmarked at 1.4 PFlops (44,016 cores from Infiniband part of Palmetto) the cluster is 100% battery-backed Compute Hardware The cluster is divided into several \"phases\"; the basic hardware configuration (node count, cores, RAM) is given below. For more detailed and up-to-date information, you can view the file /etc/hardware-table after logging in. Ethernet phases Phases 0 through 6 of the cluster consist of older hardware with 10 Gbps Ethernet interconnect. Maximum run time for a single task is limited to 168 hours. Phase Node count Cores RAM 1a 116 8 31 GB 1b 40 12 92 GB 2a 30 16 382 GB 2b 162 8 15 GB 3 224 8 15 GB 4 319 8 15 GB 5a 309 8 31 GB 5b 9 8 31 GB 5c 32 8 22 GB 5d 23 12 46 GB 6 65 24 46 GB Infiniband phases Phases 7-19 of the cluster consist of newer hardware with 56 Gbps Infiniband interconnect (Phases 7-17) and 100 Gbps Infiniband interconnect (Phases 18-19). Maximum run time for a single task is limited to 72 hours. Phase Node count Cores RAM 7a 42 16 62 GB 7b 12 16 62 GB 8a 71 16 62 GB 8b 57 16 62 GB 8c 88 16 62 GB 9 72 16 126 GB 10 80 20 126 GB 11a 40 20 126 GB 11b 4 20 126 GB 12 30 24 126 GB 13 24 24 126 GB 14 12 24 126 GB 15 32 24 126 GB 16 40 28 126 GB 17 20 28 126 GB 18a 2 40 372 GB 18b 65 40 372 GB 18c 10 40 748 GB 19a 28 40 372 GB 19b 4 48 372 GB GPUs There are 595 nodes (phases 7a-8b, 9-11a, 12-19a) on Palmetto equipped with NVIDIA Tesla GPUs (M2075, K20m, M2070q, K40m, P100, V100). Intel Xeon Phi accelerators 4 nodes (phase 11b) are equipped with Intel Phi co-processors (2 per node). Big-memory nodes Phase 0 consists of 6 \"bigmem\" machines with large core count and RAM (505 GB to 2 TB). Phase Node count Cores RAM 0 6 24 505 GB 0 1 64 2 TB 0 5 32 750 GB 0 1 40 1.5 TB 0 1 40 1 TB 0 3 80 1.5 TB Storage Various options for storing data (on temporary and permanent basis) are available to researchers using Palmetto: Location Available space Notes /home 100 GB per user Backed-up nightly, permanent storage space accessible from all nodes /scratch1 233 TB shared by all users Not backed up, temporary work space accessible from all nodes, BeeGFS Parallel File Sytem /scratch2 160 TB shared by all users Not backed up, temporary work space accessible from all nodes, XFS /scratch3 129 TB shared by all users Not backed up, temporary work space accessible from all nodes, ZFS /scratch4 174 TB shared by all users Not backed up, temporary work space accessible from all nodes, BeeGFS Parallel File Sytem /local_scratch Varies between nodes (99GB-800GB) Per-node temporary work space, accessible only for the lifetime of job Additional high-performance and backed-up storage may be purchased for group usage. Please visit http://citi.clemson.edu/infrastructure for details and pricing. Job scheduling The Palmetto cluster uses the Portable Batch Scheduling system (PBS) to schedule jobs. Condominium model Palmetto cluster operates in a condominium model which allows faculty to purchase immediate access to compute nodes on the cluster. More information can be found in the Owner's Guide . Acknowledging Palmetto Cluster We would appreciate if all publications that include results generated using the Palmetto cluster include a short statement in the Acknowledgment section. As an example, the acknowledgment may look like this: Clemson University is acknowledged for generous allotment of compute time on Palmetto cluster.","title":"About"},{"location":"about/#overview","text":"Palmetto is Clemson University's primary high-performance computing (HPC) resource; heavily utilized by researchers, students, faculty, and staff from a broad range of disciplines. Currently, Palmetto is comprised of 2021 compute nodes (totalling 23072 CPU cores), and features: 2079 compute nodes, totalling 28832 cores 595 nodes are equipped with 2x NVIDIA Tesla GPUs, with a total of 1194 GPUs in the cluster; 103 nodes each have 2x NVIDIA Tesla V100 GPUs 4 nodes with Intel Phi co-processors (2 per node) 17 large-memory nodes (with 0.5 TB - 2.0 TB of memory); in addition, 480 nodes have at least 128 GB of RAM 100 GB of personal space (backed up daily for 42 days) for each user 726 TB of scratch storage space for computing and a burst-buffer 10 and 25 Gbps Ethernet, and 56 and 100 Gbps Infiniband networks ranked 9th among the public academic institutions in the US (and 392nd overall among all world-wide supercomputers) on Top500 list, as of November 2019 benchmarked at 1.4 PFlops (44,016 cores from Infiniband part of Palmetto) the cluster is 100% battery-backed","title":"Overview"},{"location":"about/#compute-hardware","text":"The cluster is divided into several \"phases\"; the basic hardware configuration (node count, cores, RAM) is given below. For more detailed and up-to-date information, you can view the file /etc/hardware-table after logging in.","title":"Compute Hardware"},{"location":"about/#ethernet-phases","text":"Phases 0 through 6 of the cluster consist of older hardware with 10 Gbps Ethernet interconnect. Maximum run time for a single task is limited to 168 hours. Phase Node count Cores RAM 1a 116 8 31 GB 1b 40 12 92 GB 2a 30 16 382 GB 2b 162 8 15 GB 3 224 8 15 GB 4 319 8 15 GB 5a 309 8 31 GB 5b 9 8 31 GB 5c 32 8 22 GB 5d 23 12 46 GB 6 65 24 46 GB","title":"Ethernet phases"},{"location":"about/#infiniband-phases","text":"Phases 7-19 of the cluster consist of newer hardware with 56 Gbps Infiniband interconnect (Phases 7-17) and 100 Gbps Infiniband interconnect (Phases 18-19). Maximum run time for a single task is limited to 72 hours. Phase Node count Cores RAM 7a 42 16 62 GB 7b 12 16 62 GB 8a 71 16 62 GB 8b 57 16 62 GB 8c 88 16 62 GB 9 72 16 126 GB 10 80 20 126 GB 11a 40 20 126 GB 11b 4 20 126 GB 12 30 24 126 GB 13 24 24 126 GB 14 12 24 126 GB 15 32 24 126 GB 16 40 28 126 GB 17 20 28 126 GB 18a 2 40 372 GB 18b 65 40 372 GB 18c 10 40 748 GB 19a 28 40 372 GB 19b 4 48 372 GB","title":"Infiniband phases"},{"location":"about/#gpus","text":"There are 595 nodes (phases 7a-8b, 9-11a, 12-19a) on Palmetto equipped with NVIDIA Tesla GPUs (M2075, K20m, M2070q, K40m, P100, V100).","title":"GPUs"},{"location":"about/#intel-xeon-phi-accelerators","text":"4 nodes (phase 11b) are equipped with Intel Phi co-processors (2 per node).","title":"Intel Xeon Phi accelerators"},{"location":"about/#big-memory-nodes","text":"Phase 0 consists of 6 \"bigmem\" machines with large core count and RAM (505 GB to 2 TB). Phase Node count Cores RAM 0 6 24 505 GB 0 1 64 2 TB 0 5 32 750 GB 0 1 40 1.5 TB 0 1 40 1 TB 0 3 80 1.5 TB","title":"Big-memory nodes"},{"location":"about/#storage","text":"Various options for storing data (on temporary and permanent basis) are available to researchers using Palmetto: Location Available space Notes /home 100 GB per user Backed-up nightly, permanent storage space accessible from all nodes /scratch1 233 TB shared by all users Not backed up, temporary work space accessible from all nodes, BeeGFS Parallel File Sytem /scratch2 160 TB shared by all users Not backed up, temporary work space accessible from all nodes, XFS /scratch3 129 TB shared by all users Not backed up, temporary work space accessible from all nodes, ZFS /scratch4 174 TB shared by all users Not backed up, temporary work space accessible from all nodes, BeeGFS Parallel File Sytem /local_scratch Varies between nodes (99GB-800GB) Per-node temporary work space, accessible only for the lifetime of job Additional high-performance and backed-up storage may be purchased for group usage. Please visit http://citi.clemson.edu/infrastructure for details and pricing.","title":"Storage"},{"location":"about/#job-scheduling","text":"The Palmetto cluster uses the Portable Batch Scheduling system (PBS) to schedule jobs.","title":"Job scheduling"},{"location":"about/#condominium-model","text":"Palmetto cluster operates in a condominium model which allows faculty to purchase immediate access to compute nodes on the cluster. More information can be found in the Owner's Guide .","title":"Condominium model"},{"location":"about/#acknowledging-palmetto-cluster","text":"We would appreciate if all publications that include results generated using the Palmetto cluster include a short statement in the Acknowledgment section. As an example, the acknowledgment may look like this: Clemson University is acknowledged for generous allotment of compute time on Palmetto cluster.","title":"Acknowledging Palmetto Cluster"},{"location":"basic/","text":"Logging in Any user with a Palmetto Cluster account can log-in using SSH (Secure Shell) . Mac OS X and Linux systems come with an SSH client installed, while Windows users will need to download one. Two-Factor Authentication (2FA) All connections to Palmetto require 2FA. If you are not enrolled in 2FA yet, you may enroll using the link https://2fa.clemson.edu/ . 2FA comes with three options for registerd devices (smart phone or tablet): Using keyboard-interactive authentication. Duo two-factor login for $user Enter a passcode or select one of the following options: 1. Duo Push to XXX-XXX-XXXX 2. Phone call to XXX-XXX-XXXX 3. SMS passcodes to XXX-XXX-XXXX Passcode or option (1-3): Option 1: response to Duo Push to your device by clicking Approve Option 2: listen to the automatic call from system and select any key on your device. Option 3: You will receive 10 different passcodes sent to your device. Enter any of the passcode to the command prompt. Option 4: If user connecting with DUO token device purchased from CCIT, use the token generated passcode Passcode or option (1-3): 1234567 More information can be found at here Mac OS X and Linux users Mac OS X or Linux users may open a Terminal, and type in the following command: $ ssh username@login.palmetto.clemson.edu where username is your Clemson user ID. You will be prompted for both your password and DUO authentication. Windows MobaXterm is the recommended SSH client for Windows and can be downloaded here . This software is recommended because it is free and comes with: A built-in file transfer client, which allows you to exchange files and folders between your own computer and Palmetto An X11 server which allows you to run graphical programs on Palmetto cluster A graphical port-forwarding interface to support easy access to web-based programs launched inside Palmetto After downloading and installing MobaXterm, users can log-in by following these steps: Launch the MobaXterm program On the top-left corner of MobaXterm, click the Session button. Select the SSH setting and confirm that the following settings are set: Parameter Value Remote host login.palmetto.clemson.edu Port 22 X11-Forwarding enabled Compression enabled Remote environment Interactive shell SSH-browser type SCP (enhanced speed) Click OK and a new session window will be opened, where you will be prompted for your Palmetto password and the DUO authentication. After being authenticated, you will login to the login001 node. All settings for this session are saved, and for future logins, you can select this session from the Recent sessions form of the main MobaXterm window as well as the Saved sessions tab of the side window. The side window can be displayed or hidden by clicking on the blue double-arrow sign on the top left of MobaXterm. MobaXterm also comes with a built-in file browser and transfer GUI (SSH-browser). This GUI is accessible via the SCP tab of the side window. Using the Upload (green arrow pointing up) and and Download (blue arrow pointing down) buttons at the top of the SCP tab, you can easily transfer files between Palmetto and your local computer. Basic tasks Storing files and folders Home and scratch directories Various filesystems are available for users to store data. These differ in capacity, data-persistence, and efficiency, and it is important that users understand which filesystem to use under which circumstances. Location Available space Notes /home/username 100 GB per user Backed-up nightly, permanent storage space accessible from all nodes /scratch1/username 233 TB shared by all users Not backed up, temporary work space accessible from all nodes, OrangeFS Parallel File System /scratch2/username 160 TB shared by all users Not backed up, temporary work space accessible from all nodes, XFS /scratch3/username 129 TB shared by all users Not backed up, temporary work space accessible from all nodes, ZFS /scratch4/username 175 TB shared by all users Not backed up, temporary work space accessible from all nodes, BeeGFS Parallel File System /local_scratch Varies between nodes (99GB-800GB) Per-node temporary work space, accessible only for the lifetime of job The /home and /scratch directories are shared by all nodes. In contrast, each node has its own /local_scratch directory. All data in the /home directory is permanent (not automatically deleted) and backed-up on a nightly basis. If you lose data in the /home directory, it may be possible to recover it if it was previously backed up. Data in the /scratch directories is not backed up, and any data that is untouched for 30 days is automatically removed from the /scratch directories. Data that cannot easily be reproduced should not be stored in the /scratch directories, and any data that is not required should be removed as soon as possible. See this guide on how to choose the appropriate filesystem for your work. Temporary reservations All users may apply for temporary reservation of the following resources: Up to 150 TB of long-term storage Up to 8.5 TB of fast SSD scratch space All requests will be reviewed by Clemson University Computational Advisory Team (CU-CAT). Reservation requests can be made here . Purchased storage Users or groups may also purchase storage in 1 TB increments. For details about purchased storage, please contact the Palmetto support staff ( ithelp@clemson.edu , and include the word \"Palmetto\" in the subject line). Moving data in and out of the cluster Small files (kilobytes or a few megabytes) On Windows machines, using the MobaXterm SSH client, the built-in file browser can be used ( SCP tab of the side window). Using the Upload (green arrow pointing up) and and Dowload (blue arrow pointing down) buttons at the top of the SCP tab, you can easily transfer small files between Palmetto and your local computer. On Unix systems, you can use the scp (secure copy) command to perform file transfers. The general form of the scp command is: $ scp <path_to_source> username@xfer01-ext.palmetto.clemson.edu:<path_to_destination> For example, here is the scp command to copy a file from the current directory on your local machine to your /home/username directory on Palmetto (this command is entered into a terminal when not logged-in to Palmetto ): $ scp myfile.txt username@xfer01-ext.palmetto.clemson.edu:/home/username ... and to do the same in reverse, i.e., copy from Palmetto to your local machine. (again, from a terminal running on your local machine, not on Palmetto): $ scp username@xfer01-ext.palmetto.clemson.edu:/home/username/myfile.txt . The . represents the working directory on the local machine. For folders, include the -r switch: $ scp -r myfolder username@xfer01-ext.palmetto.clemson.edu:/home/username Transfering larger files (more than a few megabytes) For larger files, we recommend using the Globus file transfer application. Here, we demonstrate how to use Globus Online to transfer files between Palmetto and a local machine (laptop). However, Globus can be used for file transfers to/from other locations as well. You will need to have a Globus account set up to begin. Visit https://www.globus.org/ and set up a Globus account. To begin transfering files, navigate to the Globus Online transfer utility here: https://www.globus.org/app/transfer . The transfer utility allows you to transfer files between \"endpoints\". You will need to set your local machine as a Globus Connect Personal Endpoint for the file transfer. As a part of this step, you must install the Globus Connect Personal application (see here: https://www.globus.org/app/endpoints/create-gcp ). After installing, ensure that the application is running. You should then be able to set your local machine as one endpoint. In the figure below, the endpoint is named My Personal Mac . As the second endpoint, choose clemson#xfer01-ext.clemson.edu . You can now transfer files between any locations on your local machine and the Palmetto cluster. Checking available compute hardware The login node login001 (the node that users first log-in to), is not meant for running computationally intensive tasks. Instead, users must reserve hardware from the compute nodes of the cluster. Currently, Palmetto cluster has over 2020 compute nodes. The hardware configuration of the different nodes is available in the file /etc/hardware-table : [atrikut@login001 ~]$ cat /etc/hardware-table PALMETTO HARDWARE TABLE Last updated: Mar 04 2019 PHASE COUNT MAKE MODEL CHIP(0) CORES RAM(1) /local_scratch Interconnect GPUs PHIs SSD 0 6 HP DL580 Intel Xeon 7542 24 505 GB(2) 99 GB 1ge 0 0 0 0 1 HP DL980 Intel Xeon 7560 64 2 TB(2) 99 GB 1ge 0 0 0 0 1 HP DL560 Intel Xeon E5-4627v4 40 1.5 TB(2) 881 GB 10ge 0 0 0 0 1 Dell R830 Intel Xeon E5-4627v4 40 1.0 TB(2) 880 GB 10ge 0 0 0 0 2 HP DL560 Intel Xeon 6138G 80 1.5 TB(2) 3.6 TB 10ge 0 0 0 1 75 Dell PE1950 Intel Xeon E5345 8 12 GB 37 GB 1g 0 0 0 2a 158 Dell PE1950 Intel Xeon E5410 8 12 GB 37 GB 1g 0 0 0 2b 84 Dell PE1950 Intel Xeon E5410 8 16 GB 37 GB 1g 0 0 0 3 225 Sun X2200 AMD Opteron 2356 8 16 GB 193 GB 1g 0 0 0 4 326 IBM DX340 Intel Xeon E5410 8 16 GB 111 GB 1g 0 0 0 5a 320 Sun X6250 Intel Xeon L5420 8 32 GB 31 GB 1g 0 0 0 5b 9 Sun X4150 Intel Xeon E5410 8 32 GB 99 GB 1g 0 0 0 6 67 HP DL165 AMD Opteron 6176 24 48 GB 193 GB 1g 0 0 0 7a 42 HP SL230 Intel Xeon E5-2665 16 64 GB 240 GB 56g, fdr 0 0 0 7b 12 HP SL250s Intel Xeon E5-2665 16 64 GB 240 GB 56g, fdr 2(3) 0 0 8a 71 HP SL250s Intel Xeon E5-2665 16 64 GB 900 GB 56g, fdr 2(4) 0 300 GB(7) 8b 57 HP SL250s Intel Xeon E5-2665 16 64 GB 420 GB 56g, fdr 2(4) 0 0 8c 88 Dell PEC6220 Intel Xeon E5-2665 16 64 GB 350 GB 10ge 0 0 0 9 72 HP SL250s Intel Xeon E5-2665 16 128 GB 420 GB 56g, fdr, 10ge 2(4) 0 0 10 80 HP SL250s Intel Xeon E5-2670v2 20 128 GB 800 GB 56g, fdr, 10ge 2(4) 0 0 11a 40 HP SL250s Intel Xeon E5-2670v2 20 128 GB 800 GB 56g, fdr, 10ge 2(6) 0 0 11b 4 HP SL250s Intel Xeon E5-2670v2 20 128 GB 800 GB 56g, fdr, 10ge 0 2(8) 0 12 30 Lenovo NX360M5 Intel Xeon E5-2680v3 24 128 GB 800 GB 56g, fdr, 10ge 2(6) 0 0 13 24 Dell C4130 Intel Xeon E5-2680v3 24 128 GB 1.8 TB 56g, fdr, 10ge 2(6) 0 0 14 12 HP XL1X0R Intel Xeon E5-2680v3 24 128 GB 880 GB 56g, fdr, 10ge 2(6) 0 0 15 32 Dell C4130 Intel Xeon E5-2680v3 24 128 GB 1.8 TB 56g, fdr, 10ge 2(6) 0 0 16 40 Dell C4130 Intel Xeon E5-2680v4 28 128 GB 1.8 TB 56g, fdr, 10ge 2(9) 0 0 17 20 Dell C4130 Intel Xeon E5-2680v4 28 128 GB 1.8 TB 56g, fdr, 10ge 2(9) 0 0 18a 2 Dell C4140 Intel Xeon 6148G 40 372 GB 1.9 TB(12) 56g, fdr, 25ge 4(10) 0 0 18b 65 Dell R740 Intel Xeon 6148G 40 372 GB 1.8 TB 56g, fdr, 25ge 2(11) 0 0 18c 10 Dell R740 Intel Xeon 6148G 40 748 GB 1.8 TB 56g, fdr, 25ge 2(11) 0 0 *** PBS resource requests are always lowercase *** (0) CHIP has 3 resources: chip_manufacturer, chip_model, chip_type (1) Leave 2 or 3GB for the operating system when requesting memory in PBS jobs (2) Specify queue \"bigmem\" to access the large memory machines, only ncpus and mem are valid PBS resource requests (3) 2 NVIDIA Tesla M2075 cards per node, use resource request \"ngpus=[1|2]\" and \"gpu_model=m2075\" (4) 2 NVIDIA Tesla K20m cards per node, use resource request \"ngpus=[1|2]\" and \"gpu_model=k20\" (5) 2 NVIDIA Tesla M2070-Q cards per node, use resource request \"ngpus=[1|2]\" and \"gpu_model=m2070q\" (6) 2 NVIDIA Tesla K40m cards per node, use resource request \"ngpus=[1|2]\" and \"gpu_model=k40\" (7) Use resource request \"ssd=true\" to request a chunk with SSD in location /ssd1, /ssd2, and /ssd3 (100GB max each) (8) Use resource request \"nphis=[1|2]\" to request phi nodes, the model is Xeon 7120p (9) 2 NVIDIA Tesla P100 cards per node, use resource request \"ngpus=[1|2]\" and \"gpu_model=p100\" (10)4 NVIDIA Tesla V100 cards per node with NVLINK2, use resource request \"ngpus=[1|2|3|4]\" and \"gpu_model=v100nv\" (11)2 NVIDIA Tesla V100 cards per node, use resource request \"ngpus=[1|2]\" and \"gpu_model=v100\" (12)Phase18a nodes contain only NVMe storage for local_scratch. The compute nodes are divided into \"phases\" (currently phases 0-18). Each phase is composed of several nodes with identical configuration, e.g., each node in phase 5a has 8 cores, 32 GB ram, 31 GB local disk space, and 10 Gbps Myrinet interconnect. A useful command on the login node is whatsfree , which gives information about how many nodes from each phase are currently in use, free, or offlined for maintenance. Later sections of this guide will describe how to submit jobs to the cluster, i.e., reserve compute nodes for running computational tasks. Checking and using available software The Palmetto cluster provides a limited number of packages (including site-licensed packages), that can be used by all Palmetto users. These packages are available as modules , and must be activated/deactivated using the module command: Command Purpose module avail List all packages available (on current system) module add package/version Add a package to your current shell environment module list List packages you have loaded module rm package/version Remove a currently loaded package module purge Remove all currently loaded packages For example, to load the GCC (v4.8.1), CUDA Toolkit (v6.5.14) and OpenMPI (v1.8.4) modules, you can use the command: $ module add gcc/4.8.1 cuda-toolkit/6.5.14 openmpi/1.8.4 Then, check the version of gcc : $ gcc --version gcc (GCC) 4.8.1 Copyright (C) 2013 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Some modules when loaded, implicitly load other modules as well. If you use some modules to compile/install some software, then you will probably have to load them when running that software as well, otherwise you may see errors about missing libraries/headers. Modules do not remain loaded when you log out and log back in, i.e., they are active only for the current session - so you will need to load them for every session. As an exercise, examine the environment variables PATH, LIBRARY_PATH, etc., before and after loading some module: $ echo $PATH $ module add python/2.7.13 $ echo $PATH You can also look at the modulefiles in /software/modulefiles to understand what happens when you add a module. Start an interactive job An interactive job can be started using the qsub command. Here is an example of an interactive job: [username@login001 ~]$ qsub -I -l select=1:ncpus=2:mem=4gb,walltime=4:00:00 qsub (Warning): Interactive jobs will be treated as not rerunnable qsub: waiting for job 8730.pbs02 to start qsub: job 8730.pbs02 ready [username@node0021 ~]$ module add python/3.4 [username@node0021 ~]$ python runsim.py . . . [username@node0021 ~]$ exit [username@login001 ~]$ Above, we request an interactive job using 1 \"chunk\" of hardware ( select=1 ), 2 CPU cores per \"chunk\", and 4gb of RAM per \"chunk\", for a wall time of 4 hours. Once these resources are available, we receive a Job ID ( 8730.pbs02 ), and a command-line session running on node0021 . Submit a batch job Interactive jobs require you to be logged-in while your tasks are running. In contrast, you may logout after submitting a batch job, and examine the results at a later time. This is useful when you need to run several computational tasks to the cluster, and/or when your computational tasks are expected to run for a long time. To submit a batch job, you must prepare a batch script (you can do this using an editor like vim or nano ). Following is an example of a batch script (call it example.pbs ). In the batch job below, we really don't do anything useful (just sleep or \"do nothing\" for 60 seconds), #PBS -N example #PBS -l select=1:ncpus=1:mem=2gb,walltime=00:10:00 module add gcc/4.8.1 cd /home/username echo Hello World from `hostname` sleep 60 After saving the above file, you can submit the batch job using qsub : [username@login001 ~]$ qsub example.pbs 8738.pbs02 The returned job ID can be used to query the status of the job (using qstat ) (or delete it using qdel ): [username@login001 ~]$ qstat 8738.pbs02 Job id Name User Time Use S Queue ---------------- ---------------- ---------------- -------- - ----- 8738.pbs02 example username 00:00:00 R c1_solo Once the job is completed, you will see the files example.o8738 (containing output if any) and example.e8738 (containing errors if any) from your job. [username@login001 ~]$ cat example.o8738 Hello World from node0230.palmetto.clemson.edu Job submission and control on Palmetto The Palmetto cluster uses the Portable Batch Scheduling system (PBS) to manage jobs. Here are some basic PBS commands for submitting, querying and deleting jobs: Command Action qsub -I Submit an interactive job (reserves 1 core, 1gb RAM, 30 minutes walltime) qsub xyz.pbs Submit the job script xyz.pbs qstat <job id> Check the status of the job with given job ID qstat -u <username> Check the status of all jobs submitted by given username qstat -xf <job id> Check detailed information for job with given job ID qsub -q <queuename> xyz.pbs Submit to queue queuename qdel <job id> Delete the job (queued or running) with given job ID qpeek <job id> \"Peek\" at the standard output from a running job qdel -Wforce <job id> Use when job not responding to just qdel For more details and more advanced commands for submitting and controlling jobs, please refer to the PBS Professional User's Guide . PBS job options The following switches can be used either with qsub on the command line, or with a #PBS directive in a batch script. Parameter Purpose Example -N Job name (7 characters) -N maxrun1 -l Job limits (lowercase L), hardware & other requirements for job. -l select=1:ncpus=8:mem=1gb -q Queue to direct this job to ( workq is the default, supabad is an example of specific research group's job queue) -q supabad -o Path to stdout file for this job (environment variables are not accepted here) -o stdout.txt -e Path to stderr file for this job (environment variables are not accepted here) -e stderr.txt -m mail event: Email from the PBS server with flag a bort\\ b egin\\ e nd \\ or n o mail for job's notification. -m abe -M Specify list of user to whom mail about the job is sent. The user list argument is of the form: [user[@host],user[@host],...]. If -M is not used and -m is specified, PBS will send email to userid@clemson.edu -M user1@domain1.com,user2@domain2.com -j oe Join the output and error streams and write to a single file -j oe -r n Ask PBS not to restart the job if it's failed -r n For example, in a batch script: #PBS -N hydrogen #PBS -l select=1:ncpus=24:mem=200gb,walltime=4:00:00 #PBS -q bigmem #PBS -m abe #PBS -M userid@domain.com #PBS -j oe And in an interactive job request on the command line: $ qsub -I -N hydrogen -q bigmem -j oe -l select=1:ncpus=24:mem=200gb,walltime=4:00:00 For more detailed information, please take a look at: $man qsub Resource limits specification The -l switch provided to qsub or along with the #PBS directive can be used to specify the amount and kind of compute hardware (cores, memory, GPUs, interconnect, etc.,), its location, i.e., the node(s) and phase from which to request hardware, Option Purpose select Number of chunks and resources per chunk. Two or more \"chunks\" can be placed on a single node, but a single \"chunk\" cannot span more than one node. walltime Expected wall time of job (job is terminated after this time) place Controls the placement of the different chunks Here are some examples of resource limits specification: -l select=1:ncpus=8:chip_model=opteron:interconnect=10g -l select=1:ncpus=16:chip_type=e5-2665:interconnect=56g:mem=62gb,walltime=16:00:00 -l select=1:ncpus=8:chip_type=2356:interconnect=10g:mem=15gb -l select=1:ncpus=1:node_manufacturer=ibm:mem=15gb,walltime=00:20:00 -l select=1:ncpus=4:mem=15gb:ngpus=2,walltime=00:20:00 -l select=1:ncpus=4:mem=15gb:ngpus=1:gpu_model=k40,walltime=00:20:00 -l select=1:ncpus=2:mem=15gb:host=node1479,walltime=00:20:00 -l select=2:ncpus=2:mem=15gb,walltime=00:20:00,place=scatter # force each chunk to be on a different node -l select=2:ncpus=2:mem=15gb,walltime=00:20:00,place=pack # force each chunk to be on the same node and examples of options you can use in the job limit specification: chip_manufacturer=amd chip_manufacturer=intel chip_model=opteron chip_model=xeon chip_type=e5345 chip_type=e5410 chip_type=l5420 chip_type=x7542 chip_type=2356 chip_type=6172 chip_type=e5-2665 node_manufacturer=dell node_manufacturer=hp node_manufacturer=ibm node_manufacturer=sun gpu_model=k20 gpu_model=k40 interconnect=1g (1 Gbps Ethernet) interconnect=10ge (10 Gbps Ethernet) interconnect=56g (56 Gbps FDR InfiniBand, same as fdr) interconnect=fdr (56 Gbps FDR InfiniBand, same as 56g) ssd=true (Use a node with an SSD hard drive) Querying job information and deleting jobs The qstat command can be used to query the status of a particular job: $ qstat 7600424.pbs02 Job id Name User Time Use S Queue ---------------- ---------------- ---------------- -------- - ----- 7600424.pbs02 pi-mpi username 00:00:00 R c1_single To list the job IDs and status of all your jobs, you can use the -u switch: $ qstat -u username pbs02: Req'd Req'd Elap Job ID Username Queue Jobname SessID NDS TSK Memory Time S Time --------------- -------- -------- ---------- ------ --- --- ------ ----- - ----- 7600567.pbs02 username c1_singl pi-mpi-1 1382 4 8 4gb 00:05 R 00:00 7600569.pbs02 username c1_singl pi-mpi-2 20258 4 8 4gb 00:05 R 00:00 7600570.pbs02 username c1_singl pi-mpi-3 2457 4 8 4gb 00:05 R 00:00 Once a job has finished running, qstat -xf can be used to obtain detailed job information: $ qstat -xf 7600424.pbs02 Job Id: 7600424.pbs02 Job_Name = pi-mpi Job_Owner = username@login001.palmetto.clemson.edu resources_used.cpupercent = 103 resources_used.cput = 00:00:04 resources_used.mem = 45460kb resources_used.ncpus = 8 resources_used.vmem = 785708kb resources_used.walltime = 00:02:08 job_state = F queue = c1_single server = pbs02 Checkpoint = u ctime = Tue Dec 13 14:09:32 2016 Error_Path = login001.palmetto.clemson.edu:/home/username/MPI/pi-mpi.e7600424 exec_host = node0088/1*2+node0094/1*2+node0094/2*2+node0085/0*2 exec_vnode = (node0088:ncpus=2:mem=1048576kb:ngpus=0:nphis=0)+(node0094:ncp us=2:mem=1048576kb:ngpus=0:nphis=0)+(node0094:ncpus=2:mem=1048576kb:ngp us=0:nphis=0)+(node0085:ncpus=2:mem=1048576kb:ngpus=0:nphis=0) Hold_Types = n Join_Path = oe Keep_Files = n Mail_Points = a Mail_Users = username@clemson.edu mtime = Tue Dec 13 14:11:42 2016 Output_Path = login001.palmetto.clemson.edu:/home/username/MPI/pi-mpi.o760042 4 Priority = 0 qtime = Tue Dec 13 14:09:32 2016 Rerunable = True Resource_List.mem = 4gb Resource_List.mpiprocs = 8 Resource_List.ncpus = 8 Resource_List.ngpus = 0 Resource_List.nodect = 4 Resource_List.nphis = 0 Resource_List.place = free:shared Resource_List.qcat = c1_workq_qcat Resource_List.select = 4:ncpus=2:mem=1gb:interconnect=1g:mpiprocs=2 Resource_List.walltime = 00:05:00 stime = Tue Dec 13 14:09:33 2016 session_id = 2708 jobdir = /home/username substate = 92 Variable_List = PBS_O_SYSTEM=Linux,PBS_O_SHELL=/bin/bash, PBS_O_HOME=/home/username,PBS_O_LOGNAME=username, PBS_O_WORKDIR=/home/username/MPI,PBS_O_LANG=en_US.UTF-8, PBS_O_PATH=/software/examples/:/home/username/local/bin:/usr/lib64/qt-3 .3/bin:/opt/pbs/default/bin:/opt/gold/bin:/usr/local/bin:/bin:/usr/bin: /usr/local/sbin:/usr/sbin:/sbin:/opt/mx/bin:/home/username/bin, PBS_O_MAIL=/var/spool/mail/username,PBS_O_QUEUE=c1_workq, PBS_O_HOST=login001.palmetto.clemson.edu comment = Job run at Tue Dec 13 at 14:09 on (node0088:ncpus=2:mem=1048576kb :ngpus=0:nphis=0)+(node0094:ncpus=2:mem=1048576kb:ngpus=0:nphis=0)+(nod e0094:ncpus=2:mem=1048576kb:ngpus=0:nphis=0)+(node0085:ncpus=2:mem=1048 576kb:ngpus=0:nphis=0) and finished etime = Tue Dec 13 14:09:32 2016 run_count = 1 Stageout_status = 1 Exit_status = 0 Submit_arguments = job.sh history_timestamp = 1481656302 project = _pbs_project_default Similarly, to get detailed information about a running job, you can use qstat -f . To delete a job (whether in queued, running or error status), you can use the qdel command. $ qdel 7600424.pbs02 Job limits on Palmetto Walltime Jobs running in phases 1-6 of the cluster (nodes with interconnect 1g ) can run for a maximum walltime of 168 hours (7 days). Job running in phases 7 and higher of the cluster can run for a maximum walltime of 72 hours (3 days). Number of jobs When you submit a job, it is forwarded to a specific execution queue based on job critera (how many cores, RAM, etc.). There are three classes of execution queues: MX queues ( c1_ queues): jobs submitted to run on the older hardware (phases 1-6) will be forwarded to theses queues. IB queues ( c2_ queues): jobs submitted to run the newer hardware (phases 7 and up) will be forwarded to these queues. GPU queues ( gpu_ queues): jobs that request GPUs will be forwarded to these queues. bigmem queue: jobs submitted to the large-memory machines (phase 0). Each execution queue has its own limits for how many jobs can be running at one time, and how many jobs can be waiting in that execution queue. The maximum number of running jobs per user in execution queues may vary throughout the day depending on cluster load. Users can see what the current limits are using the checkqueuecfg command: $ checkqueuecfg MX QUEUES min_cores_per_job max_cores_per_job max_mem_per_queue max_jobs_per_queue max_walltime c1_solo 1 1 4000gb 2000 168:00:00 c1_single 2 24 90000gb 750 168:00:00 c1_tiny 25 128 25600gb 25 168:00:00 c1_small 129 512 24576gb 6 168:00:00 c1_medium 513 2048 81920gb 5 168:00:00 c1_large 2049 4096 32768gb 1 168:00:00 IB QUEUES min_cores_per_job max_cores_per_job max_mem_per_queue max_jobs_per_queue max_walltime c2_single 1 24 600gb 5 72:00:00 c2_tiny 25 128 4096gb 2 72:00:00 c2_small 129 512 6144gb 1 72:00:00 c2_medium 513 2048 16384gb 1 72:00:00 c2_large 2049 4096 0gb 0 72:00:00 GPU QUEUES min_gpus_per_job max_gpus_per_job min_cores_per_job max_cores_per_job max_mem_per_queue max_jobs_per_queue max_walltime gpu_small 1 4 1 96 3840gb 20 72:00:00 gpu_medium 5 16 1 256 5120gb 5 72:00:00 gpu_large 17 128 1 1024 20480gb 5 72:00:00 SMP QUEUE min_cores max_cores max_jobs max_walltime bigmem 1 64 3 72:00:00 'max_mem' is the maximum amount of memory all your jobs in this queue can consume at any one time. For example, if the max_mem for the solo queue is 4000gb, and your solo jobs each need 10gb, then you can run a maximum number of 4000/10 = 400 jobs in the solo queue, even though the current max_jobs setting for the solo queue may be set higher than 400. The qstat command tells you which of the execution queues your job is forwarded to. For example, here is an interactive job requesting 8 CPU cores, a K40 GPU, and 32gb RAM: $ qsub -I -l select=1:ncpus=8:ngpus=1:gpu_model=k40:mem=32gb,walltime=2:00:00 qsub (Warning): Interactive jobs will be treated as not rerunnable qsub: waiting for job 9567792.pbs02 to start We see from qstat that the job request is forward to the c2_single queue: [username@login001 ~]$ qstat 9567792.pbs02 Job id Name User Time Use S Queue ---------------- ---------------- ---------------- -------- - ----- 9567792.pbs02 STDIN username 0 Q c2_single From the output of checkqueuecfg above, we see that each user can have a maximum of 5 running jobs in this queue. Example PBS scripts A list of example PBS scripts for submitting jobs to the Palmetto cluster can be found here . Common problems/issues Program crashes on login node with message Killed When running commands or editing files on the login node, users may notice that their processes end abruptly with the error message Killed . Processes with names such as a.out , matlab , etc., are automatically killed on the login node because they may consume excessive computational resources. Unfortunately, this also means that benign processes, such as editing a file with the word matlab as part of its name could also be killed. Solution: Request an interactive session on a compute node ( qsub -I ), and then run the application/command. Home or scratch directories are sluggish or unresponsive The /home and /scratch directories can become slow/unresponsive when a user (or several users) read/write large amounts of data to these directories. When this happens, all users are affected as these filesystems are shared by all nodes of the cluster. To avoid this issue, keep in mind the following: Never use the /home directory as the working directory for jobs that read/write data. If too many jobs read/write data to the /home directory, it can render the cluster unusable by all users. Copy any input data to one of the /scratch directories and use that /scratch directory as the working directory for jobs. Periodically move important data back to the /home directory. Try to use /local_scratch whenever possible. Unlike /home or the /scratch directories, which are shared by all nodes, each node has its own /local_scratch directory. It is much faster to read/write data to /local_scratch , and doing so will not affect other users. (see example [here])(https://www.palmetto.clemson.edu/palmetto/userguide_howto_choose_right_filesystem.html).","title":"Basic"},{"location":"basic/#logging-in","text":"Any user with a Palmetto Cluster account can log-in using SSH (Secure Shell) . Mac OS X and Linux systems come with an SSH client installed, while Windows users will need to download one.","title":"Logging in"},{"location":"basic/#two-factor-authentication-2fa","text":"All connections to Palmetto require 2FA. If you are not enrolled in 2FA yet, you may enroll using the link https://2fa.clemson.edu/ . 2FA comes with three options for registerd devices (smart phone or tablet): Using keyboard-interactive authentication. Duo two-factor login for $user Enter a passcode or select one of the following options: 1. Duo Push to XXX-XXX-XXXX 2. Phone call to XXX-XXX-XXXX 3. SMS passcodes to XXX-XXX-XXXX Passcode or option (1-3): Option 1: response to Duo Push to your device by clicking Approve Option 2: listen to the automatic call from system and select any key on your device. Option 3: You will receive 10 different passcodes sent to your device. Enter any of the passcode to the command prompt. Option 4: If user connecting with DUO token device purchased from CCIT, use the token generated passcode Passcode or option (1-3): 1234567 More information can be found at here","title":"Two-Factor Authentication (2FA)"},{"location":"basic/#mac-os-x-and-linux-users","text":"Mac OS X or Linux users may open a Terminal, and type in the following command: $ ssh username@login.palmetto.clemson.edu where username is your Clemson user ID. You will be prompted for both your password and DUO authentication.","title":"Mac OS X and Linux users"},{"location":"basic/#windows","text":"MobaXterm is the recommended SSH client for Windows and can be downloaded here . This software is recommended because it is free and comes with: A built-in file transfer client, which allows you to exchange files and folders between your own computer and Palmetto An X11 server which allows you to run graphical programs on Palmetto cluster A graphical port-forwarding interface to support easy access to web-based programs launched inside Palmetto After downloading and installing MobaXterm, users can log-in by following these steps: Launch the MobaXterm program On the top-left corner of MobaXterm, click the Session button. Select the SSH setting and confirm that the following settings are set: Parameter Value Remote host login.palmetto.clemson.edu Port 22 X11-Forwarding enabled Compression enabled Remote environment Interactive shell SSH-browser type SCP (enhanced speed) Click OK and a new session window will be opened, where you will be prompted for your Palmetto password and the DUO authentication. After being authenticated, you will login to the login001 node. All settings for this session are saved, and for future logins, you can select this session from the Recent sessions form of the main MobaXterm window as well as the Saved sessions tab of the side window. The side window can be displayed or hidden by clicking on the blue double-arrow sign on the top left of MobaXterm. MobaXterm also comes with a built-in file browser and transfer GUI (SSH-browser). This GUI is accessible via the SCP tab of the side window. Using the Upload (green arrow pointing up) and and Download (blue arrow pointing down) buttons at the top of the SCP tab, you can easily transfer files between Palmetto and your local computer.","title":"Windows"},{"location":"basic/#basic-tasks","text":"","title":"Basic tasks"},{"location":"basic/#storing-files-and-folders","text":"","title":"Storing files and folders"},{"location":"basic/#home-and-scratch-directories","text":"Various filesystems are available for users to store data. These differ in capacity, data-persistence, and efficiency, and it is important that users understand which filesystem to use under which circumstances. Location Available space Notes /home/username 100 GB per user Backed-up nightly, permanent storage space accessible from all nodes /scratch1/username 233 TB shared by all users Not backed up, temporary work space accessible from all nodes, OrangeFS Parallel File System /scratch2/username 160 TB shared by all users Not backed up, temporary work space accessible from all nodes, XFS /scratch3/username 129 TB shared by all users Not backed up, temporary work space accessible from all nodes, ZFS /scratch4/username 175 TB shared by all users Not backed up, temporary work space accessible from all nodes, BeeGFS Parallel File System /local_scratch Varies between nodes (99GB-800GB) Per-node temporary work space, accessible only for the lifetime of job The /home and /scratch directories are shared by all nodes. In contrast, each node has its own /local_scratch directory. All data in the /home directory is permanent (not automatically deleted) and backed-up on a nightly basis. If you lose data in the /home directory, it may be possible to recover it if it was previously backed up. Data in the /scratch directories is not backed up, and any data that is untouched for 30 days is automatically removed from the /scratch directories. Data that cannot easily be reproduced should not be stored in the /scratch directories, and any data that is not required should be removed as soon as possible. See this guide on how to choose the appropriate filesystem for your work.","title":"Home and scratch directories"},{"location":"basic/#temporary-reservations","text":"All users may apply for temporary reservation of the following resources: Up to 150 TB of long-term storage Up to 8.5 TB of fast SSD scratch space All requests will be reviewed by Clemson University Computational Advisory Team (CU-CAT). Reservation requests can be made here .","title":"Temporary reservations"},{"location":"basic/#purchased-storage","text":"Users or groups may also purchase storage in 1 TB increments. For details about purchased storage, please contact the Palmetto support staff ( ithelp@clemson.edu , and include the word \"Palmetto\" in the subject line).","title":"Purchased storage"},{"location":"basic/#moving-data-in-and-out-of-the-cluster","text":"","title":"Moving data in and out of the cluster"},{"location":"basic/#small-files-kilobytes-or-a-few-megabytes","text":"On Windows machines, using the MobaXterm SSH client, the built-in file browser can be used ( SCP tab of the side window). Using the Upload (green arrow pointing up) and and Dowload (blue arrow pointing down) buttons at the top of the SCP tab, you can easily transfer small files between Palmetto and your local computer. On Unix systems, you can use the scp (secure copy) command to perform file transfers. The general form of the scp command is: $ scp <path_to_source> username@xfer01-ext.palmetto.clemson.edu:<path_to_destination> For example, here is the scp command to copy a file from the current directory on your local machine to your /home/username directory on Palmetto (this command is entered into a terminal when not logged-in to Palmetto ): $ scp myfile.txt username@xfer01-ext.palmetto.clemson.edu:/home/username ... and to do the same in reverse, i.e., copy from Palmetto to your local machine. (again, from a terminal running on your local machine, not on Palmetto): $ scp username@xfer01-ext.palmetto.clemson.edu:/home/username/myfile.txt . The . represents the working directory on the local machine. For folders, include the -r switch: $ scp -r myfolder username@xfer01-ext.palmetto.clemson.edu:/home/username","title":"Small files (kilobytes or a few megabytes)"},{"location":"basic/#transfering-larger-files-more-than-a-few-megabytes","text":"For larger files, we recommend using the Globus file transfer application. Here, we demonstrate how to use Globus Online to transfer files between Palmetto and a local machine (laptop). However, Globus can be used for file transfers to/from other locations as well. You will need to have a Globus account set up to begin. Visit https://www.globus.org/ and set up a Globus account. To begin transfering files, navigate to the Globus Online transfer utility here: https://www.globus.org/app/transfer . The transfer utility allows you to transfer files between \"endpoints\". You will need to set your local machine as a Globus Connect Personal Endpoint for the file transfer. As a part of this step, you must install the Globus Connect Personal application (see here: https://www.globus.org/app/endpoints/create-gcp ). After installing, ensure that the application is running. You should then be able to set your local machine as one endpoint. In the figure below, the endpoint is named My Personal Mac . As the second endpoint, choose clemson#xfer01-ext.clemson.edu . You can now transfer files between any locations on your local machine and the Palmetto cluster.","title":"Transfering larger files (more than a few megabytes)"},{"location":"basic/#checking-available-compute-hardware","text":"The login node login001 (the node that users first log-in to), is not meant for running computationally intensive tasks. Instead, users must reserve hardware from the compute nodes of the cluster. Currently, Palmetto cluster has over 2020 compute nodes. The hardware configuration of the different nodes is available in the file /etc/hardware-table : [atrikut@login001 ~]$ cat /etc/hardware-table PALMETTO HARDWARE TABLE Last updated: Mar 04 2019 PHASE COUNT MAKE MODEL CHIP(0) CORES RAM(1) /local_scratch Interconnect GPUs PHIs SSD 0 6 HP DL580 Intel Xeon 7542 24 505 GB(2) 99 GB 1ge 0 0 0 0 1 HP DL980 Intel Xeon 7560 64 2 TB(2) 99 GB 1ge 0 0 0 0 1 HP DL560 Intel Xeon E5-4627v4 40 1.5 TB(2) 881 GB 10ge 0 0 0 0 1 Dell R830 Intel Xeon E5-4627v4 40 1.0 TB(2) 880 GB 10ge 0 0 0 0 2 HP DL560 Intel Xeon 6138G 80 1.5 TB(2) 3.6 TB 10ge 0 0 0 1 75 Dell PE1950 Intel Xeon E5345 8 12 GB 37 GB 1g 0 0 0 2a 158 Dell PE1950 Intel Xeon E5410 8 12 GB 37 GB 1g 0 0 0 2b 84 Dell PE1950 Intel Xeon E5410 8 16 GB 37 GB 1g 0 0 0 3 225 Sun X2200 AMD Opteron 2356 8 16 GB 193 GB 1g 0 0 0 4 326 IBM DX340 Intel Xeon E5410 8 16 GB 111 GB 1g 0 0 0 5a 320 Sun X6250 Intel Xeon L5420 8 32 GB 31 GB 1g 0 0 0 5b 9 Sun X4150 Intel Xeon E5410 8 32 GB 99 GB 1g 0 0 0 6 67 HP DL165 AMD Opteron 6176 24 48 GB 193 GB 1g 0 0 0 7a 42 HP SL230 Intel Xeon E5-2665 16 64 GB 240 GB 56g, fdr 0 0 0 7b 12 HP SL250s Intel Xeon E5-2665 16 64 GB 240 GB 56g, fdr 2(3) 0 0 8a 71 HP SL250s Intel Xeon E5-2665 16 64 GB 900 GB 56g, fdr 2(4) 0 300 GB(7) 8b 57 HP SL250s Intel Xeon E5-2665 16 64 GB 420 GB 56g, fdr 2(4) 0 0 8c 88 Dell PEC6220 Intel Xeon E5-2665 16 64 GB 350 GB 10ge 0 0 0 9 72 HP SL250s Intel Xeon E5-2665 16 128 GB 420 GB 56g, fdr, 10ge 2(4) 0 0 10 80 HP SL250s Intel Xeon E5-2670v2 20 128 GB 800 GB 56g, fdr, 10ge 2(4) 0 0 11a 40 HP SL250s Intel Xeon E5-2670v2 20 128 GB 800 GB 56g, fdr, 10ge 2(6) 0 0 11b 4 HP SL250s Intel Xeon E5-2670v2 20 128 GB 800 GB 56g, fdr, 10ge 0 2(8) 0 12 30 Lenovo NX360M5 Intel Xeon E5-2680v3 24 128 GB 800 GB 56g, fdr, 10ge 2(6) 0 0 13 24 Dell C4130 Intel Xeon E5-2680v3 24 128 GB 1.8 TB 56g, fdr, 10ge 2(6) 0 0 14 12 HP XL1X0R Intel Xeon E5-2680v3 24 128 GB 880 GB 56g, fdr, 10ge 2(6) 0 0 15 32 Dell C4130 Intel Xeon E5-2680v3 24 128 GB 1.8 TB 56g, fdr, 10ge 2(6) 0 0 16 40 Dell C4130 Intel Xeon E5-2680v4 28 128 GB 1.8 TB 56g, fdr, 10ge 2(9) 0 0 17 20 Dell C4130 Intel Xeon E5-2680v4 28 128 GB 1.8 TB 56g, fdr, 10ge 2(9) 0 0 18a 2 Dell C4140 Intel Xeon 6148G 40 372 GB 1.9 TB(12) 56g, fdr, 25ge 4(10) 0 0 18b 65 Dell R740 Intel Xeon 6148G 40 372 GB 1.8 TB 56g, fdr, 25ge 2(11) 0 0 18c 10 Dell R740 Intel Xeon 6148G 40 748 GB 1.8 TB 56g, fdr, 25ge 2(11) 0 0 *** PBS resource requests are always lowercase *** (0) CHIP has 3 resources: chip_manufacturer, chip_model, chip_type (1) Leave 2 or 3GB for the operating system when requesting memory in PBS jobs (2) Specify queue \"bigmem\" to access the large memory machines, only ncpus and mem are valid PBS resource requests (3) 2 NVIDIA Tesla M2075 cards per node, use resource request \"ngpus=[1|2]\" and \"gpu_model=m2075\" (4) 2 NVIDIA Tesla K20m cards per node, use resource request \"ngpus=[1|2]\" and \"gpu_model=k20\" (5) 2 NVIDIA Tesla M2070-Q cards per node, use resource request \"ngpus=[1|2]\" and \"gpu_model=m2070q\" (6) 2 NVIDIA Tesla K40m cards per node, use resource request \"ngpus=[1|2]\" and \"gpu_model=k40\" (7) Use resource request \"ssd=true\" to request a chunk with SSD in location /ssd1, /ssd2, and /ssd3 (100GB max each) (8) Use resource request \"nphis=[1|2]\" to request phi nodes, the model is Xeon 7120p (9) 2 NVIDIA Tesla P100 cards per node, use resource request \"ngpus=[1|2]\" and \"gpu_model=p100\" (10)4 NVIDIA Tesla V100 cards per node with NVLINK2, use resource request \"ngpus=[1|2|3|4]\" and \"gpu_model=v100nv\" (11)2 NVIDIA Tesla V100 cards per node, use resource request \"ngpus=[1|2]\" and \"gpu_model=v100\" (12)Phase18a nodes contain only NVMe storage for local_scratch. The compute nodes are divided into \"phases\" (currently phases 0-18). Each phase is composed of several nodes with identical configuration, e.g., each node in phase 5a has 8 cores, 32 GB ram, 31 GB local disk space, and 10 Gbps Myrinet interconnect. A useful command on the login node is whatsfree , which gives information about how many nodes from each phase are currently in use, free, or offlined for maintenance. Later sections of this guide will describe how to submit jobs to the cluster, i.e., reserve compute nodes for running computational tasks.","title":"Checking available compute hardware"},{"location":"basic/#checking-and-using-available-software","text":"The Palmetto cluster provides a limited number of packages (including site-licensed packages), that can be used by all Palmetto users. These packages are available as modules , and must be activated/deactivated using the module command: Command Purpose module avail List all packages available (on current system) module add package/version Add a package to your current shell environment module list List packages you have loaded module rm package/version Remove a currently loaded package module purge Remove all currently loaded packages For example, to load the GCC (v4.8.1), CUDA Toolkit (v6.5.14) and OpenMPI (v1.8.4) modules, you can use the command: $ module add gcc/4.8.1 cuda-toolkit/6.5.14 openmpi/1.8.4 Then, check the version of gcc : $ gcc --version gcc (GCC) 4.8.1 Copyright (C) 2013 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Some modules when loaded, implicitly load other modules as well. If you use some modules to compile/install some software, then you will probably have to load them when running that software as well, otherwise you may see errors about missing libraries/headers. Modules do not remain loaded when you log out and log back in, i.e., they are active only for the current session - so you will need to load them for every session. As an exercise, examine the environment variables PATH, LIBRARY_PATH, etc., before and after loading some module: $ echo $PATH $ module add python/2.7.13 $ echo $PATH You can also look at the modulefiles in /software/modulefiles to understand what happens when you add a module.","title":"Checking and using available software"},{"location":"basic/#start-an-interactive-job","text":"An interactive job can be started using the qsub command. Here is an example of an interactive job: [username@login001 ~]$ qsub -I -l select=1:ncpus=2:mem=4gb,walltime=4:00:00 qsub (Warning): Interactive jobs will be treated as not rerunnable qsub: waiting for job 8730.pbs02 to start qsub: job 8730.pbs02 ready [username@node0021 ~]$ module add python/3.4 [username@node0021 ~]$ python runsim.py . . . [username@node0021 ~]$ exit [username@login001 ~]$ Above, we request an interactive job using 1 \"chunk\" of hardware ( select=1 ), 2 CPU cores per \"chunk\", and 4gb of RAM per \"chunk\", for a wall time of 4 hours. Once these resources are available, we receive a Job ID ( 8730.pbs02 ), and a command-line session running on node0021 .","title":"Start an interactive job"},{"location":"basic/#submit-a-batch-job","text":"Interactive jobs require you to be logged-in while your tasks are running. In contrast, you may logout after submitting a batch job, and examine the results at a later time. This is useful when you need to run several computational tasks to the cluster, and/or when your computational tasks are expected to run for a long time. To submit a batch job, you must prepare a batch script (you can do this using an editor like vim or nano ). Following is an example of a batch script (call it example.pbs ). In the batch job below, we really don't do anything useful (just sleep or \"do nothing\" for 60 seconds), #PBS -N example #PBS -l select=1:ncpus=1:mem=2gb,walltime=00:10:00 module add gcc/4.8.1 cd /home/username echo Hello World from `hostname` sleep 60 After saving the above file, you can submit the batch job using qsub : [username@login001 ~]$ qsub example.pbs 8738.pbs02 The returned job ID can be used to query the status of the job (using qstat ) (or delete it using qdel ): [username@login001 ~]$ qstat 8738.pbs02 Job id Name User Time Use S Queue ---------------- ---------------- ---------------- -------- - ----- 8738.pbs02 example username 00:00:00 R c1_solo Once the job is completed, you will see the files example.o8738 (containing output if any) and example.e8738 (containing errors if any) from your job. [username@login001 ~]$ cat example.o8738 Hello World from node0230.palmetto.clemson.edu","title":"Submit a batch job"},{"location":"basic/#job-submission-and-control-on-palmetto","text":"The Palmetto cluster uses the Portable Batch Scheduling system (PBS) to manage jobs. Here are some basic PBS commands for submitting, querying and deleting jobs: Command Action qsub -I Submit an interactive job (reserves 1 core, 1gb RAM, 30 minutes walltime) qsub xyz.pbs Submit the job script xyz.pbs qstat <job id> Check the status of the job with given job ID qstat -u <username> Check the status of all jobs submitted by given username qstat -xf <job id> Check detailed information for job with given job ID qsub -q <queuename> xyz.pbs Submit to queue queuename qdel <job id> Delete the job (queued or running) with given job ID qpeek <job id> \"Peek\" at the standard output from a running job qdel -Wforce <job id> Use when job not responding to just qdel For more details and more advanced commands for submitting and controlling jobs, please refer to the PBS Professional User's Guide .","title":"Job submission and control on Palmetto"},{"location":"basic/#pbs-job-options","text":"The following switches can be used either with qsub on the command line, or with a #PBS directive in a batch script. Parameter Purpose Example -N Job name (7 characters) -N maxrun1 -l Job limits (lowercase L), hardware & other requirements for job. -l select=1:ncpus=8:mem=1gb -q Queue to direct this job to ( workq is the default, supabad is an example of specific research group's job queue) -q supabad -o Path to stdout file for this job (environment variables are not accepted here) -o stdout.txt -e Path to stderr file for this job (environment variables are not accepted here) -e stderr.txt -m mail event: Email from the PBS server with flag a bort\\ b egin\\ e nd \\ or n o mail for job's notification. -m abe -M Specify list of user to whom mail about the job is sent. The user list argument is of the form: [user[@host],user[@host],...]. If -M is not used and -m is specified, PBS will send email to userid@clemson.edu -M user1@domain1.com,user2@domain2.com -j oe Join the output and error streams and write to a single file -j oe -r n Ask PBS not to restart the job if it's failed -r n For example, in a batch script: #PBS -N hydrogen #PBS -l select=1:ncpus=24:mem=200gb,walltime=4:00:00 #PBS -q bigmem #PBS -m abe #PBS -M userid@domain.com #PBS -j oe And in an interactive job request on the command line: $ qsub -I -N hydrogen -q bigmem -j oe -l select=1:ncpus=24:mem=200gb,walltime=4:00:00 For more detailed information, please take a look at: $man qsub","title":"PBS job options"},{"location":"basic/#resource-limits-specification","text":"The -l switch provided to qsub or along with the #PBS directive can be used to specify the amount and kind of compute hardware (cores, memory, GPUs, interconnect, etc.,), its location, i.e., the node(s) and phase from which to request hardware, Option Purpose select Number of chunks and resources per chunk. Two or more \"chunks\" can be placed on a single node, but a single \"chunk\" cannot span more than one node. walltime Expected wall time of job (job is terminated after this time) place Controls the placement of the different chunks Here are some examples of resource limits specification: -l select=1:ncpus=8:chip_model=opteron:interconnect=10g -l select=1:ncpus=16:chip_type=e5-2665:interconnect=56g:mem=62gb,walltime=16:00:00 -l select=1:ncpus=8:chip_type=2356:interconnect=10g:mem=15gb -l select=1:ncpus=1:node_manufacturer=ibm:mem=15gb,walltime=00:20:00 -l select=1:ncpus=4:mem=15gb:ngpus=2,walltime=00:20:00 -l select=1:ncpus=4:mem=15gb:ngpus=1:gpu_model=k40,walltime=00:20:00 -l select=1:ncpus=2:mem=15gb:host=node1479,walltime=00:20:00 -l select=2:ncpus=2:mem=15gb,walltime=00:20:00,place=scatter # force each chunk to be on a different node -l select=2:ncpus=2:mem=15gb,walltime=00:20:00,place=pack # force each chunk to be on the same node and examples of options you can use in the job limit specification: chip_manufacturer=amd chip_manufacturer=intel chip_model=opteron chip_model=xeon chip_type=e5345 chip_type=e5410 chip_type=l5420 chip_type=x7542 chip_type=2356 chip_type=6172 chip_type=e5-2665 node_manufacturer=dell node_manufacturer=hp node_manufacturer=ibm node_manufacturer=sun gpu_model=k20 gpu_model=k40 interconnect=1g (1 Gbps Ethernet) interconnect=10ge (10 Gbps Ethernet) interconnect=56g (56 Gbps FDR InfiniBand, same as fdr) interconnect=fdr (56 Gbps FDR InfiniBand, same as 56g) ssd=true (Use a node with an SSD hard drive)","title":"Resource limits specification"},{"location":"basic/#querying-job-information-and-deleting-jobs","text":"The qstat command can be used to query the status of a particular job: $ qstat 7600424.pbs02 Job id Name User Time Use S Queue ---------------- ---------------- ---------------- -------- - ----- 7600424.pbs02 pi-mpi username 00:00:00 R c1_single To list the job IDs and status of all your jobs, you can use the -u switch: $ qstat -u username pbs02: Req'd Req'd Elap Job ID Username Queue Jobname SessID NDS TSK Memory Time S Time --------------- -------- -------- ---------- ------ --- --- ------ ----- - ----- 7600567.pbs02 username c1_singl pi-mpi-1 1382 4 8 4gb 00:05 R 00:00 7600569.pbs02 username c1_singl pi-mpi-2 20258 4 8 4gb 00:05 R 00:00 7600570.pbs02 username c1_singl pi-mpi-3 2457 4 8 4gb 00:05 R 00:00 Once a job has finished running, qstat -xf can be used to obtain detailed job information: $ qstat -xf 7600424.pbs02 Job Id: 7600424.pbs02 Job_Name = pi-mpi Job_Owner = username@login001.palmetto.clemson.edu resources_used.cpupercent = 103 resources_used.cput = 00:00:04 resources_used.mem = 45460kb resources_used.ncpus = 8 resources_used.vmem = 785708kb resources_used.walltime = 00:02:08 job_state = F queue = c1_single server = pbs02 Checkpoint = u ctime = Tue Dec 13 14:09:32 2016 Error_Path = login001.palmetto.clemson.edu:/home/username/MPI/pi-mpi.e7600424 exec_host = node0088/1*2+node0094/1*2+node0094/2*2+node0085/0*2 exec_vnode = (node0088:ncpus=2:mem=1048576kb:ngpus=0:nphis=0)+(node0094:ncp us=2:mem=1048576kb:ngpus=0:nphis=0)+(node0094:ncpus=2:mem=1048576kb:ngp us=0:nphis=0)+(node0085:ncpus=2:mem=1048576kb:ngpus=0:nphis=0) Hold_Types = n Join_Path = oe Keep_Files = n Mail_Points = a Mail_Users = username@clemson.edu mtime = Tue Dec 13 14:11:42 2016 Output_Path = login001.palmetto.clemson.edu:/home/username/MPI/pi-mpi.o760042 4 Priority = 0 qtime = Tue Dec 13 14:09:32 2016 Rerunable = True Resource_List.mem = 4gb Resource_List.mpiprocs = 8 Resource_List.ncpus = 8 Resource_List.ngpus = 0 Resource_List.nodect = 4 Resource_List.nphis = 0 Resource_List.place = free:shared Resource_List.qcat = c1_workq_qcat Resource_List.select = 4:ncpus=2:mem=1gb:interconnect=1g:mpiprocs=2 Resource_List.walltime = 00:05:00 stime = Tue Dec 13 14:09:33 2016 session_id = 2708 jobdir = /home/username substate = 92 Variable_List = PBS_O_SYSTEM=Linux,PBS_O_SHELL=/bin/bash, PBS_O_HOME=/home/username,PBS_O_LOGNAME=username, PBS_O_WORKDIR=/home/username/MPI,PBS_O_LANG=en_US.UTF-8, PBS_O_PATH=/software/examples/:/home/username/local/bin:/usr/lib64/qt-3 .3/bin:/opt/pbs/default/bin:/opt/gold/bin:/usr/local/bin:/bin:/usr/bin: /usr/local/sbin:/usr/sbin:/sbin:/opt/mx/bin:/home/username/bin, PBS_O_MAIL=/var/spool/mail/username,PBS_O_QUEUE=c1_workq, PBS_O_HOST=login001.palmetto.clemson.edu comment = Job run at Tue Dec 13 at 14:09 on (node0088:ncpus=2:mem=1048576kb :ngpus=0:nphis=0)+(node0094:ncpus=2:mem=1048576kb:ngpus=0:nphis=0)+(nod e0094:ncpus=2:mem=1048576kb:ngpus=0:nphis=0)+(node0085:ncpus=2:mem=1048 576kb:ngpus=0:nphis=0) and finished etime = Tue Dec 13 14:09:32 2016 run_count = 1 Stageout_status = 1 Exit_status = 0 Submit_arguments = job.sh history_timestamp = 1481656302 project = _pbs_project_default Similarly, to get detailed information about a running job, you can use qstat -f . To delete a job (whether in queued, running or error status), you can use the qdel command. $ qdel 7600424.pbs02","title":"Querying job information and deleting jobs"},{"location":"basic/#job-limits-on-palmetto","text":"","title":"Job limits on Palmetto"},{"location":"basic/#walltime","text":"Jobs running in phases 1-6 of the cluster (nodes with interconnect 1g ) can run for a maximum walltime of 168 hours (7 days). Job running in phases 7 and higher of the cluster can run for a maximum walltime of 72 hours (3 days).","title":"Walltime"},{"location":"basic/#number-of-jobs","text":"When you submit a job, it is forwarded to a specific execution queue based on job critera (how many cores, RAM, etc.). There are three classes of execution queues: MX queues ( c1_ queues): jobs submitted to run on the older hardware (phases 1-6) will be forwarded to theses queues. IB queues ( c2_ queues): jobs submitted to run the newer hardware (phases 7 and up) will be forwarded to these queues. GPU queues ( gpu_ queues): jobs that request GPUs will be forwarded to these queues. bigmem queue: jobs submitted to the large-memory machines (phase 0). Each execution queue has its own limits for how many jobs can be running at one time, and how many jobs can be waiting in that execution queue. The maximum number of running jobs per user in execution queues may vary throughout the day depending on cluster load. Users can see what the current limits are using the checkqueuecfg command: $ checkqueuecfg MX QUEUES min_cores_per_job max_cores_per_job max_mem_per_queue max_jobs_per_queue max_walltime c1_solo 1 1 4000gb 2000 168:00:00 c1_single 2 24 90000gb 750 168:00:00 c1_tiny 25 128 25600gb 25 168:00:00 c1_small 129 512 24576gb 6 168:00:00 c1_medium 513 2048 81920gb 5 168:00:00 c1_large 2049 4096 32768gb 1 168:00:00 IB QUEUES min_cores_per_job max_cores_per_job max_mem_per_queue max_jobs_per_queue max_walltime c2_single 1 24 600gb 5 72:00:00 c2_tiny 25 128 4096gb 2 72:00:00 c2_small 129 512 6144gb 1 72:00:00 c2_medium 513 2048 16384gb 1 72:00:00 c2_large 2049 4096 0gb 0 72:00:00 GPU QUEUES min_gpus_per_job max_gpus_per_job min_cores_per_job max_cores_per_job max_mem_per_queue max_jobs_per_queue max_walltime gpu_small 1 4 1 96 3840gb 20 72:00:00 gpu_medium 5 16 1 256 5120gb 5 72:00:00 gpu_large 17 128 1 1024 20480gb 5 72:00:00 SMP QUEUE min_cores max_cores max_jobs max_walltime bigmem 1 64 3 72:00:00 'max_mem' is the maximum amount of memory all your jobs in this queue can consume at any one time. For example, if the max_mem for the solo queue is 4000gb, and your solo jobs each need 10gb, then you can run a maximum number of 4000/10 = 400 jobs in the solo queue, even though the current max_jobs setting for the solo queue may be set higher than 400. The qstat command tells you which of the execution queues your job is forwarded to. For example, here is an interactive job requesting 8 CPU cores, a K40 GPU, and 32gb RAM: $ qsub -I -l select=1:ncpus=8:ngpus=1:gpu_model=k40:mem=32gb,walltime=2:00:00 qsub (Warning): Interactive jobs will be treated as not rerunnable qsub: waiting for job 9567792.pbs02 to start We see from qstat that the job request is forward to the c2_single queue: [username@login001 ~]$ qstat 9567792.pbs02 Job id Name User Time Use S Queue ---------------- ---------------- ---------------- -------- - ----- 9567792.pbs02 STDIN username 0 Q c2_single From the output of checkqueuecfg above, we see that each user can have a maximum of 5 running jobs in this queue.","title":"Number of jobs"},{"location":"basic/#example-pbs-scripts","text":"A list of example PBS scripts for submitting jobs to the Palmetto cluster can be found here .","title":"Example PBS scripts"},{"location":"basic/#common-problemsissues","text":"","title":"Common problems/issues"},{"location":"basic/#program-crashes-on-login-node-with-message-killed","text":"When running commands or editing files on the login node, users may notice that their processes end abruptly with the error message Killed . Processes with names such as a.out , matlab , etc., are automatically killed on the login node because they may consume excessive computational resources. Unfortunately, this also means that benign processes, such as editing a file with the word matlab as part of its name could also be killed. Solution: Request an interactive session on a compute node ( qsub -I ), and then run the application/command.","title":"Program crashes on login node with message Killed"},{"location":"basic/#home-or-scratch-directories-are-sluggish-or-unresponsive","text":"The /home and /scratch directories can become slow/unresponsive when a user (or several users) read/write large amounts of data to these directories. When this happens, all users are affected as these filesystems are shared by all nodes of the cluster. To avoid this issue, keep in mind the following: Never use the /home directory as the working directory for jobs that read/write data. If too many jobs read/write data to the /home directory, it can render the cluster unusable by all users. Copy any input data to one of the /scratch directories and use that /scratch directory as the working directory for jobs. Periodically move important data back to the /home directory. Try to use /local_scratch whenever possible. Unlike /home or the /scratch directories, which are shared by all nodes, each node has its own /local_scratch directory. It is much faster to read/write data to /local_scratch , and doing so will not affect other users. (see example [here])(https://www.palmetto.clemson.edu/palmetto/userguide_howto_choose_right_filesystem.html).","title":"Home or scratch directories are sluggish or unresponsive"},{"location":"news/","text":"","title":"News"},{"location":"owner/","text":"","title":"Owner"},{"location":"basic/jupyter/","text":"JuputerLab Interface to Palmetto Running your first Jupyter notebook on Palmetto In this section, you will: Download a notebook file ( .ipynb ) to your local machine (laptop or workstation) Upload this file to the Palmetto cluster Run it using Jupyter on Palmetto Downloading the notebook to your local machine All Jupyter notebooks have the extension ( .ipynb ) - a result of the Jupyter project being a fork of the IPython project. The notebook for this exercise can be downloaded here . If simply clicking on the link doesn't work, try right-clicking on it and selecting \"Save Link As...\" Uploading the notebook to the Palmetto cluster You can upload any file to the Palmetto cluster via the Jupyter dashboard. Click on the \"Upload\" button on the top right of the file explorer: Be aware that there is a limit on the size of files you can upload in this way (~25 MB). Once you've uploaded the file, it should appear in the file browser: Running the notebook Simply click on the file index.ipynb to run it.","title":"JupyterLab"},{"location":"basic/jupyter/#juputerlab-interface-to-palmetto","text":"","title":"JuputerLab Interface to Palmetto"},{"location":"basic/jupyter/#running-your-first-jupyter-notebook-on-palmetto","text":"In this section, you will: Download a notebook file ( .ipynb ) to your local machine (laptop or workstation) Upload this file to the Palmetto cluster Run it using Jupyter on Palmetto","title":"Running your first Jupyter notebook on Palmetto"},{"location":"basic/jupyter/#downloading-the-notebook-to-your-local-machine","text":"All Jupyter notebooks have the extension ( .ipynb ) - a result of the Jupyter project being a fork of the IPython project. The notebook for this exercise can be downloaded here . If simply clicking on the link doesn't work, try right-clicking on it and selecting \"Save Link As...\"","title":"Downloading the notebook to your local machine"},{"location":"basic/jupyter/#uploading-the-notebook-to-the-palmetto-cluster","text":"You can upload any file to the Palmetto cluster via the Jupyter dashboard. Click on the \"Upload\" button on the top right of the file explorer: Be aware that there is a limit on the size of files you can upload in this way (~25 MB). Once you've uploaded the file, it should appear in the file browser:","title":"Uploading the notebook to the Palmetto cluster"},{"location":"basic/jupyter/#running-the-notebook","text":"Simply click on the file index.ipynb to run it.","title":"Running the notebook"},{"location":"basic/login/","text":"Logging in Any user with a Palmetto Cluster account can log-in using SSH (Secure Shell) . Mac OS X and Linux systems come with an SSH client installed, while Windows users will need to download one. Mac OS X and Linux users Mac OS X or Linux users may open a Terminal, and type in the following command: $ ssh username@login.palmetto.clemson.edu where username is your Clemson user ID. You will be prompted for both your password and DUO authentication. Windows MobaXterm is the recommended SSH client for Windows and can be downloaded from MobaXterm's download page . This software is recommended because it is free and comes with: A built-in file transfer client, which allows you to exchange files and folders between your own computer and Palmetto in a convenient manner. An X11 server which allows you to run graphical programs on Palmetto cluster A graphical port-forwarding interface to support easy access to web-based programs launched inside Palmetto If you select the installation version of MobaXterm, you will need to unzip the downloaded file before running the installation program. Windows sometimes allow users to run the installation from inside the zipped file, resulting in missing an additional utility file (still inside the zipped file). After downloading and installing MobaXterm, users can log-in by following these steps: Launch the MobaXterm program On the top-left corner of MobaXterm, click the Session button. Confirm that the following settings are set for Basic SSH settings and Advanced SSH settings : Parameter Value Remote host login.palmetto.clemson.edu Port 22 X11-Forwarding enabled Compression enabled Remote environment Interactive shell SSH-browser type SCP (enhanced speed) Click OK and a new session window will be opened, where you will be prompted for your Palmetto password and the DUO authentication. After being authenticated, you will login to the login001 node. All settings for this session are saved, and for future logins, you can select this session from the Recent sessions form of the main MobaXterm window as well as the Saved sessions tab of the side window. The side window can be displayed or hidden by clicking on the blue double-arrow sign on the top left of MobaXterm. MobaXterm also comes with a built-in file browser and transfer GUI (SSH-browser). This GUI is accessible via the SCP tab of the side window. Using the Upload (green arrow pointing up) and and Download (blue arrow pointing down) buttons at the top of the SCP tab, you can easily transfer files between Palmetto and your local computer. Two-Factor Authentication (2FA) All connections to Palmetto require 2FA. If you are not enrolled in 2FA yet, you may enroll using the link https://2fa.clemson.edu/ . After you enter your login name and password, Palmetto will ask you to provide additional authentication for 2FA via one of the following three options for registerd devices (smart phone or tablet): Using keyboard-interactive authentication. Duo two-factor login for $user Enter a passcode or select one of the following options: 1. Duo Push to XXX-XXX-XXXX 2. Phone call to XXX-XXX-XXXX 3. SMS passcodes to XXX-XXX-XXXX Passcode or option (1-3): Option 1: response to Duo Push to your device by clicking Approve Option 2: listen to the automatic call from system and select any key on your device. Option 3: enter a passcode that is shown in your DUO app. Passcode or option (1-3): 1234567 More information can be found at here Basic tasks Storing files and folders Home and scratch directories Various filesystems are available for users to store data. These differ in capacity, data-persistence, and efficiency, and it is important that users understand which filesystem to use under which circumstances. Location Available space Notes /home/username 100 GB per user Backed-up nightly, permanent storage space accessible from all nodes /scratch1/username 233 TB shared by all users Not backed up, temporary work space accessible from all nodes, OrangeFS Parallel File System /scratch2/username 160 TB shared by all users Not backed up, temporary work space accessible from all nodes, XFS /scratch3/username 129 TB shared by all users Not backed up, temporary work space accessible from all nodes, ZFS /scratch4/username 175 TB shared by all users Not backed up, temporary work space accessible from all nodes, BeeGFS Parallel File System /local_scratch Varies between nodes (99GB-800GB) Per-node temporary work space, accessible only for the lifetime of job The /home and /scratch directories are shared by all nodes. In contrast, each node has its own /local_scratch directory. All data in the /home directory is permanent (not automatically deleted) and backed-up on a nightly basis. If you lose data in the /home directory, it may be possible to recover it if it was previously backed up. Data in the /scratch directories is not backed up, and any data that is untouched for 30 days is automatically removed from the /scratch directories. Data that cannot easily be reproduced should not be stored in the /scratch directories, and any data that is not required should be removed as soon as possible. See this guide on how to choose the appropriate filesystem for your work. Temporary reservations All users may apply for temporary reservation of the following resources: Up to 150 TB of long-term storage Up to 8.5 TB of fast SSD scratch space All requests will be reviewed by Clemson University Computational Advisory Team (CU-CAT). Reservation requests can be made here . Purchased storage Users or groups may also purchase storage in 1 TB increments. For details about purchased storage, please contact the Palmetto support staff ( ithelp@clemson.edu , and include the word \"Palmetto\" in the subject line). Moving data in and out of the cluster Small files (kilobytes or a few megabytes) On Windows machines, using the MobaXterm SSH client, the built-in file browser can be used ( SCP tab of the side window). Using the Upload (green arrow pointing up) and and Dowload (blue arrow pointing down) buttons at the top of the SCP tab, you can easily transfer small files between Palmetto and your local computer. On Unix systems, you can use the scp (secure copy) command to perform file transfers. The general form of the scp command is: $ scp <path_to_source> username@xfer01-ext.palmetto.clemson.edu:<path_to_destination> For example, here is the scp command to copy a file from the current directory on your local machine to your /home/username directory on Palmetto (this command is entered into a terminal when not logged-in to Palmetto ): $ scp myfile.txt username@xfer01-ext.palmetto.clemson.edu:/home/username ... and to do the same in reverse, i.e., copy from Palmetto to your local machine. (again, from a terminal running on your local machine, not on Palmetto): $ scp username@xfer01-ext.palmetto.clemson.edu:/home/username/myfile.txt . The . represents the working directory on the local machine. For folders, include the -r switch: $ scp -r myfolder username@xfer01-ext.palmetto.clemson.edu:/home/username Transfering larger files (more than a few megabytes) For larger files, we recommend using the Globus file transfer application. Here, we demonstrate how to use Globus Online to transfer files between Palmetto and a local machine (laptop). However, Globus can be used for file transfers to/from other locations as well. You will need to have a Globus account set up to begin. Visit https://www.globus.org/ and set up a Globus account. To begin transfering files, navigate to the Globus Online transfer utility here: https://www.globus.org/app/transfer . The transfer utility allows you to transfer files between \"endpoints\". You will need to set your local machine as a Globus Connect Personal Endpoint for the file transfer. As a part of this step, you must install the Globus Connect Personal application (see here: https://www.globus.org/app/endpoints/create-gcp ). After installing, ensure that the application is running. You should then be able to set your local machine as one endpoint. In the figure below, the endpoint is named My Personal Mac . As the second endpoint, choose clemson#xfer01-ext.clemson.edu . You can now transfer files between any locations on your local machine and the Palmetto cluster. Checking available compute hardware The login node login001 (the node that users first log-in to), is not meant for running computationally intensive tasks. Instead, users must reserve hardware from the compute nodes of the cluster. Currently, Palmetto cluster has over 2020 compute nodes. The hardware configuration of the different nodes is available in the file /etc/hardware-table : [atrikut@login001 ~]$ cat /etc/hardware-table PALMETTO HARDWARE TABLE Last updated: Mar 04 2019 PHASE COUNT MAKE MODEL CHIP(0) CORES RAM(1) /local_scratch Interconnect GPUs PHIs SSD 0 6 HP DL580 Intel Xeon 7542 24 505 GB(2) 99 GB 1ge 0 0 0 0 1 HP DL980 Intel Xeon 7560 64 2 TB(2) 99 GB 1ge 0 0 0 0 1 HP DL560 Intel Xeon E5-4627v4 40 1.5 TB(2) 881 GB 10ge 0 0 0 0 1 Dell R830 Intel Xeon E5-4627v4 40 1.0 TB(2) 880 GB 10ge 0 0 0 0 2 HP DL560 Intel Xeon 6138G 80 1.5 TB(2) 3.6 TB 10ge 0 0 0 1 75 Dell PE1950 Intel Xeon E5345 8 12 GB 37 GB 1g 0 0 0 2a 158 Dell PE1950 Intel Xeon E5410 8 12 GB 37 GB 1g 0 0 0 2b 84 Dell PE1950 Intel Xeon E5410 8 16 GB 37 GB 1g 0 0 0 3 225 Sun X2200 AMD Opteron 2356 8 16 GB 193 GB 1g 0 0 0 4 326 IBM DX340 Intel Xeon E5410 8 16 GB 111 GB 1g 0 0 0 5a 320 Sun X6250 Intel Xeon L5420 8 32 GB 31 GB 1g 0 0 0 5b 9 Sun X4150 Intel Xeon E5410 8 32 GB 99 GB 1g 0 0 0 6 67 HP DL165 AMD Opteron 6176 24 48 GB 193 GB 1g 0 0 0 7a 42 HP SL230 Intel Xeon E5-2665 16 64 GB 240 GB 56g, fdr 0 0 0 7b 12 HP SL250s Intel Xeon E5-2665 16 64 GB 240 GB 56g, fdr 2(3) 0 0 8a 71 HP SL250s Intel Xeon E5-2665 16 64 GB 900 GB 56g, fdr 2(4) 0 300 GB(7) 8b 57 HP SL250s Intel Xeon E5-2665 16 64 GB 420 GB 56g, fdr 2(4) 0 0 8c 88 Dell PEC6220 Intel Xeon E5-2665 16 64 GB 350 GB 10ge 0 0 0 9 72 HP SL250s Intel Xeon E5-2665 16 128 GB 420 GB 56g, fdr, 10ge 2(4) 0 0 10 80 HP SL250s Intel Xeon E5-2670v2 20 128 GB 800 GB 56g, fdr, 10ge 2(4) 0 0 11a 40 HP SL250s Intel Xeon E5-2670v2 20 128 GB 800 GB 56g, fdr, 10ge 2(6) 0 0 11b 4 HP SL250s Intel Xeon E5-2670v2 20 128 GB 800 GB 56g, fdr, 10ge 0 2(8) 0 12 30 Lenovo NX360M5 Intel Xeon E5-2680v3 24 128 GB 800 GB 56g, fdr, 10ge 2(6) 0 0 13 24 Dell C4130 Intel Xeon E5-2680v3 24 128 GB 1.8 TB 56g, fdr, 10ge 2(6) 0 0 14 12 HP XL1X0R Intel Xeon E5-2680v3 24 128 GB 880 GB 56g, fdr, 10ge 2(6) 0 0 15 32 Dell C4130 Intel Xeon E5-2680v3 24 128 GB 1.8 TB 56g, fdr, 10ge 2(6) 0 0 16 40 Dell C4130 Intel Xeon E5-2680v4 28 128 GB 1.8 TB 56g, fdr, 10ge 2(9) 0 0 17 20 Dell C4130 Intel Xeon E5-2680v4 28 128 GB 1.8 TB 56g, fdr, 10ge 2(9) 0 0 18a 2 Dell C4140 Intel Xeon 6148G 40 372 GB 1.9 TB(12) 56g, fdr, 25ge 4(10) 0 0 18b 65 Dell R740 Intel Xeon 6148G 40 372 GB 1.8 TB 56g, fdr, 25ge 2(11) 0 0 18c 10 Dell R740 Intel Xeon 6148G 40 748 GB 1.8 TB 56g, fdr, 25ge 2(11) 0 0 *** PBS resource requests are always lowercase *** (0) CHIP has 3 resources: chip_manufacturer, chip_model, chip_type (1) Leave 2 or 3GB for the operating system when requesting memory in PBS jobs (2) Specify queue \"bigmem\" to access the large memory machines, only ncpus and mem are valid PBS resource requests (3) 2 NVIDIA Tesla M2075 cards per node, use resource request \"ngpus=[1|2]\" and \"gpu_model=m2075\" (4) 2 NVIDIA Tesla K20m cards per node, use resource request \"ngpus=[1|2]\" and \"gpu_model=k20\" (5) 2 NVIDIA Tesla M2070-Q cards per node, use resource request \"ngpus=[1|2]\" and \"gpu_model=m2070q\" (6) 2 NVIDIA Tesla K40m cards per node, use resource request \"ngpus=[1|2]\" and \"gpu_model=k40\" (7) Use resource request \"ssd=true\" to request a chunk with SSD in location /ssd1, /ssd2, and /ssd3 (100GB max each) (8) Use resource request \"nphis=[1|2]\" to request phi nodes, the model is Xeon 7120p (9) 2 NVIDIA Tesla P100 cards per node, use resource request \"ngpus=[1|2]\" and \"gpu_model=p100\" (10)4 NVIDIA Tesla V100 cards per node with NVLINK2, use resource request \"ngpus=[1|2|3|4]\" and \"gpu_model=v100nv\" (11)2 NVIDIA Tesla V100 cards per node, use resource request \"ngpus=[1|2]\" and \"gpu_model=v100\" (12)Phase18a nodes contain only NVMe storage for local_scratch. The compute nodes are divided into \"phases\" (currently phases 0-18). Each phase is composed of several nodes with identical configuration, e.g., each node in phase 5a has 8 cores, 32 GB ram, 31 GB local disk space, and 10 Gbps Myrinet interconnect. A useful command on the login node is whatsfree , which gives information about how many nodes from each phase are currently in use, free, or offlined for maintenance. Later sections of this guide will describe how to submit jobs to the cluster, i.e., reserve compute nodes for running computational tasks. Checking and using available software The Palmetto cluster provides a limited number of packages (including site-licensed packages), that can be used by all Palmetto users. These packages are available as modules , and must be activated/deactivated using the module command: Command Purpose module avail List all packages available (on current system) module add package/version Add a package to your current shell environment module list List packages you have loaded module rm package/version Remove a currently loaded package module purge Remove all currently loaded packages For example, to load the GCC (v4.8.1), CUDA Toolkit (v6.5.14) and OpenMPI (v1.8.4) modules, you can use the command: $ module add gcc/4.8.1 cuda-toolkit/6.5.14 openmpi/1.8.4 Then, check the version of gcc : $ gcc --version gcc (GCC) 4.8.1 Copyright (C) 2013 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Some modules when loaded, implicitly load other modules as well. If you use some modules to compile/install some software, then you will probably have to load them when running that software as well, otherwise you may see errors about missing libraries/headers. Modules do not remain loaded when you log out and log back in, i.e., they are active only for the current session - so you will need to load them for every session. As an exercise, examine the environment variables PATH, LIBRARY_PATH, etc., before and after loading some module: $ echo $PATH $ module add python/2.7.13 $ echo $PATH You can also look at the modulefiles in /software/modulefiles to understand what happens when you add a module. Start an interactive job An interactive job can be started using the qsub command. Here is an example of an interactive job: [username@login001 ~]$ qsub -I -l select=1:ncpus=2:mem=4gb,walltime=4:00:00 qsub (Warning): Interactive jobs will be treated as not rerunnable qsub: waiting for job 8730.pbs02 to start qsub: job 8730.pbs02 ready [username@node0021 ~]$ module add python/3.4 [username@node0021 ~]$ python runsim.py . . . [username@node0021 ~]$ exit [username@login001 ~]$ Above, we request an interactive job using 1 \"chunk\" of hardware ( select=1 ), 2 CPU cores per \"chunk\", and 4gb of RAM per \"chunk\", for a wall time of 4 hours. Once these resources are available, we receive a Job ID ( 8730.pbs02 ), and a command-line session running on node0021 . Submit a batch job Interactive jobs require you to be logged-in while your tasks are running. In contrast, you may logout after submitting a batch job, and examine the results at a later time. This is useful when you need to run several computational tasks to the cluster, and/or when your computational tasks are expected to run for a long time. To submit a batch job, you must prepare a batch script (you can do this using an editor like vim or nano ). Following is an example of a batch script (call it example.pbs ). In the batch job below, we really don't do anything useful (just sleep or \"do nothing\" for 60 seconds), #PBS -N example #PBS -l select=1:ncpus=1:mem=2gb,walltime=00:10:00 module add gcc/4.8.1 cd /home/username echo Hello World from `hostname` sleep 60 After saving the above file, you can submit the batch job using qsub : [username@login001 ~]$ qsub example.pbs 8738.pbs02 The returned job ID can be used to query the status of the job (using qstat ) (or delete it using qdel ): [username@login001 ~]$ qstat 8738.pbs02 Job id Name User Time Use S Queue ---------------- ---------------- ---------------- -------- - ----- 8738.pbs02 example username 00:00:00 R c1_solo Once the job is completed, you will see the files example.o8738 (containing output if any) and example.e8738 (containing errors if any) from your job. [username@login001 ~]$ cat example.o8738 Hello World from node0230.palmetto.clemson.edu Job submission and control on Palmetto The Palmetto cluster uses the Portable Batch Scheduling system (PBS) to manage jobs. Here are some basic PBS commands for submitting, querying and deleting jobs: Command Action qsub -I Submit an interactive job (reserves 1 core, 1gb RAM, 30 minutes walltime) qsub xyz.pbs Submit the job script xyz.pbs qstat <job id> Check the status of the job with given job ID qstat -u <username> Check the status of all jobs submitted by given username qstat -xf <job id> Check detailed information for job with given job ID qsub -q <queuename> xyz.pbs Submit to queue queuename qdel <job id> Delete the job (queued or running) with given job ID qpeek <job id> \"Peek\" at the standard output from a running job qdel -Wforce <job id> Use when job not responding to just qdel For more details and more advanced commands for submitting and controlling jobs, please refer to the PBS Professional User's Guide . PBS job options The following switches can be used either with qsub on the command line, or with a #PBS directive in a batch script. Parameter Purpose Example -N Job name (7 characters) -N maxrun1 -l Job limits (lowercase L), hardware & other requirements for job. -l select=1:ncpus=8:mem=1gb -q Queue to direct this job to ( workq is the default, supabad is an example of specific research group's job queue) -q supabad -o Path to stdout file for this job (environment variables are not accepted here) -o stdout.txt -e Path to stderr file for this job (environment variables are not accepted here) -e stderr.txt -m mail event: Email from the PBS server with flag a bort\\ b egin\\ e nd \\ or n o mail for job's notification. -m abe -M Specify list of user to whom mail about the job is sent. The user list argument is of the form: [user[@host],user[@host],...]. If -M is not used and -m is specified, PBS will send email to userid@clemson.edu -M user1@domain1.com,user2@domain2.com -j oe Join the output and error streams and write to a single file -j oe -r n Ask PBS not to restart the job if it's failed -r n For example, in a batch script: #PBS -N hydrogen #PBS -l select=1:ncpus=24:mem=200gb,walltime=4:00:00 #PBS -q bigmem #PBS -m abe #PBS -M userid@domain.com #PBS -j oe And in an interactive job request on the command line: $ qsub -I -N hydrogen -q bigmem -j oe -l select=1:ncpus=24:mem=200gb,walltime=4:00:00 For more detailed information, please take a look at: $man qsub Resource limits specification The -l switch provided to qsub or along with the #PBS directive can be used to specify the amount and kind of compute hardware (cores, memory, GPUs, interconnect, etc.,), its location, i.e., the node(s) and phase from which to request hardware, Option Purpose select Number of chunks and resources per chunk. Two or more \"chunks\" can be placed on a single node, but a single \"chunk\" cannot span more than one node. walltime Expected wall time of job (job is terminated after this time) place Controls the placement of the different chunks Here are some examples of resource limits specification: -l select=1:ncpus=8:chip_model=opteron:interconnect=10g -l select=1:ncpus=16:chip_type=e5-2665:interconnect=56g:mem=62gb,walltime=16:00:00 -l select=1:ncpus=8:chip_type=2356:interconnect=10g:mem=15gb -l select=1:ncpus=1:node_manufacturer=ibm:mem=15gb,walltime=00:20:00 -l select=1:ncpus=4:mem=15gb:ngpus=2,walltime=00:20:00 -l select=1:ncpus=4:mem=15gb:ngpus=1:gpu_model=k40,walltime=00:20:00 -l select=1:ncpus=2:mem=15gb:host=node1479,walltime=00:20:00 -l select=2:ncpus=2:mem=15gb,walltime=00:20:00,place=scatter # force each chunk to be on a different node -l select=2:ncpus=2:mem=15gb,walltime=00:20:00,place=pack # force each chunk to be on the same node and examples of options you can use in the job limit specification: chip_manufacturer=amd chip_manufacturer=intel chip_model=opteron chip_model=xeon chip_type=e5345 chip_type=e5410 chip_type=l5420 chip_type=x7542 chip_type=2356 chip_type=6172 chip_type=e5-2665 node_manufacturer=dell node_manufacturer=hp node_manufacturer=ibm node_manufacturer=sun gpu_model=k20 gpu_model=k40 interconnect=1g (1 Gbps Ethernet) interconnect=10ge (10 Gbps Ethernet) interconnect=56g (56 Gbps FDR InfiniBand, same as fdr) interconnect=fdr (56 Gbps FDR InfiniBand, same as 56g) ssd=true (Use a node with an SSD hard drive) Querying job information and deleting jobs The qstat command can be used to query the status of a particular job: $ qstat 7600424.pbs02 Job id Name User Time Use S Queue ---------------- ---------------- ---------------- -------- - ----- 7600424.pbs02 pi-mpi username 00:00:00 R c1_single To list the job IDs and status of all your jobs, you can use the -u switch: $ qstat -u username pbs02: Req'd Req'd Elap Job ID Username Queue Jobname SessID NDS TSK Memory Time S Time --------------- -------- -------- ---------- ------ --- --- ------ ----- - ----- 7600567.pbs02 username c1_singl pi-mpi-1 1382 4 8 4gb 00:05 R 00:00 7600569.pbs02 username c1_singl pi-mpi-2 20258 4 8 4gb 00:05 R 00:00 7600570.pbs02 username c1_singl pi-mpi-3 2457 4 8 4gb 00:05 R 00:00 Once a job has finished running, qstat -xf can be used to obtain detailed job information: $ qstat -xf 7600424.pbs02 Job Id: 7600424.pbs02 Job_Name = pi-mpi Job_Owner = username@login001.palmetto.clemson.edu resources_used.cpupercent = 103 resources_used.cput = 00:00:04 resources_used.mem = 45460kb resources_used.ncpus = 8 resources_used.vmem = 785708kb resources_used.walltime = 00:02:08 job_state = F queue = c1_single server = pbs02 Checkpoint = u ctime = Tue Dec 13 14:09:32 2016 Error_Path = login001.palmetto.clemson.edu:/home/username/MPI/pi-mpi.e7600424 exec_host = node0088/1*2+node0094/1*2+node0094/2*2+node0085/0*2 exec_vnode = (node0088:ncpus=2:mem=1048576kb:ngpus=0:nphis=0)+(node0094:ncp us=2:mem=1048576kb:ngpus=0:nphis=0)+(node0094:ncpus=2:mem=1048576kb:ngp us=0:nphis=0)+(node0085:ncpus=2:mem=1048576kb:ngpus=0:nphis=0) Hold_Types = n Join_Path = oe Keep_Files = n Mail_Points = a Mail_Users = username@clemson.edu mtime = Tue Dec 13 14:11:42 2016 Output_Path = login001.palmetto.clemson.edu:/home/username/MPI/pi-mpi.o760042 4 Priority = 0 qtime = Tue Dec 13 14:09:32 2016 Rerunable = True Resource_List.mem = 4gb Resource_List.mpiprocs = 8 Resource_List.ncpus = 8 Resource_List.ngpus = 0 Resource_List.nodect = 4 Resource_List.nphis = 0 Resource_List.place = free:shared Resource_List.qcat = c1_workq_qcat Resource_List.select = 4:ncpus=2:mem=1gb:interconnect=1g:mpiprocs=2 Resource_List.walltime = 00:05:00 stime = Tue Dec 13 14:09:33 2016 session_id = 2708 jobdir = /home/username substate = 92 Variable_List = PBS_O_SYSTEM=Linux,PBS_O_SHELL=/bin/bash, PBS_O_HOME=/home/username,PBS_O_LOGNAME=username, PBS_O_WORKDIR=/home/username/MPI,PBS_O_LANG=en_US.UTF-8, PBS_O_PATH=/software/examples/:/home/username/local/bin:/usr/lib64/qt-3 .3/bin:/opt/pbs/default/bin:/opt/gold/bin:/usr/local/bin:/bin:/usr/bin: /usr/local/sbin:/usr/sbin:/sbin:/opt/mx/bin:/home/username/bin, PBS_O_MAIL=/var/spool/mail/username,PBS_O_QUEUE=c1_workq, PBS_O_HOST=login001.palmetto.clemson.edu comment = Job run at Tue Dec 13 at 14:09 on (node0088:ncpus=2:mem=1048576kb :ngpus=0:nphis=0)+(node0094:ncpus=2:mem=1048576kb:ngpus=0:nphis=0)+(nod e0094:ncpus=2:mem=1048576kb:ngpus=0:nphis=0)+(node0085:ncpus=2:mem=1048 576kb:ngpus=0:nphis=0) and finished etime = Tue Dec 13 14:09:32 2016 run_count = 1 Stageout_status = 1 Exit_status = 0 Submit_arguments = job.sh history_timestamp = 1481656302 project = _pbs_project_default Similarly, to get detailed information about a running job, you can use qstat -f . To delete a job (whether in queued, running or error status), you can use the qdel command. $ qdel 7600424.pbs02 Job limits on Palmetto Walltime Jobs running in phases 1-6 of the cluster (nodes with interconnect 1g ) can run for a maximum walltime of 168 hours (7 days). Job running in phases 7 and higher of the cluster can run for a maximum walltime of 72 hours (3 days). Number of jobs When you submit a job, it is forwarded to a specific execution queue based on job critera (how many cores, RAM, etc.). There are three classes of execution queues: MX queues ( c1_ queues): jobs submitted to run on the older hardware (phases 1-6) will be forwarded to theses queues. IB queues ( c2_ queues): jobs submitted to run the newer hardware (phases 7 and up) will be forwarded to these queues. GPU queues ( gpu_ queues): jobs that request GPUs will be forwarded to these queues. bigmem queue: jobs submitted to the large-memory machines (phase 0). Each execution queue has its own limits for how many jobs can be running at one time, and how many jobs can be waiting in that execution queue. The maximum number of running jobs per user in execution queues may vary throughout the day depending on cluster load. Users can see what the current limits are using the checkqueuecfg command: $ checkqueuecfg MX QUEUES min_cores_per_job max_cores_per_job max_mem_per_queue max_jobs_per_queue max_walltime c1_solo 1 1 4000gb 2000 168:00:00 c1_single 2 24 90000gb 750 168:00:00 c1_tiny 25 128 25600gb 25 168:00:00 c1_small 129 512 24576gb 6 168:00:00 c1_medium 513 2048 81920gb 5 168:00:00 c1_large 2049 4096 32768gb 1 168:00:00 IB QUEUES min_cores_per_job max_cores_per_job max_mem_per_queue max_jobs_per_queue max_walltime c2_single 1 24 600gb 5 72:00:00 c2_tiny 25 128 4096gb 2 72:00:00 c2_small 129 512 6144gb 1 72:00:00 c2_medium 513 2048 16384gb 1 72:00:00 c2_large 2049 4096 0gb 0 72:00:00 GPU QUEUES min_gpus_per_job max_gpus_per_job min_cores_per_job max_cores_per_job max_mem_per_queue max_jobs_per_queue max_walltime gpu_small 1 4 1 96 3840gb 20 72:00:00 gpu_medium 5 16 1 256 5120gb 5 72:00:00 gpu_large 17 128 1 1024 20480gb 5 72:00:00 SMP QUEUE min_cores max_cores max_jobs max_walltime bigmem 1 64 3 72:00:00 'max_mem' is the maximum amount of memory all your jobs in this queue can consume at any one time. For example, if the max_mem for the solo queue is 4000gb, and your solo jobs each need 10gb, then you can run a maximum number of 4000/10 = 400 jobs in the solo queue, even though the current max_jobs setting for the solo queue may be set higher than 400. The qstat command tells you which of the execution queues your job is forwarded to. For example, here is an interactive job requesting 8 CPU cores, a K40 GPU, and 32gb RAM: $ qsub -I -l select=1:ncpus=8:ngpus=1:gpu_model=k40:mem=32gb,walltime=2:00:00 qsub (Warning): Interactive jobs will be treated as not rerunnable qsub: waiting for job 9567792.pbs02 to start We see from qstat that the job request is forward to the c2_single queue: [username@login001 ~]$ qstat 9567792.pbs02 Job id Name User Time Use S Queue ---------------- ---------------- ---------------- -------- - ----- 9567792.pbs02 STDIN username 0 Q c2_single From the output of checkqueuecfg above, we see that each user can have a maximum of 5 running jobs in this queue. Example PBS scripts A list of example PBS scripts for submitting jobs to the Palmetto cluster can be found here .","title":"Login"},{"location":"basic/login/#logging-in","text":"Any user with a Palmetto Cluster account can log-in using SSH (Secure Shell) . Mac OS X and Linux systems come with an SSH client installed, while Windows users will need to download one.","title":"Logging in"},{"location":"basic/login/#mac-os-x-and-linux-users","text":"Mac OS X or Linux users may open a Terminal, and type in the following command: $ ssh username@login.palmetto.clemson.edu where username is your Clemson user ID. You will be prompted for both your password and DUO authentication.","title":"Mac OS X and Linux users"},{"location":"basic/login/#windows","text":"MobaXterm is the recommended SSH client for Windows and can be downloaded from MobaXterm's download page . This software is recommended because it is free and comes with: A built-in file transfer client, which allows you to exchange files and folders between your own computer and Palmetto in a convenient manner. An X11 server which allows you to run graphical programs on Palmetto cluster A graphical port-forwarding interface to support easy access to web-based programs launched inside Palmetto If you select the installation version of MobaXterm, you will need to unzip the downloaded file before running the installation program. Windows sometimes allow users to run the installation from inside the zipped file, resulting in missing an additional utility file (still inside the zipped file). After downloading and installing MobaXterm, users can log-in by following these steps: Launch the MobaXterm program On the top-left corner of MobaXterm, click the Session button. Confirm that the following settings are set for Basic SSH settings and Advanced SSH settings : Parameter Value Remote host login.palmetto.clemson.edu Port 22 X11-Forwarding enabled Compression enabled Remote environment Interactive shell SSH-browser type SCP (enhanced speed) Click OK and a new session window will be opened, where you will be prompted for your Palmetto password and the DUO authentication. After being authenticated, you will login to the login001 node. All settings for this session are saved, and for future logins, you can select this session from the Recent sessions form of the main MobaXterm window as well as the Saved sessions tab of the side window. The side window can be displayed or hidden by clicking on the blue double-arrow sign on the top left of MobaXterm. MobaXterm also comes with a built-in file browser and transfer GUI (SSH-browser). This GUI is accessible via the SCP tab of the side window. Using the Upload (green arrow pointing up) and and Download (blue arrow pointing down) buttons at the top of the SCP tab, you can easily transfer files between Palmetto and your local computer.","title":"Windows"},{"location":"basic/login/#two-factor-authentication-2fa","text":"All connections to Palmetto require 2FA. If you are not enrolled in 2FA yet, you may enroll using the link https://2fa.clemson.edu/ . After you enter your login name and password, Palmetto will ask you to provide additional authentication for 2FA via one of the following three options for registerd devices (smart phone or tablet): Using keyboard-interactive authentication. Duo two-factor login for $user Enter a passcode or select one of the following options: 1. Duo Push to XXX-XXX-XXXX 2. Phone call to XXX-XXX-XXXX 3. SMS passcodes to XXX-XXX-XXXX Passcode or option (1-3): Option 1: response to Duo Push to your device by clicking Approve Option 2: listen to the automatic call from system and select any key on your device. Option 3: enter a passcode that is shown in your DUO app. Passcode or option (1-3): 1234567 More information can be found at here","title":"Two-Factor Authentication (2FA)"},{"location":"basic/login/#basic-tasks","text":"","title":"Basic tasks"},{"location":"basic/login/#storing-files-and-folders","text":"","title":"Storing files and folders"},{"location":"basic/login/#home-and-scratch-directories","text":"Various filesystems are available for users to store data. These differ in capacity, data-persistence, and efficiency, and it is important that users understand which filesystem to use under which circumstances. Location Available space Notes /home/username 100 GB per user Backed-up nightly, permanent storage space accessible from all nodes /scratch1/username 233 TB shared by all users Not backed up, temporary work space accessible from all nodes, OrangeFS Parallel File System /scratch2/username 160 TB shared by all users Not backed up, temporary work space accessible from all nodes, XFS /scratch3/username 129 TB shared by all users Not backed up, temporary work space accessible from all nodes, ZFS /scratch4/username 175 TB shared by all users Not backed up, temporary work space accessible from all nodes, BeeGFS Parallel File System /local_scratch Varies between nodes (99GB-800GB) Per-node temporary work space, accessible only for the lifetime of job The /home and /scratch directories are shared by all nodes. In contrast, each node has its own /local_scratch directory. All data in the /home directory is permanent (not automatically deleted) and backed-up on a nightly basis. If you lose data in the /home directory, it may be possible to recover it if it was previously backed up. Data in the /scratch directories is not backed up, and any data that is untouched for 30 days is automatically removed from the /scratch directories. Data that cannot easily be reproduced should not be stored in the /scratch directories, and any data that is not required should be removed as soon as possible. See this guide on how to choose the appropriate filesystem for your work.","title":"Home and scratch directories"},{"location":"basic/login/#temporary-reservations","text":"All users may apply for temporary reservation of the following resources: Up to 150 TB of long-term storage Up to 8.5 TB of fast SSD scratch space All requests will be reviewed by Clemson University Computational Advisory Team (CU-CAT). Reservation requests can be made here .","title":"Temporary reservations"},{"location":"basic/login/#purchased-storage","text":"Users or groups may also purchase storage in 1 TB increments. For details about purchased storage, please contact the Palmetto support staff ( ithelp@clemson.edu , and include the word \"Palmetto\" in the subject line).","title":"Purchased storage"},{"location":"basic/login/#moving-data-in-and-out-of-the-cluster","text":"","title":"Moving data in and out of the cluster"},{"location":"basic/login/#small-files-kilobytes-or-a-few-megabytes","text":"On Windows machines, using the MobaXterm SSH client, the built-in file browser can be used ( SCP tab of the side window). Using the Upload (green arrow pointing up) and and Dowload (blue arrow pointing down) buttons at the top of the SCP tab, you can easily transfer small files between Palmetto and your local computer. On Unix systems, you can use the scp (secure copy) command to perform file transfers. The general form of the scp command is: $ scp <path_to_source> username@xfer01-ext.palmetto.clemson.edu:<path_to_destination> For example, here is the scp command to copy a file from the current directory on your local machine to your /home/username directory on Palmetto (this command is entered into a terminal when not logged-in to Palmetto ): $ scp myfile.txt username@xfer01-ext.palmetto.clemson.edu:/home/username ... and to do the same in reverse, i.e., copy from Palmetto to your local machine. (again, from a terminal running on your local machine, not on Palmetto): $ scp username@xfer01-ext.palmetto.clemson.edu:/home/username/myfile.txt . The . represents the working directory on the local machine. For folders, include the -r switch: $ scp -r myfolder username@xfer01-ext.palmetto.clemson.edu:/home/username","title":"Small files (kilobytes or a few megabytes)"},{"location":"basic/login/#transfering-larger-files-more-than-a-few-megabytes","text":"For larger files, we recommend using the Globus file transfer application. Here, we demonstrate how to use Globus Online to transfer files between Palmetto and a local machine (laptop). However, Globus can be used for file transfers to/from other locations as well. You will need to have a Globus account set up to begin. Visit https://www.globus.org/ and set up a Globus account. To begin transfering files, navigate to the Globus Online transfer utility here: https://www.globus.org/app/transfer . The transfer utility allows you to transfer files between \"endpoints\". You will need to set your local machine as a Globus Connect Personal Endpoint for the file transfer. As a part of this step, you must install the Globus Connect Personal application (see here: https://www.globus.org/app/endpoints/create-gcp ). After installing, ensure that the application is running. You should then be able to set your local machine as one endpoint. In the figure below, the endpoint is named My Personal Mac . As the second endpoint, choose clemson#xfer01-ext.clemson.edu . You can now transfer files between any locations on your local machine and the Palmetto cluster.","title":"Transfering larger files (more than a few megabytes)"},{"location":"basic/login/#checking-available-compute-hardware","text":"The login node login001 (the node that users first log-in to), is not meant for running computationally intensive tasks. Instead, users must reserve hardware from the compute nodes of the cluster. Currently, Palmetto cluster has over 2020 compute nodes. The hardware configuration of the different nodes is available in the file /etc/hardware-table : [atrikut@login001 ~]$ cat /etc/hardware-table PALMETTO HARDWARE TABLE Last updated: Mar 04 2019 PHASE COUNT MAKE MODEL CHIP(0) CORES RAM(1) /local_scratch Interconnect GPUs PHIs SSD 0 6 HP DL580 Intel Xeon 7542 24 505 GB(2) 99 GB 1ge 0 0 0 0 1 HP DL980 Intel Xeon 7560 64 2 TB(2) 99 GB 1ge 0 0 0 0 1 HP DL560 Intel Xeon E5-4627v4 40 1.5 TB(2) 881 GB 10ge 0 0 0 0 1 Dell R830 Intel Xeon E5-4627v4 40 1.0 TB(2) 880 GB 10ge 0 0 0 0 2 HP DL560 Intel Xeon 6138G 80 1.5 TB(2) 3.6 TB 10ge 0 0 0 1 75 Dell PE1950 Intel Xeon E5345 8 12 GB 37 GB 1g 0 0 0 2a 158 Dell PE1950 Intel Xeon E5410 8 12 GB 37 GB 1g 0 0 0 2b 84 Dell PE1950 Intel Xeon E5410 8 16 GB 37 GB 1g 0 0 0 3 225 Sun X2200 AMD Opteron 2356 8 16 GB 193 GB 1g 0 0 0 4 326 IBM DX340 Intel Xeon E5410 8 16 GB 111 GB 1g 0 0 0 5a 320 Sun X6250 Intel Xeon L5420 8 32 GB 31 GB 1g 0 0 0 5b 9 Sun X4150 Intel Xeon E5410 8 32 GB 99 GB 1g 0 0 0 6 67 HP DL165 AMD Opteron 6176 24 48 GB 193 GB 1g 0 0 0 7a 42 HP SL230 Intel Xeon E5-2665 16 64 GB 240 GB 56g, fdr 0 0 0 7b 12 HP SL250s Intel Xeon E5-2665 16 64 GB 240 GB 56g, fdr 2(3) 0 0 8a 71 HP SL250s Intel Xeon E5-2665 16 64 GB 900 GB 56g, fdr 2(4) 0 300 GB(7) 8b 57 HP SL250s Intel Xeon E5-2665 16 64 GB 420 GB 56g, fdr 2(4) 0 0 8c 88 Dell PEC6220 Intel Xeon E5-2665 16 64 GB 350 GB 10ge 0 0 0 9 72 HP SL250s Intel Xeon E5-2665 16 128 GB 420 GB 56g, fdr, 10ge 2(4) 0 0 10 80 HP SL250s Intel Xeon E5-2670v2 20 128 GB 800 GB 56g, fdr, 10ge 2(4) 0 0 11a 40 HP SL250s Intel Xeon E5-2670v2 20 128 GB 800 GB 56g, fdr, 10ge 2(6) 0 0 11b 4 HP SL250s Intel Xeon E5-2670v2 20 128 GB 800 GB 56g, fdr, 10ge 0 2(8) 0 12 30 Lenovo NX360M5 Intel Xeon E5-2680v3 24 128 GB 800 GB 56g, fdr, 10ge 2(6) 0 0 13 24 Dell C4130 Intel Xeon E5-2680v3 24 128 GB 1.8 TB 56g, fdr, 10ge 2(6) 0 0 14 12 HP XL1X0R Intel Xeon E5-2680v3 24 128 GB 880 GB 56g, fdr, 10ge 2(6) 0 0 15 32 Dell C4130 Intel Xeon E5-2680v3 24 128 GB 1.8 TB 56g, fdr, 10ge 2(6) 0 0 16 40 Dell C4130 Intel Xeon E5-2680v4 28 128 GB 1.8 TB 56g, fdr, 10ge 2(9) 0 0 17 20 Dell C4130 Intel Xeon E5-2680v4 28 128 GB 1.8 TB 56g, fdr, 10ge 2(9) 0 0 18a 2 Dell C4140 Intel Xeon 6148G 40 372 GB 1.9 TB(12) 56g, fdr, 25ge 4(10) 0 0 18b 65 Dell R740 Intel Xeon 6148G 40 372 GB 1.8 TB 56g, fdr, 25ge 2(11) 0 0 18c 10 Dell R740 Intel Xeon 6148G 40 748 GB 1.8 TB 56g, fdr, 25ge 2(11) 0 0 *** PBS resource requests are always lowercase *** (0) CHIP has 3 resources: chip_manufacturer, chip_model, chip_type (1) Leave 2 or 3GB for the operating system when requesting memory in PBS jobs (2) Specify queue \"bigmem\" to access the large memory machines, only ncpus and mem are valid PBS resource requests (3) 2 NVIDIA Tesla M2075 cards per node, use resource request \"ngpus=[1|2]\" and \"gpu_model=m2075\" (4) 2 NVIDIA Tesla K20m cards per node, use resource request \"ngpus=[1|2]\" and \"gpu_model=k20\" (5) 2 NVIDIA Tesla M2070-Q cards per node, use resource request \"ngpus=[1|2]\" and \"gpu_model=m2070q\" (6) 2 NVIDIA Tesla K40m cards per node, use resource request \"ngpus=[1|2]\" and \"gpu_model=k40\" (7) Use resource request \"ssd=true\" to request a chunk with SSD in location /ssd1, /ssd2, and /ssd3 (100GB max each) (8) Use resource request \"nphis=[1|2]\" to request phi nodes, the model is Xeon 7120p (9) 2 NVIDIA Tesla P100 cards per node, use resource request \"ngpus=[1|2]\" and \"gpu_model=p100\" (10)4 NVIDIA Tesla V100 cards per node with NVLINK2, use resource request \"ngpus=[1|2|3|4]\" and \"gpu_model=v100nv\" (11)2 NVIDIA Tesla V100 cards per node, use resource request \"ngpus=[1|2]\" and \"gpu_model=v100\" (12)Phase18a nodes contain only NVMe storage for local_scratch. The compute nodes are divided into \"phases\" (currently phases 0-18). Each phase is composed of several nodes with identical configuration, e.g., each node in phase 5a has 8 cores, 32 GB ram, 31 GB local disk space, and 10 Gbps Myrinet interconnect. A useful command on the login node is whatsfree , which gives information about how many nodes from each phase are currently in use, free, or offlined for maintenance. Later sections of this guide will describe how to submit jobs to the cluster, i.e., reserve compute nodes for running computational tasks.","title":"Checking available compute hardware"},{"location":"basic/login/#checking-and-using-available-software","text":"The Palmetto cluster provides a limited number of packages (including site-licensed packages), that can be used by all Palmetto users. These packages are available as modules , and must be activated/deactivated using the module command: Command Purpose module avail List all packages available (on current system) module add package/version Add a package to your current shell environment module list List packages you have loaded module rm package/version Remove a currently loaded package module purge Remove all currently loaded packages For example, to load the GCC (v4.8.1), CUDA Toolkit (v6.5.14) and OpenMPI (v1.8.4) modules, you can use the command: $ module add gcc/4.8.1 cuda-toolkit/6.5.14 openmpi/1.8.4 Then, check the version of gcc : $ gcc --version gcc (GCC) 4.8.1 Copyright (C) 2013 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Some modules when loaded, implicitly load other modules as well. If you use some modules to compile/install some software, then you will probably have to load them when running that software as well, otherwise you may see errors about missing libraries/headers. Modules do not remain loaded when you log out and log back in, i.e., they are active only for the current session - so you will need to load them for every session. As an exercise, examine the environment variables PATH, LIBRARY_PATH, etc., before and after loading some module: $ echo $PATH $ module add python/2.7.13 $ echo $PATH You can also look at the modulefiles in /software/modulefiles to understand what happens when you add a module.","title":"Checking and using available software"},{"location":"basic/login/#start-an-interactive-job","text":"An interactive job can be started using the qsub command. Here is an example of an interactive job: [username@login001 ~]$ qsub -I -l select=1:ncpus=2:mem=4gb,walltime=4:00:00 qsub (Warning): Interactive jobs will be treated as not rerunnable qsub: waiting for job 8730.pbs02 to start qsub: job 8730.pbs02 ready [username@node0021 ~]$ module add python/3.4 [username@node0021 ~]$ python runsim.py . . . [username@node0021 ~]$ exit [username@login001 ~]$ Above, we request an interactive job using 1 \"chunk\" of hardware ( select=1 ), 2 CPU cores per \"chunk\", and 4gb of RAM per \"chunk\", for a wall time of 4 hours. Once these resources are available, we receive a Job ID ( 8730.pbs02 ), and a command-line session running on node0021 .","title":"Start an interactive job"},{"location":"basic/login/#submit-a-batch-job","text":"Interactive jobs require you to be logged-in while your tasks are running. In contrast, you may logout after submitting a batch job, and examine the results at a later time. This is useful when you need to run several computational tasks to the cluster, and/or when your computational tasks are expected to run for a long time. To submit a batch job, you must prepare a batch script (you can do this using an editor like vim or nano ). Following is an example of a batch script (call it example.pbs ). In the batch job below, we really don't do anything useful (just sleep or \"do nothing\" for 60 seconds), #PBS -N example #PBS -l select=1:ncpus=1:mem=2gb,walltime=00:10:00 module add gcc/4.8.1 cd /home/username echo Hello World from `hostname` sleep 60 After saving the above file, you can submit the batch job using qsub : [username@login001 ~]$ qsub example.pbs 8738.pbs02 The returned job ID can be used to query the status of the job (using qstat ) (or delete it using qdel ): [username@login001 ~]$ qstat 8738.pbs02 Job id Name User Time Use S Queue ---------------- ---------------- ---------------- -------- - ----- 8738.pbs02 example username 00:00:00 R c1_solo Once the job is completed, you will see the files example.o8738 (containing output if any) and example.e8738 (containing errors if any) from your job. [username@login001 ~]$ cat example.o8738 Hello World from node0230.palmetto.clemson.edu","title":"Submit a batch job"},{"location":"basic/login/#job-submission-and-control-on-palmetto","text":"The Palmetto cluster uses the Portable Batch Scheduling system (PBS) to manage jobs. Here are some basic PBS commands for submitting, querying and deleting jobs: Command Action qsub -I Submit an interactive job (reserves 1 core, 1gb RAM, 30 minutes walltime) qsub xyz.pbs Submit the job script xyz.pbs qstat <job id> Check the status of the job with given job ID qstat -u <username> Check the status of all jobs submitted by given username qstat -xf <job id> Check detailed information for job with given job ID qsub -q <queuename> xyz.pbs Submit to queue queuename qdel <job id> Delete the job (queued or running) with given job ID qpeek <job id> \"Peek\" at the standard output from a running job qdel -Wforce <job id> Use when job not responding to just qdel For more details and more advanced commands for submitting and controlling jobs, please refer to the PBS Professional User's Guide .","title":"Job submission and control on Palmetto"},{"location":"basic/login/#pbs-job-options","text":"The following switches can be used either with qsub on the command line, or with a #PBS directive in a batch script. Parameter Purpose Example -N Job name (7 characters) -N maxrun1 -l Job limits (lowercase L), hardware & other requirements for job. -l select=1:ncpus=8:mem=1gb -q Queue to direct this job to ( workq is the default, supabad is an example of specific research group's job queue) -q supabad -o Path to stdout file for this job (environment variables are not accepted here) -o stdout.txt -e Path to stderr file for this job (environment variables are not accepted here) -e stderr.txt -m mail event: Email from the PBS server with flag a bort\\ b egin\\ e nd \\ or n o mail for job's notification. -m abe -M Specify list of user to whom mail about the job is sent. The user list argument is of the form: [user[@host],user[@host],...]. If -M is not used and -m is specified, PBS will send email to userid@clemson.edu -M user1@domain1.com,user2@domain2.com -j oe Join the output and error streams and write to a single file -j oe -r n Ask PBS not to restart the job if it's failed -r n For example, in a batch script: #PBS -N hydrogen #PBS -l select=1:ncpus=24:mem=200gb,walltime=4:00:00 #PBS -q bigmem #PBS -m abe #PBS -M userid@domain.com #PBS -j oe And in an interactive job request on the command line: $ qsub -I -N hydrogen -q bigmem -j oe -l select=1:ncpus=24:mem=200gb,walltime=4:00:00 For more detailed information, please take a look at: $man qsub","title":"PBS job options"},{"location":"basic/login/#resource-limits-specification","text":"The -l switch provided to qsub or along with the #PBS directive can be used to specify the amount and kind of compute hardware (cores, memory, GPUs, interconnect, etc.,), its location, i.e., the node(s) and phase from which to request hardware, Option Purpose select Number of chunks and resources per chunk. Two or more \"chunks\" can be placed on a single node, but a single \"chunk\" cannot span more than one node. walltime Expected wall time of job (job is terminated after this time) place Controls the placement of the different chunks Here are some examples of resource limits specification: -l select=1:ncpus=8:chip_model=opteron:interconnect=10g -l select=1:ncpus=16:chip_type=e5-2665:interconnect=56g:mem=62gb,walltime=16:00:00 -l select=1:ncpus=8:chip_type=2356:interconnect=10g:mem=15gb -l select=1:ncpus=1:node_manufacturer=ibm:mem=15gb,walltime=00:20:00 -l select=1:ncpus=4:mem=15gb:ngpus=2,walltime=00:20:00 -l select=1:ncpus=4:mem=15gb:ngpus=1:gpu_model=k40,walltime=00:20:00 -l select=1:ncpus=2:mem=15gb:host=node1479,walltime=00:20:00 -l select=2:ncpus=2:mem=15gb,walltime=00:20:00,place=scatter # force each chunk to be on a different node -l select=2:ncpus=2:mem=15gb,walltime=00:20:00,place=pack # force each chunk to be on the same node and examples of options you can use in the job limit specification: chip_manufacturer=amd chip_manufacturer=intel chip_model=opteron chip_model=xeon chip_type=e5345 chip_type=e5410 chip_type=l5420 chip_type=x7542 chip_type=2356 chip_type=6172 chip_type=e5-2665 node_manufacturer=dell node_manufacturer=hp node_manufacturer=ibm node_manufacturer=sun gpu_model=k20 gpu_model=k40 interconnect=1g (1 Gbps Ethernet) interconnect=10ge (10 Gbps Ethernet) interconnect=56g (56 Gbps FDR InfiniBand, same as fdr) interconnect=fdr (56 Gbps FDR InfiniBand, same as 56g) ssd=true (Use a node with an SSD hard drive)","title":"Resource limits specification"},{"location":"basic/login/#querying-job-information-and-deleting-jobs","text":"The qstat command can be used to query the status of a particular job: $ qstat 7600424.pbs02 Job id Name User Time Use S Queue ---------------- ---------------- ---------------- -------- - ----- 7600424.pbs02 pi-mpi username 00:00:00 R c1_single To list the job IDs and status of all your jobs, you can use the -u switch: $ qstat -u username pbs02: Req'd Req'd Elap Job ID Username Queue Jobname SessID NDS TSK Memory Time S Time --------------- -------- -------- ---------- ------ --- --- ------ ----- - ----- 7600567.pbs02 username c1_singl pi-mpi-1 1382 4 8 4gb 00:05 R 00:00 7600569.pbs02 username c1_singl pi-mpi-2 20258 4 8 4gb 00:05 R 00:00 7600570.pbs02 username c1_singl pi-mpi-3 2457 4 8 4gb 00:05 R 00:00 Once a job has finished running, qstat -xf can be used to obtain detailed job information: $ qstat -xf 7600424.pbs02 Job Id: 7600424.pbs02 Job_Name = pi-mpi Job_Owner = username@login001.palmetto.clemson.edu resources_used.cpupercent = 103 resources_used.cput = 00:00:04 resources_used.mem = 45460kb resources_used.ncpus = 8 resources_used.vmem = 785708kb resources_used.walltime = 00:02:08 job_state = F queue = c1_single server = pbs02 Checkpoint = u ctime = Tue Dec 13 14:09:32 2016 Error_Path = login001.palmetto.clemson.edu:/home/username/MPI/pi-mpi.e7600424 exec_host = node0088/1*2+node0094/1*2+node0094/2*2+node0085/0*2 exec_vnode = (node0088:ncpus=2:mem=1048576kb:ngpus=0:nphis=0)+(node0094:ncp us=2:mem=1048576kb:ngpus=0:nphis=0)+(node0094:ncpus=2:mem=1048576kb:ngp us=0:nphis=0)+(node0085:ncpus=2:mem=1048576kb:ngpus=0:nphis=0) Hold_Types = n Join_Path = oe Keep_Files = n Mail_Points = a Mail_Users = username@clemson.edu mtime = Tue Dec 13 14:11:42 2016 Output_Path = login001.palmetto.clemson.edu:/home/username/MPI/pi-mpi.o760042 4 Priority = 0 qtime = Tue Dec 13 14:09:32 2016 Rerunable = True Resource_List.mem = 4gb Resource_List.mpiprocs = 8 Resource_List.ncpus = 8 Resource_List.ngpus = 0 Resource_List.nodect = 4 Resource_List.nphis = 0 Resource_List.place = free:shared Resource_List.qcat = c1_workq_qcat Resource_List.select = 4:ncpus=2:mem=1gb:interconnect=1g:mpiprocs=2 Resource_List.walltime = 00:05:00 stime = Tue Dec 13 14:09:33 2016 session_id = 2708 jobdir = /home/username substate = 92 Variable_List = PBS_O_SYSTEM=Linux,PBS_O_SHELL=/bin/bash, PBS_O_HOME=/home/username,PBS_O_LOGNAME=username, PBS_O_WORKDIR=/home/username/MPI,PBS_O_LANG=en_US.UTF-8, PBS_O_PATH=/software/examples/:/home/username/local/bin:/usr/lib64/qt-3 .3/bin:/opt/pbs/default/bin:/opt/gold/bin:/usr/local/bin:/bin:/usr/bin: /usr/local/sbin:/usr/sbin:/sbin:/opt/mx/bin:/home/username/bin, PBS_O_MAIL=/var/spool/mail/username,PBS_O_QUEUE=c1_workq, PBS_O_HOST=login001.palmetto.clemson.edu comment = Job run at Tue Dec 13 at 14:09 on (node0088:ncpus=2:mem=1048576kb :ngpus=0:nphis=0)+(node0094:ncpus=2:mem=1048576kb:ngpus=0:nphis=0)+(nod e0094:ncpus=2:mem=1048576kb:ngpus=0:nphis=0)+(node0085:ncpus=2:mem=1048 576kb:ngpus=0:nphis=0) and finished etime = Tue Dec 13 14:09:32 2016 run_count = 1 Stageout_status = 1 Exit_status = 0 Submit_arguments = job.sh history_timestamp = 1481656302 project = _pbs_project_default Similarly, to get detailed information about a running job, you can use qstat -f . To delete a job (whether in queued, running or error status), you can use the qdel command. $ qdel 7600424.pbs02","title":"Querying job information and deleting jobs"},{"location":"basic/login/#job-limits-on-palmetto","text":"","title":"Job limits on Palmetto"},{"location":"basic/login/#walltime","text":"Jobs running in phases 1-6 of the cluster (nodes with interconnect 1g ) can run for a maximum walltime of 168 hours (7 days). Job running in phases 7 and higher of the cluster can run for a maximum walltime of 72 hours (3 days).","title":"Walltime"},{"location":"basic/login/#number-of-jobs","text":"When you submit a job, it is forwarded to a specific execution queue based on job critera (how many cores, RAM, etc.). There are three classes of execution queues: MX queues ( c1_ queues): jobs submitted to run on the older hardware (phases 1-6) will be forwarded to theses queues. IB queues ( c2_ queues): jobs submitted to run the newer hardware (phases 7 and up) will be forwarded to these queues. GPU queues ( gpu_ queues): jobs that request GPUs will be forwarded to these queues. bigmem queue: jobs submitted to the large-memory machines (phase 0). Each execution queue has its own limits for how many jobs can be running at one time, and how many jobs can be waiting in that execution queue. The maximum number of running jobs per user in execution queues may vary throughout the day depending on cluster load. Users can see what the current limits are using the checkqueuecfg command: $ checkqueuecfg MX QUEUES min_cores_per_job max_cores_per_job max_mem_per_queue max_jobs_per_queue max_walltime c1_solo 1 1 4000gb 2000 168:00:00 c1_single 2 24 90000gb 750 168:00:00 c1_tiny 25 128 25600gb 25 168:00:00 c1_small 129 512 24576gb 6 168:00:00 c1_medium 513 2048 81920gb 5 168:00:00 c1_large 2049 4096 32768gb 1 168:00:00 IB QUEUES min_cores_per_job max_cores_per_job max_mem_per_queue max_jobs_per_queue max_walltime c2_single 1 24 600gb 5 72:00:00 c2_tiny 25 128 4096gb 2 72:00:00 c2_small 129 512 6144gb 1 72:00:00 c2_medium 513 2048 16384gb 1 72:00:00 c2_large 2049 4096 0gb 0 72:00:00 GPU QUEUES min_gpus_per_job max_gpus_per_job min_cores_per_job max_cores_per_job max_mem_per_queue max_jobs_per_queue max_walltime gpu_small 1 4 1 96 3840gb 20 72:00:00 gpu_medium 5 16 1 256 5120gb 5 72:00:00 gpu_large 17 128 1 1024 20480gb 5 72:00:00 SMP QUEUE min_cores max_cores max_jobs max_walltime bigmem 1 64 3 72:00:00 'max_mem' is the maximum amount of memory all your jobs in this queue can consume at any one time. For example, if the max_mem for the solo queue is 4000gb, and your solo jobs each need 10gb, then you can run a maximum number of 4000/10 = 400 jobs in the solo queue, even though the current max_jobs setting for the solo queue may be set higher than 400. The qstat command tells you which of the execution queues your job is forwarded to. For example, here is an interactive job requesting 8 CPU cores, a K40 GPU, and 32gb RAM: $ qsub -I -l select=1:ncpus=8:ngpus=1:gpu_model=k40:mem=32gb,walltime=2:00:00 qsub (Warning): Interactive jobs will be treated as not rerunnable qsub: waiting for job 9567792.pbs02 to start We see from qstat that the job request is forward to the c2_single queue: [username@login001 ~]$ qstat 9567792.pbs02 Job id Name User Time Use S Queue ---------------- ---------------- ---------------- -------- - ----- 9567792.pbs02 STDIN username 0 Q c2_single From the output of checkqueuecfg above, we see that each user can have a maximum of 5 running jobs in this queue.","title":"Number of jobs"},{"location":"basic/login/#example-pbs-scripts","text":"A list of example PBS scripts for submitting jobs to the Palmetto cluster can be found here .","title":"Example PBS scripts"},{"location":"faq/common/","text":"Common problems/issues Program crashes on login node with message Killed When running commands or editing files on the login node, users may notice that their processes end abruptly with the error message Killed . Processes with names such as a.out , matlab , etc., are automatically killed on the login node because they may consume excessive computational resources. Unfortunately, this also means that benign processes, such as editing a file with the word matlab as part of its name could also be killed. Solution: Request an interactive session on a compute node ( qsub -I ), and then run the application/command. Home or scratch directories are sluggish or unresponsive The /home and /scratch directories can become slow/unresponsive when a user (or several users) read/write large amounts of data to these directories. When this happens, all users are affected as these filesystems are shared by all nodes of the cluster. To avoid this issue, keep in mind the following: Never use the /home directory as the working directory for jobs that read/write data. If too many jobs read/write data to the /home directory, it can render the cluster unusable by all users. Copy any input data to one of the /scratch directories and use that /scratch directory as the working directory for jobs. Periodically move important data back to the /home directory. Try to use /local_scratch whenever possible. Unlike /home or the /scratch directories, which are shared by all nodes, each node has its own /local_scratch directory. It is much faster to read/write data to /local_scratch , and doing so will not affect other users. (see example [here])(https://www.palmetto.clemson.edu/palmetto/userguide_howto_choose_right_filesystem.html).","title":"Common Issues"},{"location":"faq/common/#common-problemsissues","text":"","title":"Common problems/issues"},{"location":"faq/common/#program-crashes-on-login-node-with-message-killed","text":"When running commands or editing files on the login node, users may notice that their processes end abruptly with the error message Killed . Processes with names such as a.out , matlab , etc., are automatically killed on the login node because they may consume excessive computational resources. Unfortunately, this also means that benign processes, such as editing a file with the word matlab as part of its name could also be killed. Solution: Request an interactive session on a compute node ( qsub -I ), and then run the application/command.","title":"Program crashes on login node with message Killed"},{"location":"faq/common/#home-or-scratch-directories-are-sluggish-or-unresponsive","text":"The /home and /scratch directories can become slow/unresponsive when a user (or several users) read/write large amounts of data to these directories. When this happens, all users are affected as these filesystems are shared by all nodes of the cluster. To avoid this issue, keep in mind the following: Never use the /home directory as the working directory for jobs that read/write data. If too many jobs read/write data to the /home directory, it can render the cluster unusable by all users. Copy any input data to one of the /scratch directories and use that /scratch directory as the working directory for jobs. Periodically move important data back to the /home directory. Try to use /local_scratch whenever possible. Unlike /home or the /scratch directories, which are shared by all nodes, each node has its own /local_scratch directory. It is much faster to read/write data to /local_scratch , and doing so will not affect other users. (see example [here])(https://www.palmetto.clemson.edu/palmetto/userguide_howto_choose_right_filesystem.html).","title":"Home or scratch directories are sluggish or unresponsive"},{"location":"faq/faq/","text":"","title":"FAQ"},{"location":"software/programming/","text":"Available Compilers The Palmetto offers the following compiler suites for C, C++ and Fortran applications: GNU Compiler Collection ( gcc ) Intel Compiler Suite PGI Compiler Suite $ module avail gcc $ module avail intel $ module avail pgi Compiling \"Hello World\" Program C Simple hello.c file that reads {% highlight cpp %} include int main(void){ printf(\"Hello, world!\\n\"); return 0; } {% endhighlight %} can be compiled on Palmetto using the following commands Compiler Module Command Intel intel/13.0 icc hello.c -o hello.x GCC gcc/4.8.1 gcc hello.c -o hello.x PGI pgi/16.10 pgcc hello.c -o hello.x C++ The same example in C++ put in a file hello.cpp {% highlight cpp %} include int main() { std::cout << \"Hello, world!\" << std::endl; return 0; } {% endhighlight %} can be compiled on Palmetto using the following commands Compiler Module Command Intel intel/13.0 icpc hello.cpp -o hello.x GCC gcc/4.8.1 g++ hello.cpp -o hello.x PGI pgi/16.10 pgc++ hello.cpp -o hello.x FORTRAN To compile a simple FORTAN program test.f90 {% highlight fortran %} program test print *,'hello from fortran' end program test {% endhighlight %} use a command appropriate for a given compiler suite Compiler Module Command Intel intel/13.0 ifort test.f90 -o test.x GCC gcc/4.8.1 gfortran test.f90 -o test.x PGI pgi/16.10 pgfortran test.f90 -o hello.x GCC compiler options Switch Description -c Compile the source file but do not link -x language Set the specific language instead of letting the compiler decide based on the source file suffix. Useful for FORTRAN i.e. language can be replaced with f77 , f77-cpp-input , f95 or f95-cpp-input -o file Change the name of the binary file from a.out to file -v Print version of the compiler with options used for its configuration -fopenmp Enables OpenMP directives to create a multithreaded code -fno-gnu-keywords Turns off GNU specific language extensions -w Suspend add warnings -Wall Enables all warnings -g Enables debugging information -p or -pg Turns on gprof profiling information -ftree-vectorizer-verbose Enables verbose mode for GCC vectorization -O , -O1 , -O2 or -O3 Different levels of code optimization; -O3 is the most aggressive -Ofast -Ofast enables all -O3 optimizations. It also enables optimizations that are not valid for all standard-compliant programs -Og Turns on debugging safe optimization mode -floop-block Perform loop blocking transformations on loops -floop-interchange Perform loop interchange transformations on loops -ftree-vectorize Perform loop vectorization on trees. This flag is enabled by default at -O3 -funroll-loops Unroll loops whose number of iterations can be determined at compile time or upon entry to the loop -fprefetch-loop-arrays Generate instructions to prefetch memory to improve the performance of loops that access large arrays -ffast-math Use faster but less accurate mathematical functions -D Define a macro -I<dir> Add <dir> to compilers path for included files -l<lib> Pass a library <lib> to the linker -L<dir> Add directory <dir> to the linker library path -march=native This selects the CPU to generate code for at compilation time by determining the processor type of the compiling machine -ffree-form FORTRAN : Specify the layout used by the source file -cpp FORTRAN : Enable preprocessor for FORTRAN files -fno-underscoring FORTRAN : Do not transform names of entities specified in the Fortran source file by appending underscores to them -fexternal-blas FORTRAN : This option will make gfortran generate calls to BLAS functions for some matrix operations like \"MATMUL\" -floop-parallelize-all Identify loops that can be parallelized -ftree-parallelize-loops=n Parallelize loops, i.e., split their iteration space to run in n threads. This option implies -pthread More information GCC autoparallelization http://gcc.gnu.org/wiki/Graphite/Parallelization Intel compiler options Switch Description -parallel enable the auto-parallelizer to generate multi-threaded code for loops that can be safely executed in parallel -par-reportX (X=1,2,3) control the auto-parallelizer diagnostic level -openmp enable the compiler to generate multi-threaded code based on the OpenMP* directives -openmp-reportX (X=1,2,3) control the OpenMP parallelizer diagnostic level -fast enable -xHOST -O3 -ipo -no-prec-div -static options set by -fast cannot be overridden with the exception of -xHOST , list options separately to change behavior -O0 disable optimizations -O1 optimize for maximum speed, but disable some optimizations which increase code size for a small speed benefit -O2 optimize for maximum speed (DEFAULT) -O3 optimize for maximum speed and enable more aggressive optimizations that may not improve performance on some programs -threads specify that multi-threaded libraries should be linked against -nothreads disables multi-threaded libraries -vec enables (DEFAULT) vectorization ( -novec disables vectorization) -mkl link to the Math Kernel Library (MKL) and bring in the associated headers, -mkl= one of parallel (default), sequential or cluster -opt-matmul replace matrix multiplication with calls to intrinsics and threading libraries for improved performance (DEFAULT at -O3 -parallel ) -xHost generate instructions for the highest instruction set and processor available on the compilation host machine -integer-size XX specifies the default size of integer and logical variables size: 16, 32, 64 -real-size XX specify the size of REAL and COMPLEX declarations, constants, functions, and intrinsics size: 32, 64, 128 -w disable all warnings, equivalent to -warn none or -nowarn -warn all enables all warnings -L<dir> instruct linker to search <dir> for libraries -l<string> instruct the linker to link in the -l<string> library -Wl,<o1>[,<o2>,...] pass options o1 , o2 , etc. to the linker for processing -V display compiler version information -multiple-processes[=<n>] create multiple processes that can be used to compile large numbers of source files at the same time -c compile to object (.o) only, do not link -S compile to assembly (.s) only, do not link -o <file> name output executable file -g produce symbolic debug information in object file (implies -O0 when another optimization option is not explicitly set) -p compile and link for function profiling with UNIX gprof tool, -pg is also valid -D<name>[=<text>] define macro -I<dir> add directory to include file search path -fpp run Fortran preprocessor on source files prior to compilation","title":"Software Development"},{"location":"software/programming/#available-compilers","text":"The Palmetto offers the following compiler suites for C, C++ and Fortran applications: GNU Compiler Collection ( gcc ) Intel Compiler Suite PGI Compiler Suite $ module avail gcc $ module avail intel $ module avail pgi","title":"Available Compilers"},{"location":"software/programming/#compiling-hello-world-program","text":"","title":"Compiling \"Hello World\" Program"},{"location":"software/programming/#c","text":"Simple hello.c file that reads {% highlight cpp %}","title":"C"},{"location":"software/programming/#include","text":"int main(void){ printf(\"Hello, world!\\n\"); return 0; } {% endhighlight %} can be compiled on Palmetto using the following commands Compiler Module Command Intel intel/13.0 icc hello.c -o hello.x GCC gcc/4.8.1 gcc hello.c -o hello.x PGI pgi/16.10 pgcc hello.c -o hello.x","title":"include "},{"location":"software/programming/#c_1","text":"The same example in C++ put in a file hello.cpp {% highlight cpp %}","title":"C++"},{"location":"software/programming/#include_1","text":"int main() { std::cout << \"Hello, world!\" << std::endl; return 0; } {% endhighlight %} can be compiled on Palmetto using the following commands Compiler Module Command Intel intel/13.0 icpc hello.cpp -o hello.x GCC gcc/4.8.1 g++ hello.cpp -o hello.x PGI pgi/16.10 pgc++ hello.cpp -o hello.x","title":"include "},{"location":"software/programming/#fortran","text":"To compile a simple FORTAN program test.f90 {% highlight fortran %} program test print *,'hello from fortran' end program test {% endhighlight %} use a command appropriate for a given compiler suite Compiler Module Command Intel intel/13.0 ifort test.f90 -o test.x GCC gcc/4.8.1 gfortran test.f90 -o test.x PGI pgi/16.10 pgfortran test.f90 -o hello.x","title":"FORTRAN"},{"location":"software/programming/#gcc-compiler-options","text":"Switch Description -c Compile the source file but do not link -x language Set the specific language instead of letting the compiler decide based on the source file suffix. Useful for FORTRAN i.e. language can be replaced with f77 , f77-cpp-input , f95 or f95-cpp-input -o file Change the name of the binary file from a.out to file -v Print version of the compiler with options used for its configuration -fopenmp Enables OpenMP directives to create a multithreaded code -fno-gnu-keywords Turns off GNU specific language extensions -w Suspend add warnings -Wall Enables all warnings -g Enables debugging information -p or -pg Turns on gprof profiling information -ftree-vectorizer-verbose Enables verbose mode for GCC vectorization -O , -O1 , -O2 or -O3 Different levels of code optimization; -O3 is the most aggressive -Ofast -Ofast enables all -O3 optimizations. It also enables optimizations that are not valid for all standard-compliant programs -Og Turns on debugging safe optimization mode -floop-block Perform loop blocking transformations on loops -floop-interchange Perform loop interchange transformations on loops -ftree-vectorize Perform loop vectorization on trees. This flag is enabled by default at -O3 -funroll-loops Unroll loops whose number of iterations can be determined at compile time or upon entry to the loop -fprefetch-loop-arrays Generate instructions to prefetch memory to improve the performance of loops that access large arrays -ffast-math Use faster but less accurate mathematical functions -D Define a macro -I<dir> Add <dir> to compilers path for included files -l<lib> Pass a library <lib> to the linker -L<dir> Add directory <dir> to the linker library path -march=native This selects the CPU to generate code for at compilation time by determining the processor type of the compiling machine -ffree-form FORTRAN : Specify the layout used by the source file -cpp FORTRAN : Enable preprocessor for FORTRAN files -fno-underscoring FORTRAN : Do not transform names of entities specified in the Fortran source file by appending underscores to them -fexternal-blas FORTRAN : This option will make gfortran generate calls to BLAS functions for some matrix operations like \"MATMUL\" -floop-parallelize-all Identify loops that can be parallelized -ftree-parallelize-loops=n Parallelize loops, i.e., split their iteration space to run in n threads. This option implies -pthread More information GCC autoparallelization http://gcc.gnu.org/wiki/Graphite/Parallelization","title":"GCC compiler options"},{"location":"software/programming/#intel-compiler-options","text":"Switch Description -parallel enable the auto-parallelizer to generate multi-threaded code for loops that can be safely executed in parallel -par-reportX (X=1,2,3) control the auto-parallelizer diagnostic level -openmp enable the compiler to generate multi-threaded code based on the OpenMP* directives -openmp-reportX (X=1,2,3) control the OpenMP parallelizer diagnostic level -fast enable -xHOST -O3 -ipo -no-prec-div -static options set by -fast cannot be overridden with the exception of -xHOST , list options separately to change behavior -O0 disable optimizations -O1 optimize for maximum speed, but disable some optimizations which increase code size for a small speed benefit -O2 optimize for maximum speed (DEFAULT) -O3 optimize for maximum speed and enable more aggressive optimizations that may not improve performance on some programs -threads specify that multi-threaded libraries should be linked against -nothreads disables multi-threaded libraries -vec enables (DEFAULT) vectorization ( -novec disables vectorization) -mkl link to the Math Kernel Library (MKL) and bring in the associated headers, -mkl= one of parallel (default), sequential or cluster -opt-matmul replace matrix multiplication with calls to intrinsics and threading libraries for improved performance (DEFAULT at -O3 -parallel ) -xHost generate instructions for the highest instruction set and processor available on the compilation host machine -integer-size XX specifies the default size of integer and logical variables size: 16, 32, 64 -real-size XX specify the size of REAL and COMPLEX declarations, constants, functions, and intrinsics size: 32, 64, 128 -w disable all warnings, equivalent to -warn none or -nowarn -warn all enables all warnings -L<dir> instruct linker to search <dir> for libraries -l<string> instruct the linker to link in the -l<string> library -Wl,<o1>[,<o2>,...] pass options o1 , o2 , etc. to the linker for processing -V display compiler version information -multiple-processes[=<n>] create multiple processes that can be used to compile large numbers of source files at the same time -c compile to object (.o) only, do not link -S compile to assembly (.s) only, do not link -o <file> name output executable file -g produce symbolic debug information in object file (implies -O0 when another optimization option is not explicitly set) -p compile and link for function profiling with UNIX gprof tool, -pg is also valid -D<name>[=<text>] define macro -I<dir> add directory to include file search path -fpp run Fortran preprocessor on source files prior to compilation","title":"Intel compiler options"},{"location":"software/software/","text":"Software installed on Palmetto Overview Modules A large number of popular software packages are installed on Palmetto and can be used without any setup or configuration. These include: Compilers (such as gcc , Intel, and PGI) Libraries (such as OpenMPI, HDF5, Boost) Programming languages (such as Python, MATLAB, R) Scientific applications (such as LAMMPS, Paraview, ANSYS) Others (e.g., Git, PostgreSQL, Singularity) These packages are available as modules on Palmetto. The following commands can be used to inspect, activate and deactivate modules: Command Purpose module avail List all packages available (on current system) module add package/version Add a package to your current shell environment module list List packages you have loaded module rm package/version Remove a currently loaded package module purge Remove all currently loaded packages See the Quick Start Guide for more details about modules. Licensed software Many site-licensed software packages are available on Palmetto cluster (e.g., MATLAB, ANSYS, COMSOL, etc.,). There are limitations on the number of jobs that can run using these packages. See this section of the User's Guide on how to check license usage. Individual-owned or group-owned licensed software can also be run on Palmetto. Software with graphical applications See this section of the User's Guide on how to use software with graphical user interface (GUI). Installing your own software See this section of the User's Guide on how to check license usage. ABAQUS ABAQUS is a Finite Element Analysis software used for engineering simulations. Currently, ABAQUS versions 6.10, 6.13, 6.14 are available on Palmetto cluster as modules. $ module avail abaqus abaqus/6.10 abaqus/6.13 abaqus/6.14 To see license usage of ABAQUS-related packages, you can use the lmstat command: /software/USR_LOCAL/flexlm/lmstat -a -c /software/USR_LOCAL/flexlm/licenses/abaqus.dat Running ABAQUS interactive viewer To run the interactive viewer, you must log-in with tunneling enabled , and then ask for an interactive session: $ qsub -I -X -l select=1:ncpus=8:mpiprocs=8:mem=6gb:interconnect=1g,walltime=00:15:00 Once logged-in to an interactive compute node, to launch the interactive viewer, load the abaqus module, and run the abaqus executable with the viewer and -mesa options: $ module add abaqus/6.14 $ abaqus viewer -mesa Similarly, to launch the ABAQUS CAE graphical interface: $ abaqus cae -mesa Running ABAQUS in batch mode To run ABAQUS in batch mode on Palmetto cluster, you can use the job script in the following example as a template. This example shows how to run ABAQUS in parallel using MPI. This demonstration runs the \"Axisymmetric analysis of bolted pipe flange connections\" example provided in the ABAQUS documentation here . Please see the documentation for the physics and simulation details. You can obtain the files required to run this example using the following commands: $ cd /scratch2/username $ module add examples $ example get ABAQUS $ cd ABAQUS && ls abaqus_v6.env boltpipeflange_axi_element.inp boltpipeflange_axi_node.inp boltpipeflange_axi_solidgask.inp job.sh The .inp files describe the model and simulation to be performed - see the documentation for details. The batch script job.sh submits the job to the cluster. The .env file is a configuration file that must be included in all ABAQUS job submission folders on Palmetto. #!/bin/bash #PBS -N AbaqusDemo #PBS -l select=2:ncpus=8:mpiprocs=8:mem=6gb:interconnect=1g,walltime=00:15:00 #PBS -j oe module purge module add abaqus/6.14 pbsdsh sleep 20 NCORES=`wc -l $PBS_NODEFILE | gawk '{print $1}'` cd $PBS_O_WORKDIR SCRATCH=$TMPDIR # copy all input files into the scratch directory for node in `uniq $PBS_NODEFILE` do ssh $node \"cp $PBS_O_WORKDIR/*.inp $SCRATCH\" done cd $SCRATCH # run the abaqus program, providing the .inp file as input abaqus job=abdemo double input=$SCRATCH/boltpipeflange_axi_solidgask.inp scratch=$SCRATCH cpus=$NCORES mp_mode=mpi interactive # copy results back from scratch directory to $PBS_O_WORKDIR for node in `uniq $PBS_NODEFILE` do ssh $node \"cp -r $SCRATCH/* $PBS_O_WORKDIR\" done In the batch script job.sh : The following line extracts the total number of CPU cores available across all the nodes requested by the job: ~~~ NCORES= wc -l $PBS_NODEFILE | gawk '{print $1}' ~~~ The following line runs the ABAQUS program, specifying various options such as the path to the .inp file, the scratch directory to use, etc., ~~~ abaqus job=abdemo double input=/scratch2/$USER/ABAQUS/boltpipeflange_axi_solidgask.inp scratch=$SCRATCH cpus=$NCORES mp_mode=mpi interactive ~~~ To submit the job: $ qsub job.sh 9668628 After job completion, you will see the job submission directory ( /scratch2/username/ABAQUS ) populated with various files: $ ls AbaqusDemo.o9668628 abdemo.dat abdemo.msg abdemo.res abdemo.stt boltpipeflange_axi_solidgask.inp abaqus_v6.env abdemo.fil abdemo.odb abdemo.sim boltpipeflange_axi_element.inp job.sh abdemo.com abdemo.mdl abdemo.prt abdemo.sta boltpipeflange_axi_node.inp If everything went well, the job output file ( AbaqusDemo.o9668628 ) should look like this: [atrikut@login001 ABAQUS]$ cat AbaqusDemo.o9668628 Abaqus JOB abdemo Abaqus 6.14-1 Abaqus License Manager checked out the following licenses: Abaqus/Standard checked out 16 tokens from Flexnet server licensevm4.clemson.edu. <567 out of 602 licenses remain available>. Begin Analysis Input File Processor Mon 13 Feb 2017 12:35:29 PM EST Run pre Mon 13 Feb 2017 12:35:31 PM EST End Analysis Input File Processor Begin Abaqus/Standard Analysis Mon 13 Feb 2017 12:35:31 PM EST Run standard Mon 13 Feb 2017 12:35:35 PM EST End Abaqus/Standard Analysis Abaqus JOB abdemo COMPLETED +------------------------------------------+ | PALMETTO CLUSTER PBS RESOURCES REQUESTED | +------------------------------------------+ mem=12gb,ncpus=16,walltime=00:15:00 +-------------------------------------+ | PALMETTO CLUSTER PBS RESOURCES USED | +-------------------------------------+ cpupercent=90,cput=00:00:10,mem=636kb,ncpus=16,vmem=12612kb,walltime=00:00:13 The output database ( .odb ) file contains the results of the simulation which can be viewed using the ABAQUS viewer: Amber In this example we will use \"alp\" test from Amber's test suite. To get the files relevant to this example: $ module add examples $ example get Amber $ cd Amber && ls amber.pbs coords md.in prmtop README.md Examine the batch script amber.pbs , and adjust the names of the files i.e. input and coordinates files. Also, adjust the number of nodes, processors per node and walltime. To submit the job: qsub amber.pbs Results will be saved in the md.out file (in our example). ANSYS Graphical Interfaces To run the various ANSYS graphical programs, you must log-in with tunneling enabled and then ask for an interactive session: $ qsub -I -X -l select=1:ncpus=8:mpiprocs=8:mem=6gb:interconnect=1g,walltime=01:00:00 Once logged-in to an interactive compute node, you must first load the ANSYS module along with the Intel module: $ module add ansys/17.2 $ module add intel/12.1 And then launch the required program: For ANSYS APDL $ ansys172 -g If you are using e.g., ANSYS 17.0 instead, then the executable is called ansys170 . For CFX $ cfxlaunch For ANSYS Workbench $ runwb2 For Fluent $ fluent For ANSYS Electromagnetics $ ansysedt Batch Mode To run ANSYS in batch mode on Palmetto cluster, you can use the job script in the following example as a template. This example shows how to run ANSYS in parallel (using multiple cores/nodes). In this demonstration, we model the strain in a 2-D flat plate. You can obtain the files required to run this example using the following commands: $ cd /scratch2/username $ module add examples $ example get ANSYS $ cd ANSYS && ls input.txt job.sh The input.txt batch file is generated for the model using the ANSYS APDL interface. The batch script job.sh submits the batch job to the cluster: #!/bin/bash #PBS -N ANSYSdis #PBS -l select=2:ncpus=4:mpiprocs=4:mem=11gb:interconnect=1g #PBS -l walltime=1:00:00 #PBS -j oe module purge module add ansys/17.2 cd $PBS_O_WORKDIR machines=$(uniq -c $PBS_NODEFILE | awk '{print $2\":\"$1}' | tr '\\n' :) for node in `uniq $PBS_NODEFILE` do ssh $node \"sleep 5\" ssh $node \"cp input.txt $TMPDIR\" done cd $TMPDIR ansys172 -dir $TMPDIR -j EXAMPLE -s read -l en-us -b -i input.txt -o output.txt -dis -machines $machines -usessh for node in `uniq $PBS_NODEFILE` do ssh $node \"cp -r $TMPDIR/* $PBS_O_WORKDIR\" done In the batch script job.sh : The following line extracts the nodes (machines) available for this job as well as the number of CPU cores allocated for each node: ~~~ machines=$(uniq -c $PBS_NODEFILE | awk '{print $2\":\"$1}' | tr '\\n' :) ~~~ For ANSYS jobs, you should always use $TMPDIR ( /local_scratch ) as the working directory. The following lines ensure that $TMPDIR is created on each node: ~~~ do ssh $node \"sleep 5\" ssh $node \"cp input.txt $TMPDIR\" done ~~~ The following line runs the ANSYS program, specifying various options such as the path to the input.txt file, the scratch directory to use, etc., ~~~ ansys172 -dir $TMPDIR -j EXAMPLE -s read -l en-us -b -i input.txt -o output.txt -dis -machines $machines -usessh ~~~ Finally, the following lines copy all the data from $TMPDIR : ~~~ do ssh $node \"cp -r $TMPDIR/* $PBS_O_WORKDIR\" done ~~~ To submit the job: $ qsub job.sh 9752784 After job completion, you will see the job submission directory ( /scratch2/username/ANSYS ) populated with various files: $ ls ANSYSdis.o9752784 EXAMPLE0.stat EXAMPLE2.err EXAMPLE3.esav EXAMPLE4.full EXAMPLE5.out EXAMPLE6.rst EXAMPLE.DSP input.txt EXAMPLE0.err EXAMPLE1.err EXAMPLE2.esav EXAMPLE3.full EXAMPLE4.out EXAMPLE5.rst EXAMPLE7.err EXAMPLE.esav job.sh EXAMPLE0.esav EXAMPLE1.esav EXAMPLE2.full EXAMPLE3.out EXAMPLE4.rst EXAMPLE6.err EXAMPLE7.esav EXAMPLE.mntr mpd.hosts EXAMPLE0.full EXAMPLE1.full EXAMPLE2.out EXAMPLE3.rst EXAMPLE5.err EXAMPLE6.esav EXAMPLE7.full EXAMPLE.rst mpd.hosts.bak EXAMPLE0.log EXAMPLE1.out EXAMPLE2.rst EXAMPLE4.err EXAMPLE5.esav EXAMPLE6.full EXAMPLE7.out host.list output.txt EXAMPLE0.rst EXAMPLE1.rst EXAMPLE3.err EXAMPLE4.esav EXAMPLE5.full EXAMPLE6.out EXAMPLE7.rst host.list.bak If everything went well, the job output file ( ANSYSdis.o9752784 ) should look like this: +------------------------------------------+ | PALMETTO CLUSTER PBS RESOURCES REQUESTED | +------------------------------------------+ mem=22gb,ncpus=8,walltime=01:00:00 +-------------------------------------+ | PALMETTO CLUSTER PBS RESOURCES USED | +-------------------------------------+ cpupercent=27,cput=00:00:17,mem=3964kb,ncpus=8,vmem=327820kb,walltime=00:01:07 The results file ( EXAMPLE.rst ) contains the results of the simulation which can be viewed using the ANSYS APDL graphical interface: COMSOL COMSOL is an application for solving Multiphysics problems. To see the available COMSOL modules on Palmetto: $ module avail comsol comsol/4.3b comsol/4.4 comsol/5.0 comsol/5.1 comsol/5.2 comsol/5.3 To see license usage of COMSOL-related packages, you can use the lmstat command: /software/USR_LOCAL/flexlm/lmstat -a -c /software/USR_LOCAL/flexlm/licenses/comsol.dat Graphical Interface To run the COMSOL graphical interface, you must log-in with tunneling enabled , and then ask for an interactive session: $ qsub -I -X -l select=1:ncpus=8:mpiprocs=8:mem=6gb:interconnect=1g,walltime=00:15:00 Once logged-in to an interactive compute node, to launch the interactive viewer, you can use the comsol command to run COMSOL: $ module add comsol/5.2 $ comsol -np 8 -tmpdir $TMPDIR The -np option can be used to specify the number of CPU cores to use. Remember to always use $TMPDIR as the working directory for COMSOL jobs. Batch Mode To run COMSOL in batch mode on Palmetto cluster, you can use the example batch scripts below as a template. The first example demonstrates running COMSOL using multiple cores on a single node, while the second demonstrates running COMSOL across multiple nodes using MPI. You can obtain the files required to run this example using the following commands: $ module add examples $ example get COMSOL $ cd COMSOL && ls job.sh job_mpi.sh Both of these examples run the \"Heat Transfer by Free Convection\" application described here . In addition to the job.sh and job_mpi.sh scripts, to run the examples and reproduce the results, you will need to download the file free_convection.mph (choose the correct version) provided with the description (login required). COMSOL batch job on a single node, using multiple cores: #!/bin/bash #PBS -N COMSOL #PBS -l select=1:ncpus=8:mem=32gb,walltime=01:30:00 #PBS -j oe module purge module add comsol/5.2 cd $PBS_O_WORKDIR comsol batch -np 8 -tmpdir $TMPDIR -inputfile free_convection.mph -outputfile free_convection_output.mph COMSOL batch job across several nodes #!/bin/bash #PBS -N COMSOL #PBS -l select=2:ncpus=8:mpiprocs=8:mem=32gb,walltime=01:30:00 #PBS -j oe module purge module add comsol/5.2 cd $PBS_O_WORKDIR uniq $PBS_NODEFILE > comsol_nodefile comsol batch -clustersimple -f comsol_nodefile -tmpdir $TMPDIR -inputfile free_convection.mph -outputfile free_convection_output.mph Gephi Gephi is an interactive visualization and exploration platform for all kinds of networks and complex systems, dynamic and hierarchical graphs. At this time, we don't have Gephi setup as a software module on Palmetto, but downloading and installing it in your own /home or /newscratch directory (and running it from that location) is easy to do. Below are the procedures I used to do this: Download and Setup the Gephi Binary wget https://launchpad.net/gephi/0.8/0.8.2beta/+download/gephi-0.8.2-beta.tar.gz tar -zxf gephi-0.8.2-beta.tar.gz Now, the gephi binary is installed in ~/gephi/bin . Running Gephi Connect to Palmetto with X11 tunneling enabled so you can \"tunnel\" the GUI running on Palmetto through your SSH connection to your desktop: ssh -X galen@login.palmetto.clemson.edu Once you're logged-in to Palmetto, you'll need to launch an interactive job with X11 tunneling enabled. Be sure to request an allocation with enough memory for Gephi to handle your input data properly (here, I'm requesing 31 GB RAM): qsub -I -X -l select=1:ncpus=4:mem=31gb,walltime=6:00:00 If you are using a Windows system, you'll need to launch a local Xserver (like Xming) so your system can display the tunneled GUI information. Once your local Xserver is ready, launch the Gephi GUI in your interactive session on Palmetto: /home/galen/gephi/bin/gephi Now, I can browse for data I have stored in my directories on Palmetto. If your data is stored on your local workstation, you must move that data to Palmetto so you can open it with Gephi. You may experience a short delay as the GUI is generated and tunneled to your desktop. Responsiveness between your local workstation and the tunneled GUI may be a bit slow/laggy, but the compute operations taking place on that Palmetto compute node are not delayed. GROMACS Various installations of GROMACS are available on the cluster. Different modules are provided for GPU-enabled and non-GPU versions of GROMACS: $ module avail gromacs ---------------------------------------------------------------- /software/modulefiles ----------------------------------------------------------------- gromacs/4.5.4-sp gromacs/4.6.5-sp-k20-ompi gromacs/5.0.1-sp-k20-g481-o181 gromacs/5.0.5-nogpu gromacs/4.6.5-dp-ompi gromacs/5.0.1-dp-g481-o181 gromacs/5.0.5-gpu ------------------------------------------------------------ /usr/share/Modules/modulefiles ------------------------------------------------------------ gromacs/4.5.4-sp Running GROMACS without GPU To use the non-GPU version of GROMACS, here is an example interactive job: $ qsub -I -l select=2:ncpus=20:phase=10:mem=100gb:mpiprocs=20,walltime=1: 00:00 $ module load gromacs/5.0.5-nogpu $ mpirun mdrun_mpi ... where ... specifies the input files and options of Gromacs. And an example of a PBS batch script for submitting a GROMACS batch script: #!/bin/bash #PBS -l select=2:ncpus=20:phase=10:mem=100gb:mpiprocs=20 #PBS -l walltime=1:00:00 module load gromacs/5.0.5-nogpu cd $PBS_O_WORKDIR export OMP_NUM_THREADS=1 mpirun mdrun_mpi ... Running GPU-enabled GROMACS For the GPU version #!/bin/bash #PBS -l select=2:ncpus=20:ngpus=2:phase=10:mem=100gb:mpiprocs=2 #PBS -l walltime=1:00:00 module load gromacs/5.0.5-gpu cd $PBS_O_WORKDIR export OMP_NUM_THREADS=10 mpirun mdrun_mpi ... Note that the number of MPI processes per chunk is set as 2 when using the GPU. Normally, the number of MPI processes is the same as the number of CPU cores requested. But because there are only 2 GPUs per node, we launch only two MPI processes. In the above example, each MPI process can also use 10 CPU cores in addition to a GPU (this is enabled by setting the variable OMP_NUM_THREADS to 10). GPU-enabled GROMACS using Singularity and NGC The NVIDIA GPU cloud provides images for GPU-enabled GROMACS that can be downloaded and run using Singularity on the Palmetto cluster. This is the recommended way to run GROMACS on Palmetto. Downloading the image Before downloading images from NGC, you will need to obtain an NVIDIA NGC API key, instructions for which can be found here . Start an interactive job, and create a directory for storing Singularity images (if you don't have one already): $ qsub -I -l select=1:ngpus=2:ncpus=16:mem=20gb:gpu_model=p100,walltime=5:00:00 $ mkdir -p singularity-images Set the required environment variables (you will need the API key for this step): $ export SINGULARITY_DOCKER_USERNAME='$oauthtoken' $ export SINGULARITY_DOCKER_PASSWORD=<NVIDA NGC API key> Navigate to the singularity-images directory and pull the GROMACS image (choose from the images listed here : $ cd ~/singularity-images $ module load singularity $ singularity pull docker://nvcr.io/hpc/gromacs:2016.4 The above should take a few seconds to complete. Running GROMACS interactively As an example, we'll consider running the GROMACS ADH benchmark. First, request an interactive job: $ qsub -I -l select=1:ngpus=2:ncpus=16:mpiprocs=16:mem=120gb:gpu_model=p100,walltime=5:00:00 Load the singularity module: $ module load singularity Prepare the input and output directories: In this case, we use /scratch3/$USER/gromacs_ADH as both the input directory (containing the data) and the output directory (where output will be stored). Remember that $TMPDIR is cleaned up after the job completes, so it's important to move any data out of $TMPDIR after GROMACS completes running. Also, in this case, we are downloading the input data from the web, but if your input is stored in the /home /, /scratch or some other directory, then you can copy it to $TMPDIR : $ mkdir -p /scratch3/$USER/gromacs_ADH_benchmark $ cd /scratch3/$USER/gromacs_ADH_benchmark $ wget ftp://ftp.gromacs.org/pub/benchmarks/ADH_bench_systems.tar.gz $ tar -xvf ADH_bench_systems.tar.gz Use singularity shell to interactively run a container from the downloaded GROMACS image. The -B switch is used to \"bind\" directories on the host (Palmetto compute node) to directories in the container. The different bindings used are: /scratch3/$USER/gromacs_ADH_benchmark is bound to /input /scratch3/$USER/gromacs_ADH_benchmark is bound to /output $TMPDIR is bound to /work $ singularity shell --nv \\ -B /scratch3/$USER/gromacs_ADH_benchmark:/input \\ -B /scratch3/$USER/gromacs_ADH_benchmark:/output \\ -B $TMPDIR:/work \\ --pwd /work \\ ~/singularity-images/gromacs-2016.4.simg \\ Running the benchmark: $ source /opt/gromacs/install/2016.4/bin/GMXRC $ gmx_mpi grompp -f /input/adh_cubic/pme_verlet.mdp -c /input/adh_cubic/conf.gro -p /input/adh_cubic/topol.top $ export OMP_NUM_THREADS=8 $ mpirun -np 2 gmx_mpi mdrun -g adh_cubic.log -pin on -resethway -v -noconfout -nsteps 10000 -s topol.tpr -ntomp $OMP_NUM_THREADS After the last command above completes, the .edr and .log files produced by GROMACS should be visible. Typically, the next step is to copy these results to the output directory: $ cp *.log *.edr /output $ exit Upon exiting the container, the .log and .edr files will be found in the output directory, /scratch3/$USER/gromacs_ADH_benchmark . Running GROMACS in batch mode The same benchmark can be run in batch mode by encapsulating the commands in a script run_adh.sh , and running it non-interactively using singularity exec . # run_adh.sh source /opt/gromacs/install/2016.4/bin/GMXRC gmx_mpi grompp -f /input/adh_cubic/pme_verlet.mdp -c /input/adh_cubic/conf.gro -p /input/adh_cubic/topol.top export OMP_NUM_THREADS=8 mpirun -np 2 gmx_mpi mdrun -g adh_cubic.log -pin on -resethway -v -noconfout -nsteps 10000 -s topol.tpr -ntomp $OMP_NUM_THREADS cp *.log *.edr /output The PBS batch script for submitting the above (assuming that the /scratch3/$USER/gromacs_ADH_benchmark directory already contains the input files): #PBS -N adh_cubic #PBS -l select=1:ngpus=2:ncpus=16:mem=20gb:gpu_model=p100,walltime=5:00:00 module load singularity cd $PBS_O_WORKDIR cp run_adh.sh $TMPDIR singularity exec --nv \\ -B /scratch3/$USER/gromacs_ADH_benchmark:/input \\ -B /scratch3/$USER/gromacs_ADH_benchmark:/output \\ -B $TMPDIR:/work \\ --pwd /work \\ ~/singularity-images/gromacs-2016.4.simg bash run_adh.sh HOOMD Run HOOMD Now the HOOMD-BLUE v2.3.5 has been installed. Create a simple python file \u201ctest_hoomd.py\u201d to run HOOMD import hoomd import hoomd.md hoomd.context.initialize(\"\"); hoomd.init.create_lattice(unitcell=hoomd.lattice.sc(a=2.0), n=5); nl = hoomd.md.nlist.cell(); lj = hoomd.md.pair.lj(r_cut=2.5, nlist=nl); lj.pair_coeff.set('A', 'A', epsilon=1.0, sigma=1.0); hoomd.md.integrate.mode_standard(dt=0.005); all = hoomd.group.all(); hoomd.md.integrate.langevin(group=all, kT=0.2, seed=42);hoomd.analyze.log(filename=\"log-output.log\", quantities=['potential_energy', 'temperature'], period=100, overwrite=True); hoomd.dump.gsd(\"trajectory.gsd\", period=2e3, group=all, overwrite=True); hoomd.run(1e4); If you have logged out of the node, request an interactive session on a GPU node and add required modules: $ qsub -I -l select=1:ncpus=16:mem=64gb:ngpus=2:gpu_model=p100:mpiprocs=16:interconnect=fdr,walltime=2:00:00 $ module add anaconda3/5.1.0 gcc/5.4.0 cuda-toolkit/9.0.176 Run the script interactively: $ python test_hoomd.py Alternatively, you can setup a PBS job script to run HOOMD in batch mode. A sample is below for Test_Hoomd.sh : #PBS -N HOOMD #PBS -l select=1:ncpus=16:mem=64gb:ngpus=2:gpu_model=p100:mpiprocs=16:interconnect=fdr,walltime=02:00:00 #PBS -j oe module purge module add anaconda3/5.1.0 gcc/5.4.0 cuda-toolkit/9.0.176 cd $PBS_O_WORKDIR python test_hoomd.py Submit the job: $ qsub Test_Hoomd.sh This is it Java The Java Runtime Environment (JRE) version 1.6.0_11 is currently available cluster-wide on Palmetto. If a user needs a different version of Java, or if the Java Development Kit (JDK, which includes the JRE) is needed, that user is encouraged to download and install Java (JRE or JDK) for herself. Below is a brief overview of installing the JDK in a user's /home directory. JRE vs. JDK The JRE is basically the Java Virtual Machine (Java VM) that provides a platform for running your Java programs. The JDK is the fully featured Software Development Kit for Java, including the JRE, compilers, and tools like JavaDoc and Java Debugger used to create and compile programs. Usually, when you only care about running Java programs, the JRE is all you'll need. If you are planning to do some Java programming, you will need the JDK. Downloading the JDK The JDK cannot be downloaded directly using the wget utility because a user must agree to Oracle's Java license ageement when downloading. So, download the JDK using a web browser and transfer the downloaded jdk-7uXX-linux-x64.tar.gz file to your /home directory on Palmetto using scp , sftp , or FileZilla: scp jdk-7u45-linux-x64.tar.gz galen@login.palmetto.clemson.edu:/home/galen jdk-7u45-linux-x64.tar.gz 100% 132MB 57.7KB/s 38:58 Installing the JDK The JDK is distributed in a Linux x86_64 compatible binary format, so once it has been unpacked, it is ready to use (no need to compile). However, you will need to setup your environment for using this new package by adding lines similar to the following at the end of your ~/.bashrc file: export JAVA_HOME=/home/galen/jdk1.7.0_45 export PATH=$JAVA_HOME/bin:$PATH export MANPATH=$JAVA_HOME/man:$MANPATH Once this is done, you can log-out and log-in again or simply source your ~/.bashrc file and then you'll be ready to begin using your new Java installation. Julia Julia: high-level dynamic programming language that was originally designed to address the needs of high-performance numerical analysis and computational science. Run Julia in Palmetto: Interactive There are a few different versions of Julia available on the cluster. $ module avail julia --------------------------------------------- /software/modulefiles --------------------------------------------- julia/0.6.2 julia/0.7.0 julia/1.0.4 julia/1.1.1 Let demonstrate how to use julia/1.1.1 in the Palmetto cluster together with Gurobi Optimizer (a commercial optimization solver for linear programming), quadratic programming, etc. Clemson University has different version of licenses for Gurobi solver. In this example, I would like to use Julia and Gurobi solver to solve a linear math problem using Palmetto HPC Problems: Maximize x+y Given the following constrains: 50 x + 24 y <= 2400 30 x + 33 y <= 2100 x >= 5, y >= 45 Let prepare a script to solve this problem, named: jump_gurobi.jl. You can save this file to: /scratch1/$username/Julia/ # Request for a compute node: $ qsub -I -l select=1:ncpus=8:mem=16gb:interconnect=fdr,walltime=01:00:00 # Go to working folder: $ cd /scratch1/$username/Julia $ nano jump_gurobi.jl Then type/copy the following code to the file jump_gurobi.jl import Pkg using JuMP using Gurobi m = Model(with_optimizer(Gurobi.Optimizer)) @variable(m, x >= 5) @variable(m, y >= 45) @objective(m, Max, x + y) @constraint(m, 50x + 24y <= 2400) @constraint(m, 30x + 33y <= 2100) status = optimize!(m) println(\" x = \", JuMP.value(x), \" y = \", JuMP.value(y)) Save the jump_gurobi.jl file then you are ready to run julia: $ module add julia/1.1.1 gurobi/7.0.2 $ julia # the julia prompt appears: julia> # Next install Package: JuMP and Gurobi julia> using Pkg julia> Pkg.add(\"JuMP\") julia> Pkg.add(\"Gurobi\") julia> exit() Run the julia_gurobi.jl script: $ julia jump_gurobi.jl Run Julia in Palmetto: Batch mode Alternatively, you can setup a PBS job script to run Julia in batch mode. A sample is below for submit_julia.sh : You must install the JuMP and Gurobi package first (one time installation) #!/bin/bash #PBS -N Julia #PBS -l select=1:ncpus=8:mem=16gb:interconnect=fdr #PBS -l walltime=02:00:00 #PBS -j oe module purge module add julia/1.1.1 gurobi/7.0.2 cd $PBS_O_WORKDIR julia jump_gurobi.jl > output_JuMP.txt Submit the job: $ qsub submit_julia.sh The output file can be found at the same folder: output_JuMP.txt Install your own Julia package using conda environment and running in Jupyterhub In addition to traditional compilation of Julia, it is possible to install your own version of Julia and setup kernel to work using Jupterhub. # Request for a compute node: $ qsub -I -l select=1:ncpus=8:mem=16gb:interconnect=fdr,walltime=01:00:00 $ module add anaconda3/5.1.0 # Create conda environment with the name as \"Julia\" $ conda create -n Julia -c conda-forge julia $ source activate Julia (Julia) [$username@node1234 ~]$ (Julia) [$username@node1234 ~]$ julia julia> julia> using Pkg julia> Pkg.add(\"IJulia\") julia> exit Exit Julia and Start Jupyterhub in Palmetto After spawning, Click on New kernel in Jupyterhub, you will see Julia 1.1.1 kernel available for use Type in the follwing code to test: println(\"Hello world\") LAMMPS There are a few different versions of LAMMPS available on the cluster, but users are encouraged to install their own version of LAMMPS in case newer versions different configurations are desired. In particular, there are two main components of LAMMPS which can be run with CPUs (lmp_mpi) or with GPUs (lmp) $ module avail lammps lammps/10Jan15-dp lammps/17Dec13-dp lammps/17Dec13-dp-k20 lammps/2018Dec12 lammps/29Aug14-sp-k20 Installing LAMMPS on Palmetto cluster In this example, we will demonstrate installing LAMMPS to run with CPUs and GPUs (version 7 Aug 2019 ). Detail information can be found here: https://lammps.sandia.gov/doc/Install.html After logging in, ask for an interactive session (with GPU): ~~~ $ qsub -I -l select=1:ncpus=8:mpiprocs=8:ngpus=1:mem=64gb:gpu_model=v100,walltime=8:00:00 ~~~ Load the required modules. Specifically, note that we are loading the CUDA-enabled module: ~~~~ $ module load openmpi/3.1.4-gcc71-ucx fftw/3.3.4-g481 cuda-toolkit/9.2 cmake/3.13.1 ~~~~ Download the LAMMPS source code from http://lammps.sandia.gov/download.html. The detailed instruction on usage and compilation options are available at http://lammps.sandia.gov/doc/Manual.html. Unpack the source code and enter the package directory: ~~~ $ tar -xvf lammps-stable.tar.gz $ cd lammps-7Aug19 ~~~ There are two different methods of compiling LAMMPS using make (support CPUs) or cmake (support GPUs). We introduce both methods here: Compile LAMMPS using make with KOKKOS support ~~~ $ cd src $ make yes-kokkos $ make mpi -j8 ~~~ A new executable file lmp_mpi will be produced in the src folder. You can copy this executable over to the working directory of your jobs. Note that the size of the executable may be a hundred MB Note: there are optional packages which can be installed together with the above package using make yes- prior to running make mpi . For example: ~~~ $ make yes-USER-MISC $ make yes-KSPACE $ make yes-MOLECULE $ make yes-MISC $ make yes-manybody $ make yes-dipole $ make yes-class2 $ make yes-asphere $ make yes-replica $ make yes-granular $ make yes-rigid ~~~ Compile LAMMPS using cmake with GPU support: The detailed manual can be found here with more information on the options to be included. In this instruction, we only cover a certain number of options that are enough install cmake with GPU: ~~~ $ mkdir build $ cd build $ cmake ../cmake/ -DPKG_GPU=on -DGPU_API=cuda -DGPU_PREC=double -DGPU_ARCH=sm_70 -DCUDA_CUDA_LIBRARY=/usr/lib64/libcuda.so -DFFMPEG_EXECUTABLE=/software/ffmpeg/3.3.2/bin/ffmpeg -DPKG_MC=on -DPKG_REPLICA=on -DPKG_SNAP=on -DPKG-MANYBODY=on -DPKG_USER-MEAMC=on -DPKG_USER-MISC=on -DPKG_MISC=on $ make -j8 ~~~ In the above script; the -DGPU_ARCH=sm_70 was used because the gpu_model was set at Volta (v100). Please refer to the above guideline for setting particular gpu architecture Note: there are optional packages which can be installed together with the above package using -D[Options]=on . For example: ~~~ $ cmake ../cmake/ -DPKG_MISC=on -DPKG_KSPACE=on -DPKG_MOLECULE=on -DPKG_MANYBODY=on -DPKG_DIPOLE=on -DPKG_CLASS2=on -DPKG_ASPHERE=on -DPKG_REPLICA=on -DPKG_GRANULAR=on -DPKG_RIGID=on ~~~ The executable file lmp will be produced. You can copy this executable over to the working directory of your jobs. Note that the size of the executable may be a hundred MB. Running LAMMPS - an example Several existing examples are in the installed folder: lammps-7Aug19/examples/ Detailes description of all examples are here . In order to run the example, simply copy the executable files created from step 5 and 6 to particular example folder and follow the README for detailed description on how to run. For instance, in order to run an example accelerate using GPU package. Copy lmp to that folder. Here is a sample batch script job.sh for this example: #PBS -N accelerate #PBS -l select=1:ncpus=8:mpiprocs=8:ngpus=1:gpu_model=v100:mem=64gb,walltime=1:00:00 #PBS -j oe module purge module load openmpi/3.1.4-gcc71-ucx mpirun -np 8 lmp -sf gpu < in.lj # 8 MPI, 8 MPI/GPU MATLAB Checking license usage for MATLAB You can check the availability of MATLAB licenses using the lmstat command: $ /software/USR_LOCAL/flexlm/lmstat -a -c /software/USR_LOCAL/flexlm/licenses/matlab.dat Running the MATLAB graphical interface To launch the MATLAB graphical interface, you must first you must log-in with tunneling enabled , and then ask for an interactive session: $ qsub -I -X -l select=1:ncpus=2:mem=24gb,walltime=1:00:00 Once logged-in, you must load one of the MATLAB modules: $ module add matlab/2018b And then launch the MATLAB program: $ matlab Warning : DO NOT attempt to run MATLAB right after logging-in (i.e., on the login001 node). Always ask for an interactive job first. MATLAB sessions are automatically killed on the login node. Running the MATLAB command line without graphics To use the MATLAB command-line interface without graphics, you can additionally use the -nodisplay and -nosplash options: $ matlab -nodisplay -nosplash To quit matlab command-line interface, type: $ exit MATLAB in batch jobs To use MATLAB in your batch jobs, you can use the -r switch provided by MATLAB, which lets you r un commands specified on the command-line. For example: $ matlab -nodisplay -nosplash -r myscript will run the MATLAB script myscript.m , Or: $ matlab -nodisplay -nosplash < myscript.m > myscript_results.txt will run the MATLAB script myscript.m and write the output to myscript_results.txt file. Thus, an example batch job using MATLAB could have a batch script as follows: #!/bin/bash # #PBS -N test_matlab #PBS -l select=1:ncpus=1:mem=5gb #PBS -l walltime=1:00:00 module add matlab/2018b cd $PBS_O_WORKDIR taskset -c 0-$(($OMP_NUM_THREADS-1)) matlab -nodisplay -nosplash < myscript.m > myscript_results.txt Note : MATLAB will sometimes attemps to use all available CPU cores on the node it is running on. If you haven't reserved all cores on the node, your job may be killed if this happens. To avoid this, you can use the taskset utility to set the \"core affinity\" (as shown above). As an example: $ taskset 0-2 <application> will limit application to using 3 CPU cores. On Palmetto, the variable OMP_NUM_THREADS is automatically set to be the number of cores requested for a job. Thus, you can use 0-$((OMP_NUM_THREADS-1)) as shown in the above batch script to use all the cores you requested. Compiling MATLAB code to create an executable Often, you need to run a large number of MATLAB jobs concurrently (e.g,m each job operating on different data). In such cases, you can avoid over-utilizing MATLAB licenses by compiling your MATLAB code into an executable. This can be done from within the MATLAB command-line as follows: $ matlab -nodisplay -nosplash >> mcc -R -nodisplay -R -singleCompThread -R -nojvm -m mycode.m Note: MATLAB will try to use all the available CPU cores on the system where it is running, and this presents a problem when your compiled executable on the cluster where available cores on a single node might be shared amongst mulitple users. You can disable this \"feature\" when you compile your code by adding the -R -singleCompThread option, as shown above. The above command will produce the executable mycode , corresponding to the M-file mycode.m . If you have multiple M-files in your project and want to create a single excutable, you can use a command like the following: >> mcc -R -nodisplay -R -singleCompThread -R -nojvm -m my_main_code.m myfunction1.m myfunction2.m myfunction3.m Once the executable is produced, you can run it like any other executable in your batch jobs. Of course, you'll also need the same matlab and (optional) GCC module loaded for your job's runtime environment. mothur This page instructs how to install mothur software to Palmetto The code and the issue tracker can be found here Request an interactive session. For example: $ qsub -I -l select=1:ncpus=6:mem=24gb:interconnect=fdr,walltime=3:00:00 Load the required modules $ module load boost/1.65.1 hdf5/1.10.1 openmpi/1.10.3 gcc/8.2.0 Download mothur from source . Here we download the latest version 1.41.3 $ wget https://github.com/mothur/mothur/archive/v1.41.3.tar.gz Unpack the downloaded file and go into mothur source folder $ tar -xvf v1.41.3.tar.gz $ cd mothur-1.41.3 Modify the Makefile $ nano Makefile #Replace the following lines 22-32 with information: (Note: username should be replaced) OPTIMIZE ?= yes USEREADLINE ?= yes USEBOOST ?= yes USEHDF5 ?= yes LOGFILE_NAME ?= no BOOST_LIBRARY_DIR ?= \"/software/boost/1.65.1/lib\" BOOST_INCLUDE_DIR ?= \"/software/boost/1.65.1/include\" HDF5_LIBRARY_DIR ?= \"/software/hdf5/1.10.1/lib\" HDF5_INCLUDE_DIR ?= \"/software/hdf5/1.10.1/include\" MOTHUR_FILES ?= \"\\\"home/username/application/bin/mothur\\\"\" VERSION = \"\\\"1.41.3\\\"\" Run make file $ make The mothur executable file will be created in the installation folder: /home/username/applications/bin . Make sure you set the correct environment PATH in ~/.bashrc file export PATH=$PATH:$HOME/applications/bin MrBayes MrBayes is a program for Bayesian inference and model choice across a wide range of phylogenetic and evolutionary models. MrBayes uses Markov chain Monte Carlo (MCMC) methods to estimate the posterior distribution of model parameters. Installing BEAGLE Library BEAGLE library is supported to dramatic speedups for codon and amino acid models on compatible hardware (NVIDIA graphics cards); To install MrBayes in your /home directory on Palmetto, you'll need to begin by installing the BEAGLE library. Here are the steps, starting with checking-out the source code using Subversion: [galen@login001 ~]$ cd [galen@login001 ~]$ svn checkout http://beagle-lib.googlecode.com/svn/trunk/ beagle-setup [galen@login001 ~]$ cd beagle-setup [galen@login001 beagle-setup]$ ./autogen.sh [galen@login001 beagle-setup]$ ./configure --prefix=/home/galen/beagle-lib [galen@login001 beagle-setup]$ make install Once installed, I also needed to add the location of these new libraries to my LD_LIBRARY_PATH (this can be done in your ~/.bashrc file, or in your PBS job script as I have done at the bottom of this section): [galen@login001 beagle-setup]$ export LD_LIBRARY_PATH=/home/galen/beagle-lib/lib:$LD_LIBRARY_PATH Finally, verify that everything is setup properly: [galen@login001 beagle-setup]$ make check Installing MrBayes You can build a parallel version of MrBayes using OpenMPI or MPICH2. I used MPICH2 and I included the BEAGLE libraries when compiling MrBayes, so those libraries will also be needed whenever I run MrBayes. [galen@login001 src]$ module add gcc/4.4 mpich2/1.4 [galen@login001 ~]$ wget http://downloads.sourceforge.net/project/mrbayes/mrbayes/3.2.1/mrbayes-3.2.1.tar.gz [galen@login001 ~]$ tar -zxf mrbayes-3.2.1.tar.gz [galen@login001 ~]$ cd mrbayes_3.2.1/src [galen@login001 src]$ export PKG_CONFIG_PATH=/home/galen/beagle-lib/lib/pkgconfig:$PKG_CONFIG_PATH [galen@login001 src]$ autoconf [galen@login001 src]$ ./configure --enable-mpi=yes --with-beagle=/home/galen/beagle-lib [galen@login001 src]$ make Here, the mb executable was created in my /home/galen/mrbayes_3.2.1/src directory. Running MrBayes When running MrBayes in parallel, you'll need to use 1 processor core for each Markov chain, and the default number of chains is 4 (3 heated and 1 that's not heated). Below is an example MrBayes job that uses one of the example Nexus files included with my installation package /home/galen/mrbayes_3.2.1/examples/primates.nex . My MrBayes input file (mb_input) contains these commands: begin mrbayes; set autoclose=yes nowarn=yes; execute primates.nex; lset nst=6 rates=gamma; mcmc nruns=1 ngen=10000 samplefreq=10 file=primates.nex; mcmc file=primates.nex2; mcmc file=primates.nex3; end; My PBS job script for running this job in parallel looks like this: #!/bin/bash #PBS -N MrBayes #PBS -l select=1:ncpus=4:mpiprocs=4:mem=6gb:interconnect=1g,walltime=02:00:00 #PBS -j oe source /etc/profile.d/modules.sh module purge module add gcc/4.4 mpich2/1.4 export LD_LIBRARY_PATH=/home/galen/beagle-lib/lib:$LD_LIBRARY_PATH NCORES=`qstat -xf $PBS_JOBID | grep List.ncpus | sed 's/^.\\{26\\}//'` cd $PBS_O_WORKDIR mpiexec -n $NCORES /home/galen/mrbayes_3.2.1/src/mb mb_input > mb.log Paraview Using Paraview+GPUs to visualize very large datasets Paraview can also use multiple GPUs on Palmetto cluster to visualize very large datasets. For this, Paraview must be run in client-server mode. The \"client\" is your local machine on which Paraview must be installed, and the \"server\" is the Palmetto cluster on which the computations/rendering is done. The version of Paraview on the client needs to match exactly the version of Paraview on the server. The client must be running Linux. You can obtain the source code used for installation of Paraview 5.0.1 on Palmetto from /software/paraview/ParaView-v5.0.1-source.tar.gz . Copy this file to the client, extract it and compile Paraview. Compilation instructions can be found in the Paraview documentation . You will need to run the Paraview server on Palmetto cluster. First, log-in with X11 tunneling enabled, and request an interactive session: $ qsub -I -X -l select=4:ncpus=2:mpiprocs=2:ngpus=2:mem=32gb,walltime=1:00:00 In the above example, we request 4 nodes with 2 GPUs each. Next, launch the Paraview server: $ module add paraview/5.0 $ export DISPLAY=:0 $ mpiexec -n 8 pvserver -display :0 The server will be serving on a specific port number (like 11111) on this node. Note this number down. Next, you will need to set up \"port-forwarding\" from the lead node (the node your interactive session is running one) to your local machine. This can be done by opening a terminal running on the local machine, and typing the following: $ ssh -L 11111:nodeXYZ:11111 username@login.palmetto.clemson.edu Once port-forwarding is set up, you can launch Paraview on your local machine, rclone rclone is a command-line program that can be used to sync files and folders to and from cloud services such as Google Drive, Amazon S3, Dropbox, and many others . In this example, we will show how to use rclone to sync files to a Google Drive account, but the official documentation has specific instructions for other services. Setting up rclone for use with Google Drive on Palmetto To use rclone with any of the above cloud storage services, you must perform a one-time configuration. You can configure rclone to work with as many services as you like. For the one-time configuration, you will need to log-in with tunneling enabled . Once logged-in, ask for an interactive job: $ qsub -I -X Once the job starts, load the rclone module: $ module add rclone/1.23 After rclone is loaded, you must set up a \"remote\". In this case, we will configure a remote for Google Drive. You can create and manage a separate remote for each cloud storage service you want to use. Start by entering the following command: $ rclone config n) New remote q) Quit config n/q> Hit n then Enter to create a new remote host name> Provide any name for this remote host. For example: gmaildrive What type of source is it? Choose a number from below 1) amazon cloud drive 2) drive 3) dropbox 4) google cloud storage 5) local 6) s3 7) swift type> Provide any number for the remote source. For example choose number 2 for goolge drive. Google Application Client Id - leave blank normally. client_id> # Enter to leave blank Google Application Client Secret - leave blank normally. client_secret> # Enter to leave blank Remote config Use auto config? * Say Y if not sure * Say N if you are working on a remote or headless machine or Y didn't work y) Yes n) No y/n> Use y if you are not sure If your browser doesn't open automatically go to the following link: http://127.0.0.1:53682/auth Log in and authorize rclone for access Waiting for code... This will open the Firefox web browser, allowing you to log-in to your Google account. Enter your username and password then accept to let rclone access your Goolge drive. Once this is done, the browser will ask you to go back to rclone to continue. Got code -------------------- [gmaildrive] client_id = client_secret = token = {\"access_token\":\"xyz\",\"token_type\":\"Bearer\",\"refresh_token\":\"xyz\",\"expiry\":\"yyyy-mm-ddThh:mm:ss\"} -------------------- y) Yes this is OK e) Edit this remote d) Delete this remote y/e/d> Select y to finish configure this remote host. The gmaildrive host will then be created. Current remotes: Name Type ==== ==== gmaildrive drive e) Edit existing remote n) New remote d) Delete remote q) Quit config e/n/d/q> After this, you can quit the config using q , kill the job and exit this ssh session: $ exit Using rclone Whenever transfering files (using rclone or otherwise), login to the transfer node xfer01-ext.palmetto.clemson.edu . In MobaXterm, create a new ssh session with xfer01-ext.palmetto.clemson.edu as the Remote host In MacOS, open new terminal and ssh user@xfer01-ext.palmetto.clemson.edu Once logged-in, load the rclone module: $ module add rclone/1.23 You can check the content of the remote host gmaildrive : $ rclone ls gmaildrive: $ rclone lsd gmaildrive: You can use rclone to (for example) copy a file from Palmetto to any folder in your Google Drive: $ rclone copy /path/to/file/on/palmetto gmaildrive:/path/to/folder/on/drive Or if you want to copy to a specific destination on Google Drive back to Palmetto: $ rclone copy gmaildrive:/path/to/folder/on/drive /path/to/file/on/palmetto Additional rclone commands can be found here . Singularity Singularity is a tool for creating and running containers on HPC systems, similar to Docker . For further information on Singularity, and on downloading, building and running containers with Singularity, please refer to the Singularity documentation . This page provides information about singularity specific to the Palmetto cluster. Running Singularity Singularity is installed on all of the Palmetto compute nodes and the Palmetto LoginVMs, but it IS NOT present on the login.palmetto.clemson.edu node. To run singularity, you may simply run singularity or more specifically /bin/singularity . e.g. $ singularity --version singularity version 3.5.3-1.el7 An important change for existing singularity users Formerly, Palmetto administrators had installed singularity as a \"software module\" on Palmetto, but that is no longer the case. If your job scripts have any statements that use the singularity module, then those statements will need to be completely removed; otherwise, your job script may error. Remove any statements from your job scripts that resemble the following lines: module <some_command> singularity Where to download containers Containers can be downloaded from DockerHub DockerHub contains containers for various software packages, and Singularity is compatible with Docker images . SingularityHub The NVIDIA GPU Cloud for GPU-optimized images. Many individual projects contain specific instructions for installation via Docker and/or Singularity, and may host pre-built images in other locations. Example: Running OpenFOAM using Singularity As an example, we consider installing and running the OpenFOAM CFD solver using Singularity. OpenFOAM can be quite difficult to install manually, but singularity makes it very easy. This example shows how to use singularity interactively, but singularity containers can be run in batch jobs as well. Start by requesting an interactive job. NOTE: Singularity can only be run on the compute nodes and Palmetto Login VMs: $ qsub -I -l select=1:ngpus=2:ncpus=16:mpiprocs=16:mem=120gb,walltime=5:00:00 We recommend that all users store built singularity images in their /home directories. Singularity images can be quite large, so be sure to delete unused or old images: $ mkdir ~/singularity-images $ cd ~/singularity-images Next, we download the singularity image for OpenFOAM from DockerHub. This takes a few seconds to complete: $ singularity pull docker://openfoam/openfoam6-paraview54 Once the image is downloaded, we are ready to run OpenFOAM. We use singularity shell to start a container, and run a shell in the container. The -B option is used to \"bind\" the /scratch2/$USER directory to a directory named /scratch in the container. We also the --pwd option to specify the working directory in the running container (in this case /scratch ). This is always recommended. Typically, the working directory may be the $TMPDIR directory or one of the scratch directories. $ singularity shell -B /scratch2/atrikut:/scratch --pwd /scratch openfoam6-paraview54.simg Before running OpenFOAM commands, we need to source a few environment variables (this step is specific to OpenFOAM): $ source /opt/openfoam6/etc/bashrc Now, we are ready to run a simple example using OpenFOAM: $ cp -r $FOAM_TUTORIALS/incompressible/simpleFoam/pitzDaily . $ cd pitzDaily $ blockMesh $ simpleFoam The simulation takes a few seconds to complete, and should finish with the following output: smoothSolver: Solving for Ux, Initial residual = 0.00012056, Final residual = 7.8056e-06, No Iterations 6 smoothSolver: Solving for Uy, Initial residual = 0.000959834, Final residual = 6.43909e-05, No Iterations 6 GAMG: Solving for p, Initial residual = 0.00191644, Final residual = 0.000161493, No Iterations 3 time step continuity errors : sum local = 0.00681813, global = -0.000731564, cumulative = 0.941842 smoothSolver: Solving for epsilon, Initial residual = 0.000137225, Final residual = 8.98917e-06, No Iterations 3 smoothSolver: Solving for k, Initial residual = 0.000215144, Final residual = 1.30281e-05, No Iterations 4 ExecutionTime = 10.77 s ClockTime = 11 s SIMPLE solution converged in 288 iterations streamLine streamlines write: seeded 10 particles Tracks:10 Total samples:11980 Writing data to \"/scratch/pitzDaily/postProcessing/sets/streamlines/288\" End We are now ready to exit the container: $ exit Because the directory /scratch was bound to /scratch2/$USER , the simulation output is available in the directory /scratch2/$USER/pitzDaily/postProcessing/ : $ ls /scratch2/$USER/pitzDaily/postProcessing/ sets GPU-enabled software using Singularity containers (NVIDIA GPU Cloud) Palmetto also supports use of images provided by the NVIDIA GPU Cloud (NGC) . The provides GPU-accelerated HPC and deep learning containers for scientific computing. NVIDIA tests HPC container compatibility with the Singularity runtime through a rigorous QA process. Pulling NGC images Singularity images may be pulled directly from the Palmetto GPU compute nodes, an interactive job is most convenient for this. Singularity uses multiple CPU cores when building the image and so it is recommended that a minimum of 4 CPU cores are reserved. For instance to reserve 4 CPU cores, 2 NVIDIA Pascal GPUs, for 20 minutes the following could be used: $ qsub -I -lselect=1:ncpus=4:mem=2gb:ngpus=2:gpu_model=p100,walltime=00:20:00 Wait for the interactive job to give you control over the shell. Before pulling an NGC image, authentication credentials must be set. This is most easily accomplished by setting the following variables in the build environment. $ export SINGULARITY_DOCKER_USERNAME='$oauthtoken' $ export SINGULARITY_DOCKER_PASSWORD=<NVIDIA NGC API key> More information describing how to obtain and use your NVIDIA NGC API key can be found here . Once credentials are set in the environment, we\u2019re ready to pull and convert the NGC image to a local Singularity image file. The general form of this command for NGC HPC images is: $ singularity build <local_image> docker://nvcr.io/<registry>/<app:tag> This singularity build command will download the app:tag NGC Docker image, convert it to Singularity format, and save it to the local file named local_image. For example to pull the namd NGC container tagged with version 2.12-171025 to a local file named namd.simg we can run: $ singularity build ~/namd.simg docker://nvcr.io/hpc/namd:2.12-171025 After this command has finished we'll have a Singularity image file, namd.simg : Running NGC containers Running NGC containers on Palmetto presents few differences from the run instructions provided on NGC for each application. Application-specific information may vary so it is recommended that you follow the container specific documentation before running with Singularity. If the container documentation does not include Singularity information, then the container has not yet been tested under Singularity. As all NGC containers are optimized for NVIDIA GPU acceleration we will always want to add the --nv flag to enable NVIDIA GPU support within the container. The Singularity command below represents the standard form of the Singularity command used on the Palmetto cluster. It will mount the present working directory on the host to /host_pwd in the container process and set the present working directory of the container process to /host_pwd This means that when our process starts it will be effectively running in the host directory the singularity command was launched from. $ singularity exec --nv -B $(pwd):/host_pwd --pwd /host_pwd <image.simg> <cmd>","title":"Software on Palmetto"},{"location":"software/software/#software-installed-on-palmetto","text":"","title":"Software installed on Palmetto"},{"location":"software/software/#overview","text":"","title":"Overview"},{"location":"software/software/#modules","text":"A large number of popular software packages are installed on Palmetto and can be used without any setup or configuration. These include: Compilers (such as gcc , Intel, and PGI) Libraries (such as OpenMPI, HDF5, Boost) Programming languages (such as Python, MATLAB, R) Scientific applications (such as LAMMPS, Paraview, ANSYS) Others (e.g., Git, PostgreSQL, Singularity) These packages are available as modules on Palmetto. The following commands can be used to inspect, activate and deactivate modules: Command Purpose module avail List all packages available (on current system) module add package/version Add a package to your current shell environment module list List packages you have loaded module rm package/version Remove a currently loaded package module purge Remove all currently loaded packages See the Quick Start Guide for more details about modules.","title":"Modules"},{"location":"software/software/#licensed-software","text":"Many site-licensed software packages are available on Palmetto cluster (e.g., MATLAB, ANSYS, COMSOL, etc.,). There are limitations on the number of jobs that can run using these packages. See this section of the User's Guide on how to check license usage. Individual-owned or group-owned licensed software can also be run on Palmetto.","title":"Licensed software"},{"location":"software/software/#software-with-graphical-applications","text":"See this section of the User's Guide on how to use software with graphical user interface (GUI).","title":"Software with graphical applications"},{"location":"software/software/#installing-your-own-software","text":"See this section of the User's Guide on how to check license usage.","title":"Installing your own software"},{"location":"software/software/#abaqus","text":"ABAQUS is a Finite Element Analysis software used for engineering simulations. Currently, ABAQUS versions 6.10, 6.13, 6.14 are available on Palmetto cluster as modules. $ module avail abaqus abaqus/6.10 abaqus/6.13 abaqus/6.14 To see license usage of ABAQUS-related packages, you can use the lmstat command: /software/USR_LOCAL/flexlm/lmstat -a -c /software/USR_LOCAL/flexlm/licenses/abaqus.dat","title":"ABAQUS"},{"location":"software/software/#running-abaqus-interactive-viewer","text":"To run the interactive viewer, you must log-in with tunneling enabled , and then ask for an interactive session: $ qsub -I -X -l select=1:ncpus=8:mpiprocs=8:mem=6gb:interconnect=1g,walltime=00:15:00 Once logged-in to an interactive compute node, to launch the interactive viewer, load the abaqus module, and run the abaqus executable with the viewer and -mesa options: $ module add abaqus/6.14 $ abaqus viewer -mesa Similarly, to launch the ABAQUS CAE graphical interface: $ abaqus cae -mesa","title":"Running ABAQUS interactive viewer"},{"location":"software/software/#running-abaqus-in-batch-mode","text":"To run ABAQUS in batch mode on Palmetto cluster, you can use the job script in the following example as a template. This example shows how to run ABAQUS in parallel using MPI. This demonstration runs the \"Axisymmetric analysis of bolted pipe flange connections\" example provided in the ABAQUS documentation here . Please see the documentation for the physics and simulation details. You can obtain the files required to run this example using the following commands: $ cd /scratch2/username $ module add examples $ example get ABAQUS $ cd ABAQUS && ls abaqus_v6.env boltpipeflange_axi_element.inp boltpipeflange_axi_node.inp boltpipeflange_axi_solidgask.inp job.sh The .inp files describe the model and simulation to be performed - see the documentation for details. The batch script job.sh submits the job to the cluster. The .env file is a configuration file that must be included in all ABAQUS job submission folders on Palmetto. #!/bin/bash #PBS -N AbaqusDemo #PBS -l select=2:ncpus=8:mpiprocs=8:mem=6gb:interconnect=1g,walltime=00:15:00 #PBS -j oe module purge module add abaqus/6.14 pbsdsh sleep 20 NCORES=`wc -l $PBS_NODEFILE | gawk '{print $1}'` cd $PBS_O_WORKDIR SCRATCH=$TMPDIR # copy all input files into the scratch directory for node in `uniq $PBS_NODEFILE` do ssh $node \"cp $PBS_O_WORKDIR/*.inp $SCRATCH\" done cd $SCRATCH # run the abaqus program, providing the .inp file as input abaqus job=abdemo double input=$SCRATCH/boltpipeflange_axi_solidgask.inp scratch=$SCRATCH cpus=$NCORES mp_mode=mpi interactive # copy results back from scratch directory to $PBS_O_WORKDIR for node in `uniq $PBS_NODEFILE` do ssh $node \"cp -r $SCRATCH/* $PBS_O_WORKDIR\" done In the batch script job.sh : The following line extracts the total number of CPU cores available across all the nodes requested by the job: ~~~ NCORES= wc -l $PBS_NODEFILE | gawk '{print $1}' ~~~ The following line runs the ABAQUS program, specifying various options such as the path to the .inp file, the scratch directory to use, etc., ~~~ abaqus job=abdemo double input=/scratch2/$USER/ABAQUS/boltpipeflange_axi_solidgask.inp scratch=$SCRATCH cpus=$NCORES mp_mode=mpi interactive ~~~ To submit the job: $ qsub job.sh 9668628 After job completion, you will see the job submission directory ( /scratch2/username/ABAQUS ) populated with various files: $ ls AbaqusDemo.o9668628 abdemo.dat abdemo.msg abdemo.res abdemo.stt boltpipeflange_axi_solidgask.inp abaqus_v6.env abdemo.fil abdemo.odb abdemo.sim boltpipeflange_axi_element.inp job.sh abdemo.com abdemo.mdl abdemo.prt abdemo.sta boltpipeflange_axi_node.inp If everything went well, the job output file ( AbaqusDemo.o9668628 ) should look like this: [atrikut@login001 ABAQUS]$ cat AbaqusDemo.o9668628 Abaqus JOB abdemo Abaqus 6.14-1 Abaqus License Manager checked out the following licenses: Abaqus/Standard checked out 16 tokens from Flexnet server licensevm4.clemson.edu. <567 out of 602 licenses remain available>. Begin Analysis Input File Processor Mon 13 Feb 2017 12:35:29 PM EST Run pre Mon 13 Feb 2017 12:35:31 PM EST End Analysis Input File Processor Begin Abaqus/Standard Analysis Mon 13 Feb 2017 12:35:31 PM EST Run standard Mon 13 Feb 2017 12:35:35 PM EST End Abaqus/Standard Analysis Abaqus JOB abdemo COMPLETED +------------------------------------------+ | PALMETTO CLUSTER PBS RESOURCES REQUESTED | +------------------------------------------+ mem=12gb,ncpus=16,walltime=00:15:00 +-------------------------------------+ | PALMETTO CLUSTER PBS RESOURCES USED | +-------------------------------------+ cpupercent=90,cput=00:00:10,mem=636kb,ncpus=16,vmem=12612kb,walltime=00:00:13 The output database ( .odb ) file contains the results of the simulation which can be viewed using the ABAQUS viewer:","title":"Running ABAQUS in batch mode"},{"location":"software/software/#amber","text":"In this example we will use \"alp\" test from Amber's test suite. To get the files relevant to this example: $ module add examples $ example get Amber $ cd Amber && ls amber.pbs coords md.in prmtop README.md Examine the batch script amber.pbs , and adjust the names of the files i.e. input and coordinates files. Also, adjust the number of nodes, processors per node and walltime. To submit the job: qsub amber.pbs Results will be saved in the md.out file (in our example).","title":"Amber"},{"location":"software/software/#ansys","text":"","title":"ANSYS"},{"location":"software/software/#graphical-interfaces","text":"To run the various ANSYS graphical programs, you must log-in with tunneling enabled and then ask for an interactive session: $ qsub -I -X -l select=1:ncpus=8:mpiprocs=8:mem=6gb:interconnect=1g,walltime=01:00:00 Once logged-in to an interactive compute node, you must first load the ANSYS module along with the Intel module: $ module add ansys/17.2 $ module add intel/12.1 And then launch the required program: For ANSYS APDL $ ansys172 -g If you are using e.g., ANSYS 17.0 instead, then the executable is called ansys170 . For CFX $ cfxlaunch For ANSYS Workbench $ runwb2 For Fluent $ fluent For ANSYS Electromagnetics $ ansysedt","title":"Graphical Interfaces"},{"location":"software/software/#batch-mode","text":"To run ANSYS in batch mode on Palmetto cluster, you can use the job script in the following example as a template. This example shows how to run ANSYS in parallel (using multiple cores/nodes). In this demonstration, we model the strain in a 2-D flat plate. You can obtain the files required to run this example using the following commands: $ cd /scratch2/username $ module add examples $ example get ANSYS $ cd ANSYS && ls input.txt job.sh The input.txt batch file is generated for the model using the ANSYS APDL interface. The batch script job.sh submits the batch job to the cluster: #!/bin/bash #PBS -N ANSYSdis #PBS -l select=2:ncpus=4:mpiprocs=4:mem=11gb:interconnect=1g #PBS -l walltime=1:00:00 #PBS -j oe module purge module add ansys/17.2 cd $PBS_O_WORKDIR machines=$(uniq -c $PBS_NODEFILE | awk '{print $2\":\"$1}' | tr '\\n' :) for node in `uniq $PBS_NODEFILE` do ssh $node \"sleep 5\" ssh $node \"cp input.txt $TMPDIR\" done cd $TMPDIR ansys172 -dir $TMPDIR -j EXAMPLE -s read -l en-us -b -i input.txt -o output.txt -dis -machines $machines -usessh for node in `uniq $PBS_NODEFILE` do ssh $node \"cp -r $TMPDIR/* $PBS_O_WORKDIR\" done In the batch script job.sh : The following line extracts the nodes (machines) available for this job as well as the number of CPU cores allocated for each node: ~~~ machines=$(uniq -c $PBS_NODEFILE | awk '{print $2\":\"$1}' | tr '\\n' :) ~~~ For ANSYS jobs, you should always use $TMPDIR ( /local_scratch ) as the working directory. The following lines ensure that $TMPDIR is created on each node: ~~~ do ssh $node \"sleep 5\" ssh $node \"cp input.txt $TMPDIR\" done ~~~ The following line runs the ANSYS program, specifying various options such as the path to the input.txt file, the scratch directory to use, etc., ~~~ ansys172 -dir $TMPDIR -j EXAMPLE -s read -l en-us -b -i input.txt -o output.txt -dis -machines $machines -usessh ~~~ Finally, the following lines copy all the data from $TMPDIR : ~~~ do ssh $node \"cp -r $TMPDIR/* $PBS_O_WORKDIR\" done ~~~ To submit the job: $ qsub job.sh 9752784 After job completion, you will see the job submission directory ( /scratch2/username/ANSYS ) populated with various files: $ ls ANSYSdis.o9752784 EXAMPLE0.stat EXAMPLE2.err EXAMPLE3.esav EXAMPLE4.full EXAMPLE5.out EXAMPLE6.rst EXAMPLE.DSP input.txt EXAMPLE0.err EXAMPLE1.err EXAMPLE2.esav EXAMPLE3.full EXAMPLE4.out EXAMPLE5.rst EXAMPLE7.err EXAMPLE.esav job.sh EXAMPLE0.esav EXAMPLE1.esav EXAMPLE2.full EXAMPLE3.out EXAMPLE4.rst EXAMPLE6.err EXAMPLE7.esav EXAMPLE.mntr mpd.hosts EXAMPLE0.full EXAMPLE1.full EXAMPLE2.out EXAMPLE3.rst EXAMPLE5.err EXAMPLE6.esav EXAMPLE7.full EXAMPLE.rst mpd.hosts.bak EXAMPLE0.log EXAMPLE1.out EXAMPLE2.rst EXAMPLE4.err EXAMPLE5.esav EXAMPLE6.full EXAMPLE7.out host.list output.txt EXAMPLE0.rst EXAMPLE1.rst EXAMPLE3.err EXAMPLE4.esav EXAMPLE5.full EXAMPLE6.out EXAMPLE7.rst host.list.bak If everything went well, the job output file ( ANSYSdis.o9752784 ) should look like this: +------------------------------------------+ | PALMETTO CLUSTER PBS RESOURCES REQUESTED | +------------------------------------------+ mem=22gb,ncpus=8,walltime=01:00:00 +-------------------------------------+ | PALMETTO CLUSTER PBS RESOURCES USED | +-------------------------------------+ cpupercent=27,cput=00:00:17,mem=3964kb,ncpus=8,vmem=327820kb,walltime=00:01:07 The results file ( EXAMPLE.rst ) contains the results of the simulation which can be viewed using the ANSYS APDL graphical interface:","title":"Batch Mode"},{"location":"software/software/#comsol","text":"COMSOL is an application for solving Multiphysics problems. To see the available COMSOL modules on Palmetto: $ module avail comsol comsol/4.3b comsol/4.4 comsol/5.0 comsol/5.1 comsol/5.2 comsol/5.3 To see license usage of COMSOL-related packages, you can use the lmstat command: /software/USR_LOCAL/flexlm/lmstat -a -c /software/USR_LOCAL/flexlm/licenses/comsol.dat","title":"COMSOL"},{"location":"software/software/#graphical-interface","text":"To run the COMSOL graphical interface, you must log-in with tunneling enabled , and then ask for an interactive session: $ qsub -I -X -l select=1:ncpus=8:mpiprocs=8:mem=6gb:interconnect=1g,walltime=00:15:00 Once logged-in to an interactive compute node, to launch the interactive viewer, you can use the comsol command to run COMSOL: $ module add comsol/5.2 $ comsol -np 8 -tmpdir $TMPDIR The -np option can be used to specify the number of CPU cores to use. Remember to always use $TMPDIR as the working directory for COMSOL jobs.","title":"Graphical Interface"},{"location":"software/software/#batch-mode_1","text":"To run COMSOL in batch mode on Palmetto cluster, you can use the example batch scripts below as a template. The first example demonstrates running COMSOL using multiple cores on a single node, while the second demonstrates running COMSOL across multiple nodes using MPI. You can obtain the files required to run this example using the following commands: $ module add examples $ example get COMSOL $ cd COMSOL && ls job.sh job_mpi.sh Both of these examples run the \"Heat Transfer by Free Convection\" application described here . In addition to the job.sh and job_mpi.sh scripts, to run the examples and reproduce the results, you will need to download the file free_convection.mph (choose the correct version) provided with the description (login required).","title":"Batch Mode"},{"location":"software/software/#comsol-batch-job-on-a-single-node-using-multiple-cores","text":"#!/bin/bash #PBS -N COMSOL #PBS -l select=1:ncpus=8:mem=32gb,walltime=01:30:00 #PBS -j oe module purge module add comsol/5.2 cd $PBS_O_WORKDIR comsol batch -np 8 -tmpdir $TMPDIR -inputfile free_convection.mph -outputfile free_convection_output.mph","title":"COMSOL batch job on a single node, using multiple cores:"},{"location":"software/software/#comsol-batch-job-across-several-nodes","text":"#!/bin/bash #PBS -N COMSOL #PBS -l select=2:ncpus=8:mpiprocs=8:mem=32gb,walltime=01:30:00 #PBS -j oe module purge module add comsol/5.2 cd $PBS_O_WORKDIR uniq $PBS_NODEFILE > comsol_nodefile comsol batch -clustersimple -f comsol_nodefile -tmpdir $TMPDIR -inputfile free_convection.mph -outputfile free_convection_output.mph","title":"COMSOL batch job across several nodes"},{"location":"software/software/#gephi","text":"Gephi is an interactive visualization and exploration platform for all kinds of networks and complex systems, dynamic and hierarchical graphs. At this time, we don't have Gephi setup as a software module on Palmetto, but downloading and installing it in your own /home or /newscratch directory (and running it from that location) is easy to do. Below are the procedures I used to do this:","title":"Gephi"},{"location":"software/software/#download-and-setup-the-gephi-binary","text":"wget https://launchpad.net/gephi/0.8/0.8.2beta/+download/gephi-0.8.2-beta.tar.gz tar -zxf gephi-0.8.2-beta.tar.gz Now, the gephi binary is installed in ~/gephi/bin .","title":"Download and Setup the Gephi Binary"},{"location":"software/software/#running-gephi","text":"Connect to Palmetto with X11 tunneling enabled so you can \"tunnel\" the GUI running on Palmetto through your SSH connection to your desktop: ssh -X galen@login.palmetto.clemson.edu Once you're logged-in to Palmetto, you'll need to launch an interactive job with X11 tunneling enabled. Be sure to request an allocation with enough memory for Gephi to handle your input data properly (here, I'm requesing 31 GB RAM): qsub -I -X -l select=1:ncpus=4:mem=31gb,walltime=6:00:00 If you are using a Windows system, you'll need to launch a local Xserver (like Xming) so your system can display the tunneled GUI information. Once your local Xserver is ready, launch the Gephi GUI in your interactive session on Palmetto: /home/galen/gephi/bin/gephi Now, I can browse for data I have stored in my directories on Palmetto. If your data is stored on your local workstation, you must move that data to Palmetto so you can open it with Gephi. You may experience a short delay as the GUI is generated and tunneled to your desktop. Responsiveness between your local workstation and the tunneled GUI may be a bit slow/laggy, but the compute operations taking place on that Palmetto compute node are not delayed.","title":"Running Gephi"},{"location":"software/software/#gromacs","text":"Various installations of GROMACS are available on the cluster. Different modules are provided for GPU-enabled and non-GPU versions of GROMACS: $ module avail gromacs ---------------------------------------------------------------- /software/modulefiles ----------------------------------------------------------------- gromacs/4.5.4-sp gromacs/4.6.5-sp-k20-ompi gromacs/5.0.1-sp-k20-g481-o181 gromacs/5.0.5-nogpu gromacs/4.6.5-dp-ompi gromacs/5.0.1-dp-g481-o181 gromacs/5.0.5-gpu ------------------------------------------------------------ /usr/share/Modules/modulefiles ------------------------------------------------------------ gromacs/4.5.4-sp","title":"GROMACS"},{"location":"software/software/#running-gromacs-without-gpu","text":"To use the non-GPU version of GROMACS, here is an example interactive job: $ qsub -I -l select=2:ncpus=20:phase=10:mem=100gb:mpiprocs=20,walltime=1: 00:00 $ module load gromacs/5.0.5-nogpu $ mpirun mdrun_mpi ... where ... specifies the input files and options of Gromacs. And an example of a PBS batch script for submitting a GROMACS batch script: #!/bin/bash #PBS -l select=2:ncpus=20:phase=10:mem=100gb:mpiprocs=20 #PBS -l walltime=1:00:00 module load gromacs/5.0.5-nogpu cd $PBS_O_WORKDIR export OMP_NUM_THREADS=1 mpirun mdrun_mpi ...","title":"Running GROMACS without GPU"},{"location":"software/software/#running-gpu-enabled-gromacs","text":"For the GPU version #!/bin/bash #PBS -l select=2:ncpus=20:ngpus=2:phase=10:mem=100gb:mpiprocs=2 #PBS -l walltime=1:00:00 module load gromacs/5.0.5-gpu cd $PBS_O_WORKDIR export OMP_NUM_THREADS=10 mpirun mdrun_mpi ... Note that the number of MPI processes per chunk is set as 2 when using the GPU. Normally, the number of MPI processes is the same as the number of CPU cores requested. But because there are only 2 GPUs per node, we launch only two MPI processes. In the above example, each MPI process can also use 10 CPU cores in addition to a GPU (this is enabled by setting the variable OMP_NUM_THREADS to 10).","title":"Running GPU-enabled GROMACS"},{"location":"software/software/#gpu-enabled-gromacs-using-singularity-and-ngc","text":"The NVIDIA GPU cloud provides images for GPU-enabled GROMACS that can be downloaded and run using Singularity on the Palmetto cluster. This is the recommended way to run GROMACS on Palmetto.","title":"GPU-enabled GROMACS using Singularity and NGC"},{"location":"software/software/#downloading-the-image","text":"Before downloading images from NGC, you will need to obtain an NVIDIA NGC API key, instructions for which can be found here . Start an interactive job, and create a directory for storing Singularity images (if you don't have one already): $ qsub -I -l select=1:ngpus=2:ncpus=16:mem=20gb:gpu_model=p100,walltime=5:00:00 $ mkdir -p singularity-images Set the required environment variables (you will need the API key for this step): $ export SINGULARITY_DOCKER_USERNAME='$oauthtoken' $ export SINGULARITY_DOCKER_PASSWORD=<NVIDA NGC API key> Navigate to the singularity-images directory and pull the GROMACS image (choose from the images listed here : $ cd ~/singularity-images $ module load singularity $ singularity pull docker://nvcr.io/hpc/gromacs:2016.4 The above should take a few seconds to complete.","title":"Downloading the image"},{"location":"software/software/#running-gromacs-interactively","text":"As an example, we'll consider running the GROMACS ADH benchmark. First, request an interactive job: $ qsub -I -l select=1:ngpus=2:ncpus=16:mpiprocs=16:mem=120gb:gpu_model=p100,walltime=5:00:00 Load the singularity module: $ module load singularity Prepare the input and output directories: In this case, we use /scratch3/$USER/gromacs_ADH as both the input directory (containing the data) and the output directory (where output will be stored). Remember that $TMPDIR is cleaned up after the job completes, so it's important to move any data out of $TMPDIR after GROMACS completes running. Also, in this case, we are downloading the input data from the web, but if your input is stored in the /home /, /scratch or some other directory, then you can copy it to $TMPDIR : $ mkdir -p /scratch3/$USER/gromacs_ADH_benchmark $ cd /scratch3/$USER/gromacs_ADH_benchmark $ wget ftp://ftp.gromacs.org/pub/benchmarks/ADH_bench_systems.tar.gz $ tar -xvf ADH_bench_systems.tar.gz Use singularity shell to interactively run a container from the downloaded GROMACS image. The -B switch is used to \"bind\" directories on the host (Palmetto compute node) to directories in the container. The different bindings used are: /scratch3/$USER/gromacs_ADH_benchmark is bound to /input /scratch3/$USER/gromacs_ADH_benchmark is bound to /output $TMPDIR is bound to /work $ singularity shell --nv \\ -B /scratch3/$USER/gromacs_ADH_benchmark:/input \\ -B /scratch3/$USER/gromacs_ADH_benchmark:/output \\ -B $TMPDIR:/work \\ --pwd /work \\ ~/singularity-images/gromacs-2016.4.simg \\ Running the benchmark: $ source /opt/gromacs/install/2016.4/bin/GMXRC $ gmx_mpi grompp -f /input/adh_cubic/pme_verlet.mdp -c /input/adh_cubic/conf.gro -p /input/adh_cubic/topol.top $ export OMP_NUM_THREADS=8 $ mpirun -np 2 gmx_mpi mdrun -g adh_cubic.log -pin on -resethway -v -noconfout -nsteps 10000 -s topol.tpr -ntomp $OMP_NUM_THREADS After the last command above completes, the .edr and .log files produced by GROMACS should be visible. Typically, the next step is to copy these results to the output directory: $ cp *.log *.edr /output $ exit Upon exiting the container, the .log and .edr files will be found in the output directory, /scratch3/$USER/gromacs_ADH_benchmark .","title":"Running GROMACS interactively"},{"location":"software/software/#running-gromacs-in-batch-mode","text":"The same benchmark can be run in batch mode by encapsulating the commands in a script run_adh.sh , and running it non-interactively using singularity exec . # run_adh.sh source /opt/gromacs/install/2016.4/bin/GMXRC gmx_mpi grompp -f /input/adh_cubic/pme_verlet.mdp -c /input/adh_cubic/conf.gro -p /input/adh_cubic/topol.top export OMP_NUM_THREADS=8 mpirun -np 2 gmx_mpi mdrun -g adh_cubic.log -pin on -resethway -v -noconfout -nsteps 10000 -s topol.tpr -ntomp $OMP_NUM_THREADS cp *.log *.edr /output The PBS batch script for submitting the above (assuming that the /scratch3/$USER/gromacs_ADH_benchmark directory already contains the input files): #PBS -N adh_cubic #PBS -l select=1:ngpus=2:ncpus=16:mem=20gb:gpu_model=p100,walltime=5:00:00 module load singularity cd $PBS_O_WORKDIR cp run_adh.sh $TMPDIR singularity exec --nv \\ -B /scratch3/$USER/gromacs_ADH_benchmark:/input \\ -B /scratch3/$USER/gromacs_ADH_benchmark:/output \\ -B $TMPDIR:/work \\ --pwd /work \\ ~/singularity-images/gromacs-2016.4.simg bash run_adh.sh","title":"Running GROMACS in batch mode"},{"location":"software/software/#hoomd","text":"","title":"HOOMD"},{"location":"software/software/#run-hoomd","text":"Now the HOOMD-BLUE v2.3.5 has been installed. Create a simple python file \u201ctest_hoomd.py\u201d to run HOOMD import hoomd import hoomd.md hoomd.context.initialize(\"\"); hoomd.init.create_lattice(unitcell=hoomd.lattice.sc(a=2.0), n=5); nl = hoomd.md.nlist.cell(); lj = hoomd.md.pair.lj(r_cut=2.5, nlist=nl); lj.pair_coeff.set('A', 'A', epsilon=1.0, sigma=1.0); hoomd.md.integrate.mode_standard(dt=0.005); all = hoomd.group.all(); hoomd.md.integrate.langevin(group=all, kT=0.2, seed=42);hoomd.analyze.log(filename=\"log-output.log\", quantities=['potential_energy', 'temperature'], period=100, overwrite=True); hoomd.dump.gsd(\"trajectory.gsd\", period=2e3, group=all, overwrite=True); hoomd.run(1e4); If you have logged out of the node, request an interactive session on a GPU node and add required modules: $ qsub -I -l select=1:ncpus=16:mem=64gb:ngpus=2:gpu_model=p100:mpiprocs=16:interconnect=fdr,walltime=2:00:00 $ module add anaconda3/5.1.0 gcc/5.4.0 cuda-toolkit/9.0.176 Run the script interactively: $ python test_hoomd.py Alternatively, you can setup a PBS job script to run HOOMD in batch mode. A sample is below for Test_Hoomd.sh : #PBS -N HOOMD #PBS -l select=1:ncpus=16:mem=64gb:ngpus=2:gpu_model=p100:mpiprocs=16:interconnect=fdr,walltime=02:00:00 #PBS -j oe module purge module add anaconda3/5.1.0 gcc/5.4.0 cuda-toolkit/9.0.176 cd $PBS_O_WORKDIR python test_hoomd.py Submit the job: $ qsub Test_Hoomd.sh This is it","title":"Run HOOMD"},{"location":"software/software/#java","text":"The Java Runtime Environment (JRE) version 1.6.0_11 is currently available cluster-wide on Palmetto. If a user needs a different version of Java, or if the Java Development Kit (JDK, which includes the JRE) is needed, that user is encouraged to download and install Java (JRE or JDK) for herself. Below is a brief overview of installing the JDK in a user's /home directory.","title":"Java"},{"location":"software/software/#jre-vs-jdk","text":"The JRE is basically the Java Virtual Machine (Java VM) that provides a platform for running your Java programs. The JDK is the fully featured Software Development Kit for Java, including the JRE, compilers, and tools like JavaDoc and Java Debugger used to create and compile programs. Usually, when you only care about running Java programs, the JRE is all you'll need. If you are planning to do some Java programming, you will need the JDK.","title":"JRE vs. JDK"},{"location":"software/software/#downloading-the-jdk","text":"The JDK cannot be downloaded directly using the wget utility because a user must agree to Oracle's Java license ageement when downloading. So, download the JDK using a web browser and transfer the downloaded jdk-7uXX-linux-x64.tar.gz file to your /home directory on Palmetto using scp , sftp , or FileZilla: scp jdk-7u45-linux-x64.tar.gz galen@login.palmetto.clemson.edu:/home/galen jdk-7u45-linux-x64.tar.gz 100% 132MB 57.7KB/s 38:58","title":"Downloading the JDK"},{"location":"software/software/#installing-the-jdk","text":"The JDK is distributed in a Linux x86_64 compatible binary format, so once it has been unpacked, it is ready to use (no need to compile). However, you will need to setup your environment for using this new package by adding lines similar to the following at the end of your ~/.bashrc file: export JAVA_HOME=/home/galen/jdk1.7.0_45 export PATH=$JAVA_HOME/bin:$PATH export MANPATH=$JAVA_HOME/man:$MANPATH Once this is done, you can log-out and log-in again or simply source your ~/.bashrc file and then you'll be ready to begin using your new Java installation.","title":"Installing the JDK"},{"location":"software/software/#julia","text":"Julia: high-level dynamic programming language that was originally designed to address the needs of high-performance numerical analysis and computational science.","title":"Julia"},{"location":"software/software/#run-julia-in-palmetto-interactive","text":"There are a few different versions of Julia available on the cluster. $ module avail julia --------------------------------------------- /software/modulefiles --------------------------------------------- julia/0.6.2 julia/0.7.0 julia/1.0.4 julia/1.1.1 Let demonstrate how to use julia/1.1.1 in the Palmetto cluster together with Gurobi Optimizer (a commercial optimization solver for linear programming), quadratic programming, etc. Clemson University has different version of licenses for Gurobi solver. In this example, I would like to use Julia and Gurobi solver to solve a linear math problem using Palmetto HPC Problems: Maximize x+y Given the following constrains: 50 x + 24 y <= 2400 30 x + 33 y <= 2100 x >= 5, y >= 45 Let prepare a script to solve this problem, named: jump_gurobi.jl. You can save this file to: /scratch1/$username/Julia/ # Request for a compute node: $ qsub -I -l select=1:ncpus=8:mem=16gb:interconnect=fdr,walltime=01:00:00 # Go to working folder: $ cd /scratch1/$username/Julia $ nano jump_gurobi.jl Then type/copy the following code to the file jump_gurobi.jl import Pkg using JuMP using Gurobi m = Model(with_optimizer(Gurobi.Optimizer)) @variable(m, x >= 5) @variable(m, y >= 45) @objective(m, Max, x + y) @constraint(m, 50x + 24y <= 2400) @constraint(m, 30x + 33y <= 2100) status = optimize!(m) println(\" x = \", JuMP.value(x), \" y = \", JuMP.value(y)) Save the jump_gurobi.jl file then you are ready to run julia: $ module add julia/1.1.1 gurobi/7.0.2 $ julia # the julia prompt appears: julia> # Next install Package: JuMP and Gurobi julia> using Pkg julia> Pkg.add(\"JuMP\") julia> Pkg.add(\"Gurobi\") julia> exit() Run the julia_gurobi.jl script: $ julia jump_gurobi.jl","title":"Run Julia in Palmetto: Interactive"},{"location":"software/software/#run-julia-in-palmetto-batch-mode","text":"Alternatively, you can setup a PBS job script to run Julia in batch mode. A sample is below for submit_julia.sh : You must install the JuMP and Gurobi package first (one time installation) #!/bin/bash #PBS -N Julia #PBS -l select=1:ncpus=8:mem=16gb:interconnect=fdr #PBS -l walltime=02:00:00 #PBS -j oe module purge module add julia/1.1.1 gurobi/7.0.2 cd $PBS_O_WORKDIR julia jump_gurobi.jl > output_JuMP.txt Submit the job: $ qsub submit_julia.sh The output file can be found at the same folder: output_JuMP.txt","title":"Run Julia in Palmetto: Batch mode"},{"location":"software/software/#install-your-own-julia-package-using-conda-environment-and-running-in-jupyterhub","text":"In addition to traditional compilation of Julia, it is possible to install your own version of Julia and setup kernel to work using Jupterhub. # Request for a compute node: $ qsub -I -l select=1:ncpus=8:mem=16gb:interconnect=fdr,walltime=01:00:00 $ module add anaconda3/5.1.0 # Create conda environment with the name as \"Julia\" $ conda create -n Julia -c conda-forge julia $ source activate Julia (Julia) [$username@node1234 ~]$ (Julia) [$username@node1234 ~]$ julia julia> julia> using Pkg julia> Pkg.add(\"IJulia\") julia> exit Exit Julia and Start Jupyterhub in Palmetto After spawning, Click on New kernel in Jupyterhub, you will see Julia 1.1.1 kernel available for use Type in the follwing code to test: println(\"Hello world\")","title":"Install your own Julia package using conda environment and running in Jupyterhub"},{"location":"software/software/#lammps","text":"There are a few different versions of LAMMPS available on the cluster, but users are encouraged to install their own version of LAMMPS in case newer versions different configurations are desired. In particular, there are two main components of LAMMPS which can be run with CPUs (lmp_mpi) or with GPUs (lmp) $ module avail lammps lammps/10Jan15-dp lammps/17Dec13-dp lammps/17Dec13-dp-k20 lammps/2018Dec12 lammps/29Aug14-sp-k20","title":"LAMMPS"},{"location":"software/software/#installing-lammps-on-palmetto-cluster","text":"In this example, we will demonstrate installing LAMMPS to run with CPUs and GPUs (version 7 Aug 2019 ). Detail information can be found here: https://lammps.sandia.gov/doc/Install.html After logging in, ask for an interactive session (with GPU): ~~~ $ qsub -I -l select=1:ncpus=8:mpiprocs=8:ngpus=1:mem=64gb:gpu_model=v100,walltime=8:00:00 ~~~ Load the required modules. Specifically, note that we are loading the CUDA-enabled module: ~~~~ $ module load openmpi/3.1.4-gcc71-ucx fftw/3.3.4-g481 cuda-toolkit/9.2 cmake/3.13.1 ~~~~ Download the LAMMPS source code from http://lammps.sandia.gov/download.html. The detailed instruction on usage and compilation options are available at http://lammps.sandia.gov/doc/Manual.html. Unpack the source code and enter the package directory: ~~~ $ tar -xvf lammps-stable.tar.gz $ cd lammps-7Aug19 ~~~ There are two different methods of compiling LAMMPS using make (support CPUs) or cmake (support GPUs). We introduce both methods here: Compile LAMMPS using make with KOKKOS support ~~~ $ cd src $ make yes-kokkos $ make mpi -j8 ~~~ A new executable file lmp_mpi will be produced in the src folder. You can copy this executable over to the working directory of your jobs. Note that the size of the executable may be a hundred MB Note: there are optional packages which can be installed together with the above package using make yes- prior to running make mpi . For example: ~~~ $ make yes-USER-MISC $ make yes-KSPACE $ make yes-MOLECULE $ make yes-MISC $ make yes-manybody $ make yes-dipole $ make yes-class2 $ make yes-asphere $ make yes-replica $ make yes-granular $ make yes-rigid ~~~ Compile LAMMPS using cmake with GPU support: The detailed manual can be found here with more information on the options to be included. In this instruction, we only cover a certain number of options that are enough install cmake with GPU: ~~~ $ mkdir build $ cd build $ cmake ../cmake/ -DPKG_GPU=on -DGPU_API=cuda -DGPU_PREC=double -DGPU_ARCH=sm_70 -DCUDA_CUDA_LIBRARY=/usr/lib64/libcuda.so -DFFMPEG_EXECUTABLE=/software/ffmpeg/3.3.2/bin/ffmpeg -DPKG_MC=on -DPKG_REPLICA=on -DPKG_SNAP=on -DPKG-MANYBODY=on -DPKG_USER-MEAMC=on -DPKG_USER-MISC=on -DPKG_MISC=on $ make -j8 ~~~ In the above script; the -DGPU_ARCH=sm_70 was used because the gpu_model was set at Volta (v100). Please refer to the above guideline for setting particular gpu architecture Note: there are optional packages which can be installed together with the above package using -D[Options]=on . For example: ~~~ $ cmake ../cmake/ -DPKG_MISC=on -DPKG_KSPACE=on -DPKG_MOLECULE=on -DPKG_MANYBODY=on -DPKG_DIPOLE=on -DPKG_CLASS2=on -DPKG_ASPHERE=on -DPKG_REPLICA=on -DPKG_GRANULAR=on -DPKG_RIGID=on ~~~ The executable file lmp will be produced. You can copy this executable over to the working directory of your jobs. Note that the size of the executable may be a hundred MB.","title":"Installing LAMMPS on Palmetto cluster"},{"location":"software/software/#running-lammps-an-example","text":"Several existing examples are in the installed folder: lammps-7Aug19/examples/ Detailes description of all examples are here . In order to run the example, simply copy the executable files created from step 5 and 6 to particular example folder and follow the README for detailed description on how to run. For instance, in order to run an example accelerate using GPU package. Copy lmp to that folder. Here is a sample batch script job.sh for this example: #PBS -N accelerate #PBS -l select=1:ncpus=8:mpiprocs=8:ngpus=1:gpu_model=v100:mem=64gb,walltime=1:00:00 #PBS -j oe module purge module load openmpi/3.1.4-gcc71-ucx mpirun -np 8 lmp -sf gpu < in.lj # 8 MPI, 8 MPI/GPU","title":"Running LAMMPS - an example"},{"location":"software/software/#matlab","text":"","title":"MATLAB"},{"location":"software/software/#checking-license-usage-for-matlab","text":"You can check the availability of MATLAB licenses using the lmstat command: $ /software/USR_LOCAL/flexlm/lmstat -a -c /software/USR_LOCAL/flexlm/licenses/matlab.dat","title":"Checking license usage for MATLAB"},{"location":"software/software/#running-the-matlab-graphical-interface","text":"To launch the MATLAB graphical interface, you must first you must log-in with tunneling enabled , and then ask for an interactive session: $ qsub -I -X -l select=1:ncpus=2:mem=24gb,walltime=1:00:00 Once logged-in, you must load one of the MATLAB modules: $ module add matlab/2018b And then launch the MATLAB program: $ matlab Warning : DO NOT attempt to run MATLAB right after logging-in (i.e., on the login001 node). Always ask for an interactive job first. MATLAB sessions are automatically killed on the login node.","title":"Running the MATLAB graphical interface"},{"location":"software/software/#running-the-matlab-command-line-without-graphics","text":"To use the MATLAB command-line interface without graphics, you can additionally use the -nodisplay and -nosplash options: $ matlab -nodisplay -nosplash To quit matlab command-line interface, type: $ exit","title":"Running the MATLAB command line without graphics"},{"location":"software/software/#matlab-in-batch-jobs","text":"To use MATLAB in your batch jobs, you can use the -r switch provided by MATLAB, which lets you r un commands specified on the command-line. For example: $ matlab -nodisplay -nosplash -r myscript will run the MATLAB script myscript.m , Or: $ matlab -nodisplay -nosplash < myscript.m > myscript_results.txt will run the MATLAB script myscript.m and write the output to myscript_results.txt file. Thus, an example batch job using MATLAB could have a batch script as follows: #!/bin/bash # #PBS -N test_matlab #PBS -l select=1:ncpus=1:mem=5gb #PBS -l walltime=1:00:00 module add matlab/2018b cd $PBS_O_WORKDIR taskset -c 0-$(($OMP_NUM_THREADS-1)) matlab -nodisplay -nosplash < myscript.m > myscript_results.txt Note : MATLAB will sometimes attemps to use all available CPU cores on the node it is running on. If you haven't reserved all cores on the node, your job may be killed if this happens. To avoid this, you can use the taskset utility to set the \"core affinity\" (as shown above). As an example: $ taskset 0-2 <application> will limit application to using 3 CPU cores. On Palmetto, the variable OMP_NUM_THREADS is automatically set to be the number of cores requested for a job. Thus, you can use 0-$((OMP_NUM_THREADS-1)) as shown in the above batch script to use all the cores you requested.","title":"MATLAB in batch jobs"},{"location":"software/software/#compiling-matlab-code-to-create-an-executable","text":"Often, you need to run a large number of MATLAB jobs concurrently (e.g,m each job operating on different data). In such cases, you can avoid over-utilizing MATLAB licenses by compiling your MATLAB code into an executable. This can be done from within the MATLAB command-line as follows: $ matlab -nodisplay -nosplash >> mcc -R -nodisplay -R -singleCompThread -R -nojvm -m mycode.m Note: MATLAB will try to use all the available CPU cores on the system where it is running, and this presents a problem when your compiled executable on the cluster where available cores on a single node might be shared amongst mulitple users. You can disable this \"feature\" when you compile your code by adding the -R -singleCompThread option, as shown above. The above command will produce the executable mycode , corresponding to the M-file mycode.m . If you have multiple M-files in your project and want to create a single excutable, you can use a command like the following: >> mcc -R -nodisplay -R -singleCompThread -R -nojvm -m my_main_code.m myfunction1.m myfunction2.m myfunction3.m Once the executable is produced, you can run it like any other executable in your batch jobs. Of course, you'll also need the same matlab and (optional) GCC module loaded for your job's runtime environment.","title":"Compiling MATLAB code to create an executable"},{"location":"software/software/#mothur","text":"This page instructs how to install mothur software to Palmetto The code and the issue tracker can be found here Request an interactive session. For example: $ qsub -I -l select=1:ncpus=6:mem=24gb:interconnect=fdr,walltime=3:00:00 Load the required modules $ module load boost/1.65.1 hdf5/1.10.1 openmpi/1.10.3 gcc/8.2.0 Download mothur from source . Here we download the latest version 1.41.3 $ wget https://github.com/mothur/mothur/archive/v1.41.3.tar.gz Unpack the downloaded file and go into mothur source folder $ tar -xvf v1.41.3.tar.gz $ cd mothur-1.41.3 Modify the Makefile $ nano Makefile #Replace the following lines 22-32 with information: (Note: username should be replaced) OPTIMIZE ?= yes USEREADLINE ?= yes USEBOOST ?= yes USEHDF5 ?= yes LOGFILE_NAME ?= no BOOST_LIBRARY_DIR ?= \"/software/boost/1.65.1/lib\" BOOST_INCLUDE_DIR ?= \"/software/boost/1.65.1/include\" HDF5_LIBRARY_DIR ?= \"/software/hdf5/1.10.1/lib\" HDF5_INCLUDE_DIR ?= \"/software/hdf5/1.10.1/include\" MOTHUR_FILES ?= \"\\\"home/username/application/bin/mothur\\\"\" VERSION = \"\\\"1.41.3\\\"\" Run make file $ make The mothur executable file will be created in the installation folder: /home/username/applications/bin . Make sure you set the correct environment PATH in ~/.bashrc file export PATH=$PATH:$HOME/applications/bin","title":"mothur"},{"location":"software/software/#mrbayes","text":"MrBayes is a program for Bayesian inference and model choice across a wide range of phylogenetic and evolutionary models. MrBayes uses Markov chain Monte Carlo (MCMC) methods to estimate the posterior distribution of model parameters.","title":"MrBayes"},{"location":"software/software/#installing-beagle-library","text":"BEAGLE library is supported to dramatic speedups for codon and amino acid models on compatible hardware (NVIDIA graphics cards); To install MrBayes in your /home directory on Palmetto, you'll need to begin by installing the BEAGLE library. Here are the steps, starting with checking-out the source code using Subversion: [galen@login001 ~]$ cd [galen@login001 ~]$ svn checkout http://beagle-lib.googlecode.com/svn/trunk/ beagle-setup [galen@login001 ~]$ cd beagle-setup [galen@login001 beagle-setup]$ ./autogen.sh [galen@login001 beagle-setup]$ ./configure --prefix=/home/galen/beagle-lib [galen@login001 beagle-setup]$ make install Once installed, I also needed to add the location of these new libraries to my LD_LIBRARY_PATH (this can be done in your ~/.bashrc file, or in your PBS job script as I have done at the bottom of this section): [galen@login001 beagle-setup]$ export LD_LIBRARY_PATH=/home/galen/beagle-lib/lib:$LD_LIBRARY_PATH Finally, verify that everything is setup properly: [galen@login001 beagle-setup]$ make check","title":"Installing BEAGLE Library"},{"location":"software/software/#installing-mrbayes","text":"You can build a parallel version of MrBayes using OpenMPI or MPICH2. I used MPICH2 and I included the BEAGLE libraries when compiling MrBayes, so those libraries will also be needed whenever I run MrBayes. [galen@login001 src]$ module add gcc/4.4 mpich2/1.4 [galen@login001 ~]$ wget http://downloads.sourceforge.net/project/mrbayes/mrbayes/3.2.1/mrbayes-3.2.1.tar.gz [galen@login001 ~]$ tar -zxf mrbayes-3.2.1.tar.gz [galen@login001 ~]$ cd mrbayes_3.2.1/src [galen@login001 src]$ export PKG_CONFIG_PATH=/home/galen/beagle-lib/lib/pkgconfig:$PKG_CONFIG_PATH [galen@login001 src]$ autoconf [galen@login001 src]$ ./configure --enable-mpi=yes --with-beagle=/home/galen/beagle-lib [galen@login001 src]$ make Here, the mb executable was created in my /home/galen/mrbayes_3.2.1/src directory.","title":"Installing MrBayes"},{"location":"software/software/#running-mrbayes","text":"When running MrBayes in parallel, you'll need to use 1 processor core for each Markov chain, and the default number of chains is 4 (3 heated and 1 that's not heated). Below is an example MrBayes job that uses one of the example Nexus files included with my installation package /home/galen/mrbayes_3.2.1/examples/primates.nex . My MrBayes input file (mb_input) contains these commands: begin mrbayes; set autoclose=yes nowarn=yes; execute primates.nex; lset nst=6 rates=gamma; mcmc nruns=1 ngen=10000 samplefreq=10 file=primates.nex; mcmc file=primates.nex2; mcmc file=primates.nex3; end; My PBS job script for running this job in parallel looks like this: #!/bin/bash #PBS -N MrBayes #PBS -l select=1:ncpus=4:mpiprocs=4:mem=6gb:interconnect=1g,walltime=02:00:00 #PBS -j oe source /etc/profile.d/modules.sh module purge module add gcc/4.4 mpich2/1.4 export LD_LIBRARY_PATH=/home/galen/beagle-lib/lib:$LD_LIBRARY_PATH NCORES=`qstat -xf $PBS_JOBID | grep List.ncpus | sed 's/^.\\{26\\}//'` cd $PBS_O_WORKDIR mpiexec -n $NCORES /home/galen/mrbayes_3.2.1/src/mb mb_input > mb.log","title":"Running MrBayes"},{"location":"software/software/#paraview","text":"","title":"Paraview"},{"location":"software/software/#using-paraviewgpus-to-visualize-very-large-datasets","text":"Paraview can also use multiple GPUs on Palmetto cluster to visualize very large datasets. For this, Paraview must be run in client-server mode. The \"client\" is your local machine on which Paraview must be installed, and the \"server\" is the Palmetto cluster on which the computations/rendering is done. The version of Paraview on the client needs to match exactly the version of Paraview on the server. The client must be running Linux. You can obtain the source code used for installation of Paraview 5.0.1 on Palmetto from /software/paraview/ParaView-v5.0.1-source.tar.gz . Copy this file to the client, extract it and compile Paraview. Compilation instructions can be found in the Paraview documentation . You will need to run the Paraview server on Palmetto cluster. First, log-in with X11 tunneling enabled, and request an interactive session: $ qsub -I -X -l select=4:ncpus=2:mpiprocs=2:ngpus=2:mem=32gb,walltime=1:00:00 In the above example, we request 4 nodes with 2 GPUs each. Next, launch the Paraview server: $ module add paraview/5.0 $ export DISPLAY=:0 $ mpiexec -n 8 pvserver -display :0 The server will be serving on a specific port number (like 11111) on this node. Note this number down. Next, you will need to set up \"port-forwarding\" from the lead node (the node your interactive session is running one) to your local machine. This can be done by opening a terminal running on the local machine, and typing the following: $ ssh -L 11111:nodeXYZ:11111 username@login.palmetto.clemson.edu Once port-forwarding is set up, you can launch Paraview on your local machine,","title":"Using Paraview+GPUs to visualize very large datasets"},{"location":"software/software/#rclone","text":"rclone is a command-line program that can be used to sync files and folders to and from cloud services such as Google Drive, Amazon S3, Dropbox, and many others . In this example, we will show how to use rclone to sync files to a Google Drive account, but the official documentation has specific instructions for other services.","title":"rclone"},{"location":"software/software/#setting-up-rclone-for-use-with-google-drive-on-palmetto","text":"To use rclone with any of the above cloud storage services, you must perform a one-time configuration. You can configure rclone to work with as many services as you like. For the one-time configuration, you will need to log-in with tunneling enabled . Once logged-in, ask for an interactive job: $ qsub -I -X Once the job starts, load the rclone module: $ module add rclone/1.23 After rclone is loaded, you must set up a \"remote\". In this case, we will configure a remote for Google Drive. You can create and manage a separate remote for each cloud storage service you want to use. Start by entering the following command: $ rclone config n) New remote q) Quit config n/q> Hit n then Enter to create a new remote host name> Provide any name for this remote host. For example: gmaildrive What type of source is it? Choose a number from below 1) amazon cloud drive 2) drive 3) dropbox 4) google cloud storage 5) local 6) s3 7) swift type> Provide any number for the remote source. For example choose number 2 for goolge drive. Google Application Client Id - leave blank normally. client_id> # Enter to leave blank Google Application Client Secret - leave blank normally. client_secret> # Enter to leave blank Remote config Use auto config? * Say Y if not sure * Say N if you are working on a remote or headless machine or Y didn't work y) Yes n) No y/n> Use y if you are not sure If your browser doesn't open automatically go to the following link: http://127.0.0.1:53682/auth Log in and authorize rclone for access Waiting for code... This will open the Firefox web browser, allowing you to log-in to your Google account. Enter your username and password then accept to let rclone access your Goolge drive. Once this is done, the browser will ask you to go back to rclone to continue. Got code -------------------- [gmaildrive] client_id = client_secret = token = {\"access_token\":\"xyz\",\"token_type\":\"Bearer\",\"refresh_token\":\"xyz\",\"expiry\":\"yyyy-mm-ddThh:mm:ss\"} -------------------- y) Yes this is OK e) Edit this remote d) Delete this remote y/e/d> Select y to finish configure this remote host. The gmaildrive host will then be created. Current remotes: Name Type ==== ==== gmaildrive drive e) Edit existing remote n) New remote d) Delete remote q) Quit config e/n/d/q> After this, you can quit the config using q , kill the job and exit this ssh session: $ exit","title":"Setting up rclone for use with Google Drive on Palmetto"},{"location":"software/software/#using-rclone","text":"Whenever transfering files (using rclone or otherwise), login to the transfer node xfer01-ext.palmetto.clemson.edu . In MobaXterm, create a new ssh session with xfer01-ext.palmetto.clemson.edu as the Remote host In MacOS, open new terminal and ssh user@xfer01-ext.palmetto.clemson.edu Once logged-in, load the rclone module: $ module add rclone/1.23 You can check the content of the remote host gmaildrive : $ rclone ls gmaildrive: $ rclone lsd gmaildrive: You can use rclone to (for example) copy a file from Palmetto to any folder in your Google Drive: $ rclone copy /path/to/file/on/palmetto gmaildrive:/path/to/folder/on/drive Or if you want to copy to a specific destination on Google Drive back to Palmetto: $ rclone copy gmaildrive:/path/to/folder/on/drive /path/to/file/on/palmetto Additional rclone commands can be found here .","title":"Using rclone"},{"location":"software/software/#singularity","text":"Singularity is a tool for creating and running containers on HPC systems, similar to Docker . For further information on Singularity, and on downloading, building and running containers with Singularity, please refer to the Singularity documentation . This page provides information about singularity specific to the Palmetto cluster.","title":"Singularity"},{"location":"software/software/#running-singularity","text":"Singularity is installed on all of the Palmetto compute nodes and the Palmetto LoginVMs, but it IS NOT present on the login.palmetto.clemson.edu node. To run singularity, you may simply run singularity or more specifically /bin/singularity . e.g. $ singularity --version singularity version 3.5.3-1.el7","title":"Running Singularity"},{"location":"software/software/#an-important-change-for-existing-singularity-users","text":"Formerly, Palmetto administrators had installed singularity as a \"software module\" on Palmetto, but that is no longer the case. If your job scripts have any statements that use the singularity module, then those statements will need to be completely removed; otherwise, your job script may error. Remove any statements from your job scripts that resemble the following lines: module <some_command> singularity","title":"An important change for existing singularity users"},{"location":"software/software/#where-to-download-containers","text":"Containers can be downloaded from DockerHub DockerHub contains containers for various software packages, and Singularity is compatible with Docker images . SingularityHub The NVIDIA GPU Cloud for GPU-optimized images. Many individual projects contain specific instructions for installation via Docker and/or Singularity, and may host pre-built images in other locations.","title":"Where to download containers"},{"location":"software/software/#example-running-openfoam-using-singularity","text":"As an example, we consider installing and running the OpenFOAM CFD solver using Singularity. OpenFOAM can be quite difficult to install manually, but singularity makes it very easy. This example shows how to use singularity interactively, but singularity containers can be run in batch jobs as well. Start by requesting an interactive job. NOTE: Singularity can only be run on the compute nodes and Palmetto Login VMs: $ qsub -I -l select=1:ngpus=2:ncpus=16:mpiprocs=16:mem=120gb,walltime=5:00:00 We recommend that all users store built singularity images in their /home directories. Singularity images can be quite large, so be sure to delete unused or old images: $ mkdir ~/singularity-images $ cd ~/singularity-images Next, we download the singularity image for OpenFOAM from DockerHub. This takes a few seconds to complete: $ singularity pull docker://openfoam/openfoam6-paraview54 Once the image is downloaded, we are ready to run OpenFOAM. We use singularity shell to start a container, and run a shell in the container. The -B option is used to \"bind\" the /scratch2/$USER directory to a directory named /scratch in the container. We also the --pwd option to specify the working directory in the running container (in this case /scratch ). This is always recommended. Typically, the working directory may be the $TMPDIR directory or one of the scratch directories. $ singularity shell -B /scratch2/atrikut:/scratch --pwd /scratch openfoam6-paraview54.simg Before running OpenFOAM commands, we need to source a few environment variables (this step is specific to OpenFOAM): $ source /opt/openfoam6/etc/bashrc Now, we are ready to run a simple example using OpenFOAM: $ cp -r $FOAM_TUTORIALS/incompressible/simpleFoam/pitzDaily . $ cd pitzDaily $ blockMesh $ simpleFoam The simulation takes a few seconds to complete, and should finish with the following output: smoothSolver: Solving for Ux, Initial residual = 0.00012056, Final residual = 7.8056e-06, No Iterations 6 smoothSolver: Solving for Uy, Initial residual = 0.000959834, Final residual = 6.43909e-05, No Iterations 6 GAMG: Solving for p, Initial residual = 0.00191644, Final residual = 0.000161493, No Iterations 3 time step continuity errors : sum local = 0.00681813, global = -0.000731564, cumulative = 0.941842 smoothSolver: Solving for epsilon, Initial residual = 0.000137225, Final residual = 8.98917e-06, No Iterations 3 smoothSolver: Solving for k, Initial residual = 0.000215144, Final residual = 1.30281e-05, No Iterations 4 ExecutionTime = 10.77 s ClockTime = 11 s SIMPLE solution converged in 288 iterations streamLine streamlines write: seeded 10 particles Tracks:10 Total samples:11980 Writing data to \"/scratch/pitzDaily/postProcessing/sets/streamlines/288\" End We are now ready to exit the container: $ exit Because the directory /scratch was bound to /scratch2/$USER , the simulation output is available in the directory /scratch2/$USER/pitzDaily/postProcessing/ : $ ls /scratch2/$USER/pitzDaily/postProcessing/ sets","title":"Example: Running OpenFOAM using Singularity"},{"location":"software/software/#gpu-enabled-software-using-singularity-containers-nvidia-gpu-cloud","text":"Palmetto also supports use of images provided by the NVIDIA GPU Cloud (NGC) . The provides GPU-accelerated HPC and deep learning containers for scientific computing. NVIDIA tests HPC container compatibility with the Singularity runtime through a rigorous QA process.","title":"GPU-enabled software using Singularity containers (NVIDIA GPU Cloud)"},{"location":"software/software/#pulling-ngc-images","text":"Singularity images may be pulled directly from the Palmetto GPU compute nodes, an interactive job is most convenient for this. Singularity uses multiple CPU cores when building the image and so it is recommended that a minimum of 4 CPU cores are reserved. For instance to reserve 4 CPU cores, 2 NVIDIA Pascal GPUs, for 20 minutes the following could be used: $ qsub -I -lselect=1:ncpus=4:mem=2gb:ngpus=2:gpu_model=p100,walltime=00:20:00 Wait for the interactive job to give you control over the shell. Before pulling an NGC image, authentication credentials must be set. This is most easily accomplished by setting the following variables in the build environment. $ export SINGULARITY_DOCKER_USERNAME='$oauthtoken' $ export SINGULARITY_DOCKER_PASSWORD=<NVIDIA NGC API key> More information describing how to obtain and use your NVIDIA NGC API key can be found here . Once credentials are set in the environment, we\u2019re ready to pull and convert the NGC image to a local Singularity image file. The general form of this command for NGC HPC images is: $ singularity build <local_image> docker://nvcr.io/<registry>/<app:tag> This singularity build command will download the app:tag NGC Docker image, convert it to Singularity format, and save it to the local file named local_image. For example to pull the namd NGC container tagged with version 2.12-171025 to a local file named namd.simg we can run: $ singularity build ~/namd.simg docker://nvcr.io/hpc/namd:2.12-171025 After this command has finished we'll have a Singularity image file, namd.simg :","title":"Pulling NGC images"},{"location":"software/software/#running-ngc-containers","text":"Running NGC containers on Palmetto presents few differences from the run instructions provided on NGC for each application. Application-specific information may vary so it is recommended that you follow the container specific documentation before running with Singularity. If the container documentation does not include Singularity information, then the container has not yet been tested under Singularity. As all NGC containers are optimized for NVIDIA GPU acceleration we will always want to add the --nv flag to enable NVIDIA GPU support within the container. The Singularity command below represents the standard form of the Singularity command used on the Palmetto cluster. It will mount the present working directory on the host to /host_pwd in the container process and set the present working directory of the container process to /host_pwd This means that when our process starts it will be effectively running in the host directory the singularity command was launched from. $ singularity exec --nv -B $(pwd):/host_pwd --pwd /host_pwd <image.simg> <cmd>","title":"Running NGC containers"},{"location":"software/spack/","text":"Spack documentation goes here","title":"User Software Installation"},{"location":"software/packages/abaqus/","text":"ABAQUS ABAQUS is a Finite Element Analysis software used for engineering simulations. Currently, ABAQUS versions 6.10, 6.13, 6.14 are available on Palmetto cluster as modules. $ module avail abaqus abaqus/6.10 abaqus/6.13 abaqus/6.14 To see license usage of ABAQUS-related packages, you can use the lmstat command: /software/USR_LOCAL/flexlm/lmstat -a -c /software/USR_LOCAL/flexlm/licenses/abaqus.dat Running ABAQUS interactive viewer To run the interactive viewer, you must log-in with tunneling enabled , and then ask for an interactive session: $ qsub -I -X -l select=1:ncpus=8:mpiprocs=8:mem=6gb:interconnect=1g,walltime=00:15:00 Once logged-in to an interactive compute node, to launch the interactive viewer, load the abaqus module, and run the abaqus executable with the viewer and -mesa options: $ module add abaqus/6.14 $ abaqus viewer -mesa Similarly, to launch the ABAQUS CAE graphical interface: $ abaqus cae -mesa Running ABAQUS in batch mode To run ABAQUS in batch mode on Palmetto cluster, you can use the job script in the following example as a template. This example shows how to run ABAQUS in parallel using MPI. This demonstration runs the \"Axisymmetric analysis of bolted pipe flange connections\" example provided in the ABAQUS documentation here . Please see the documentation for the physics and simulation details. You can obtain the files required to run this example using the following commands: $ cd /scratch2/username $ module add examples $ example get ABAQUS $ cd ABAQUS && ls abaqus_v6.env boltpipeflange_axi_element.inp boltpipeflange_axi_node.inp boltpipeflange_axi_solidgask.inp job.sh The .inp files describe the model and simulation to be performed - see the documentation for details. The batch script job.sh submits the job to the cluster. The .env file is a configuration file that must be included in all ABAQUS job submission folders on Palmetto. #!/bin/bash #PBS -N AbaqusDemo #PBS -l select=2:ncpus=8:mpiprocs=8:mem=6gb:interconnect=1g,walltime=00:15:00 #PBS -j oe module purge module add abaqus/6.14 pbsdsh sleep 20 NCORES=`wc -l $PBS_NODEFILE | gawk '{print $1}'` cd $PBS_O_WORKDIR SCRATCH=$TMPDIR # copy all input files into the scratch directory for node in `uniq $PBS_NODEFILE` do ssh $node \"cp $PBS_O_WORKDIR/*.inp $SCRATCH\" done cd $SCRATCH # run the abaqus program, providing the .inp file as input abaqus job=abdemo double input=$SCRATCH/boltpipeflange_axi_solidgask.inp scratch=$SCRATCH cpus=$NCORES mp_mode=mpi interactive # copy results back from scratch directory to $PBS_O_WORKDIR for node in `uniq $PBS_NODEFILE` do ssh $node \"cp -r $SCRATCH/* $PBS_O_WORKDIR\" done In the batch script job.sh : The following line extracts the total number of CPU cores available across all the nodes requested by the job: ~~~ NCORES= wc -l $PBS_NODEFILE | gawk '{print $1}' ~~~ The following line runs the ABAQUS program, specifying various options such as the path to the .inp file, the scratch directory to use, etc., ~~~ abaqus job=abdemo double input=/scratch2/$USER/ABAQUS/boltpipeflange_axi_solidgask.inp scratch=$SCRATCH cpus=$NCORES mp_mode=mpi interactive ~~~ To submit the job: $ qsub job.sh 9668628 After job completion, you will see the job submission directory ( /scratch2/username/ABAQUS ) populated with various files: $ ls AbaqusDemo.o9668628 abdemo.dat abdemo.msg abdemo.res abdemo.stt boltpipeflange_axi_solidgask.inp abaqus_v6.env abdemo.fil abdemo.odb abdemo.sim boltpipeflange_axi_element.inp job.sh abdemo.com abdemo.mdl abdemo.prt abdemo.sta boltpipeflange_axi_node.inp If everything went well, the job output file ( AbaqusDemo.o9668628 ) should look like this: [atrikut@login001 ABAQUS]$ cat AbaqusDemo.o9668628 Abaqus JOB abdemo Abaqus 6.14-1 Abaqus License Manager checked out the following licenses: Abaqus/Standard checked out 16 tokens from Flexnet server licensevm4.clemson.edu. <567 out of 602 licenses remain available>. Begin Analysis Input File Processor Mon 13 Feb 2017 12:35:29 PM EST Run pre Mon 13 Feb 2017 12:35:31 PM EST End Analysis Input File Processor Begin Abaqus/Standard Analysis Mon 13 Feb 2017 12:35:31 PM EST Run standard Mon 13 Feb 2017 12:35:35 PM EST End Abaqus/Standard Analysis Abaqus JOB abdemo COMPLETED +------------------------------------------+ | PALMETTO CLUSTER PBS RESOURCES REQUESTED | +------------------------------------------+ mem=12gb,ncpus=16,walltime=00:15:00 +-------------------------------------+ | PALMETTO CLUSTER PBS RESOURCES USED | +-------------------------------------+ cpupercent=90,cput=00:00:10,mem=636kb,ncpus=16,vmem=12612kb,walltime=00:00:13 The output database ( .odb ) file contains the results of the simulation which can be viewed using the ABAQUS viewer:","title":"Abaqus"},{"location":"software/packages/abaqus/#abaqus","text":"ABAQUS is a Finite Element Analysis software used for engineering simulations. Currently, ABAQUS versions 6.10, 6.13, 6.14 are available on Palmetto cluster as modules. $ module avail abaqus abaqus/6.10 abaqus/6.13 abaqus/6.14 To see license usage of ABAQUS-related packages, you can use the lmstat command: /software/USR_LOCAL/flexlm/lmstat -a -c /software/USR_LOCAL/flexlm/licenses/abaqus.dat","title":"ABAQUS"},{"location":"software/packages/abaqus/#running-abaqus-interactive-viewer","text":"To run the interactive viewer, you must log-in with tunneling enabled , and then ask for an interactive session: $ qsub -I -X -l select=1:ncpus=8:mpiprocs=8:mem=6gb:interconnect=1g,walltime=00:15:00 Once logged-in to an interactive compute node, to launch the interactive viewer, load the abaqus module, and run the abaqus executable with the viewer and -mesa options: $ module add abaqus/6.14 $ abaqus viewer -mesa Similarly, to launch the ABAQUS CAE graphical interface: $ abaqus cae -mesa","title":"Running ABAQUS interactive viewer"},{"location":"software/packages/abaqus/#running-abaqus-in-batch-mode","text":"To run ABAQUS in batch mode on Palmetto cluster, you can use the job script in the following example as a template. This example shows how to run ABAQUS in parallel using MPI. This demonstration runs the \"Axisymmetric analysis of bolted pipe flange connections\" example provided in the ABAQUS documentation here . Please see the documentation for the physics and simulation details. You can obtain the files required to run this example using the following commands: $ cd /scratch2/username $ module add examples $ example get ABAQUS $ cd ABAQUS && ls abaqus_v6.env boltpipeflange_axi_element.inp boltpipeflange_axi_node.inp boltpipeflange_axi_solidgask.inp job.sh The .inp files describe the model and simulation to be performed - see the documentation for details. The batch script job.sh submits the job to the cluster. The .env file is a configuration file that must be included in all ABAQUS job submission folders on Palmetto. #!/bin/bash #PBS -N AbaqusDemo #PBS -l select=2:ncpus=8:mpiprocs=8:mem=6gb:interconnect=1g,walltime=00:15:00 #PBS -j oe module purge module add abaqus/6.14 pbsdsh sleep 20 NCORES=`wc -l $PBS_NODEFILE | gawk '{print $1}'` cd $PBS_O_WORKDIR SCRATCH=$TMPDIR # copy all input files into the scratch directory for node in `uniq $PBS_NODEFILE` do ssh $node \"cp $PBS_O_WORKDIR/*.inp $SCRATCH\" done cd $SCRATCH # run the abaqus program, providing the .inp file as input abaqus job=abdemo double input=$SCRATCH/boltpipeflange_axi_solidgask.inp scratch=$SCRATCH cpus=$NCORES mp_mode=mpi interactive # copy results back from scratch directory to $PBS_O_WORKDIR for node in `uniq $PBS_NODEFILE` do ssh $node \"cp -r $SCRATCH/* $PBS_O_WORKDIR\" done In the batch script job.sh : The following line extracts the total number of CPU cores available across all the nodes requested by the job: ~~~ NCORES= wc -l $PBS_NODEFILE | gawk '{print $1}' ~~~ The following line runs the ABAQUS program, specifying various options such as the path to the .inp file, the scratch directory to use, etc., ~~~ abaqus job=abdemo double input=/scratch2/$USER/ABAQUS/boltpipeflange_axi_solidgask.inp scratch=$SCRATCH cpus=$NCORES mp_mode=mpi interactive ~~~ To submit the job: $ qsub job.sh 9668628 After job completion, you will see the job submission directory ( /scratch2/username/ABAQUS ) populated with various files: $ ls AbaqusDemo.o9668628 abdemo.dat abdemo.msg abdemo.res abdemo.stt boltpipeflange_axi_solidgask.inp abaqus_v6.env abdemo.fil abdemo.odb abdemo.sim boltpipeflange_axi_element.inp job.sh abdemo.com abdemo.mdl abdemo.prt abdemo.sta boltpipeflange_axi_node.inp If everything went well, the job output file ( AbaqusDemo.o9668628 ) should look like this: [atrikut@login001 ABAQUS]$ cat AbaqusDemo.o9668628 Abaqus JOB abdemo Abaqus 6.14-1 Abaqus License Manager checked out the following licenses: Abaqus/Standard checked out 16 tokens from Flexnet server licensevm4.clemson.edu. <567 out of 602 licenses remain available>. Begin Analysis Input File Processor Mon 13 Feb 2017 12:35:29 PM EST Run pre Mon 13 Feb 2017 12:35:31 PM EST End Analysis Input File Processor Begin Abaqus/Standard Analysis Mon 13 Feb 2017 12:35:31 PM EST Run standard Mon 13 Feb 2017 12:35:35 PM EST End Abaqus/Standard Analysis Abaqus JOB abdemo COMPLETED +------------------------------------------+ | PALMETTO CLUSTER PBS RESOURCES REQUESTED | +------------------------------------------+ mem=12gb,ncpus=16,walltime=00:15:00 +-------------------------------------+ | PALMETTO CLUSTER PBS RESOURCES USED | +-------------------------------------+ cpupercent=90,cput=00:00:10,mem=636kb,ncpus=16,vmem=12612kb,walltime=00:00:13 The output database ( .odb ) file contains the results of the simulation which can be viewed using the ABAQUS viewer:","title":"Running ABAQUS in batch mode"},{"location":"software/packages/amber/","text":"Amber In this example we will use \"alp\" test from Amber's test suite. To get the files relevant to this example: $ module add examples $ example get Amber $ cd Amber && ls amber.pbs coords md.in prmtop README.md Examine the batch script amber.pbs , and adjust the names of the files i.e. input and coordinates files. Also, adjust the number of nodes, processors per node and walltime. To submit the job: qsub amber.pbs Results will be saved in the md.out file (in our example).","title":"Amber"},{"location":"software/packages/amber/#amber","text":"In this example we will use \"alp\" test from Amber's test suite. To get the files relevant to this example: $ module add examples $ example get Amber $ cd Amber && ls amber.pbs coords md.in prmtop README.md Examine the batch script amber.pbs , and adjust the names of the files i.e. input and coordinates files. Also, adjust the number of nodes, processors per node and walltime. To submit the job: qsub amber.pbs Results will be saved in the md.out file (in our example).","title":"Amber"},{"location":"software/packages/ansys/","text":"ANSYS Graphical Interfaces To run the various ANSYS graphical programs, you must log-in with tunneling enabled and then ask for an interactive session: $ qsub -I -X -l select=1:ncpus=8:mpiprocs=8:mem=6gb:interconnect=1g,walltime=01:00:00 Once logged-in to an interactive compute node, you must first load the ANSYS module along with the Intel module: $ module add ansys/17.2 $ module add intel/12.1 And then launch the required program: For ANSYS APDL $ ansys172 -g If you are using e.g., ANSYS 17.0 instead, then the executable is called ansys170 . For CFX $ cfxlaunch For ANSYS Workbench $ runwb2 For Fluent $ fluent For ANSYS Electromagnetics $ ansysedt Batch Mode To run ANSYS in batch mode on Palmetto cluster, you can use the job script in the following example as a template. This example shows how to run ANSYS in parallel (using multiple cores/nodes). In this demonstration, we model the strain in a 2-D flat plate. You can obtain the files required to run this example using the following commands: $ cd /scratch2/username $ module add examples $ example get ANSYS $ cd ANSYS && ls input.txt job.sh The input.txt batch file is generated for the model using the ANSYS APDL interface. The batch script job.sh submits the batch job to the cluster: #!/bin/bash #PBS -N ANSYSdis #PBS -l select=2:ncpus=4:mpiprocs=4:mem=11gb:interconnect=1g #PBS -l walltime=1:00:00 #PBS -j oe module purge module add ansys/17.2 cd $PBS_O_WORKDIR machines=$(uniq -c $PBS_NODEFILE | awk '{print $2\":\"$1}' | tr '\\n' :) for node in `uniq $PBS_NODEFILE` do ssh $node \"sleep 5\" ssh $node \"cp input.txt $TMPDIR\" done cd $TMPDIR ansys172 -dir $TMPDIR -j EXAMPLE -s read -l en-us -b -i input.txt -o output.txt -dis -machines $machines -usessh for node in `uniq $PBS_NODEFILE` do ssh $node \"cp -r $TMPDIR/* $PBS_O_WORKDIR\" done In the batch script job.sh : The following line extracts the nodes (machines) available for this job as well as the number of CPU cores allocated for each node: ~~~ machines=$(uniq -c $PBS_NODEFILE | awk '{print $2\":\"$1}' | tr '\\n' :) ~~~ For ANSYS jobs, you should always use $TMPDIR ( /local_scratch ) as the working directory. The following lines ensure that $TMPDIR is created on each node: ~~~ do ssh $node \"sleep 5\" ssh $node \"cp input.txt $TMPDIR\" done ~~~ The following line runs the ANSYS program, specifying various options such as the path to the input.txt file, the scratch directory to use, etc., ~~~ ansys172 -dir $TMPDIR -j EXAMPLE -s read -l en-us -b -i input.txt -o output.txt -dis -machines $machines -usessh ~~~ Finally, the following lines copy all the data from $TMPDIR : ~~~ do ssh $node \"cp -r $TMPDIR/* $PBS_O_WORKDIR\" done ~~~ To submit the job: $ qsub job.sh 9752784 After job completion, you will see the job submission directory ( /scratch2/username/ANSYS ) populated with various files: $ ls ANSYSdis.o9752784 EXAMPLE0.stat EXAMPLE2.err EXAMPLE3.esav EXAMPLE4.full EXAMPLE5.out EXAMPLE6.rst EXAMPLE.DSP input.txt EXAMPLE0.err EXAMPLE1.err EXAMPLE2.esav EXAMPLE3.full EXAMPLE4.out EXAMPLE5.rst EXAMPLE7.err EXAMPLE.esav job.sh EXAMPLE0.esav EXAMPLE1.esav EXAMPLE2.full EXAMPLE3.out EXAMPLE4.rst EXAMPLE6.err EXAMPLE7.esav EXAMPLE.mntr mpd.hosts EXAMPLE0.full EXAMPLE1.full EXAMPLE2.out EXAMPLE3.rst EXAMPLE5.err EXAMPLE6.esav EXAMPLE7.full EXAMPLE.rst mpd.hosts.bak EXAMPLE0.log EXAMPLE1.out EXAMPLE2.rst EXAMPLE4.err EXAMPLE5.esav EXAMPLE6.full EXAMPLE7.out host.list output.txt EXAMPLE0.rst EXAMPLE1.rst EXAMPLE3.err EXAMPLE4.esav EXAMPLE5.full EXAMPLE6.out EXAMPLE7.rst host.list.bak If everything went well, the job output file ( ANSYSdis.o9752784 ) should look like this: +------------------------------------------+ | PALMETTO CLUSTER PBS RESOURCES REQUESTED | +------------------------------------------+ mem=22gb,ncpus=8,walltime=01:00:00 +-------------------------------------+ | PALMETTO CLUSTER PBS RESOURCES USED | +-------------------------------------+ cpupercent=27,cput=00:00:17,mem=3964kb,ncpus=8,vmem=327820kb,walltime=00:01:07 The results file ( EXAMPLE.rst ) contains the results of the simulation which can be viewed using the ANSYS APDL graphical interface:","title":"Ansys"},{"location":"software/packages/ansys/#ansys","text":"","title":"ANSYS"},{"location":"software/packages/ansys/#graphical-interfaces","text":"To run the various ANSYS graphical programs, you must log-in with tunneling enabled and then ask for an interactive session: $ qsub -I -X -l select=1:ncpus=8:mpiprocs=8:mem=6gb:interconnect=1g,walltime=01:00:00 Once logged-in to an interactive compute node, you must first load the ANSYS module along with the Intel module: $ module add ansys/17.2 $ module add intel/12.1 And then launch the required program: For ANSYS APDL $ ansys172 -g If you are using e.g., ANSYS 17.0 instead, then the executable is called ansys170 . For CFX $ cfxlaunch For ANSYS Workbench $ runwb2 For Fluent $ fluent For ANSYS Electromagnetics $ ansysedt","title":"Graphical Interfaces"},{"location":"software/packages/ansys/#batch-mode","text":"To run ANSYS in batch mode on Palmetto cluster, you can use the job script in the following example as a template. This example shows how to run ANSYS in parallel (using multiple cores/nodes). In this demonstration, we model the strain in a 2-D flat plate. You can obtain the files required to run this example using the following commands: $ cd /scratch2/username $ module add examples $ example get ANSYS $ cd ANSYS && ls input.txt job.sh The input.txt batch file is generated for the model using the ANSYS APDL interface. The batch script job.sh submits the batch job to the cluster: #!/bin/bash #PBS -N ANSYSdis #PBS -l select=2:ncpus=4:mpiprocs=4:mem=11gb:interconnect=1g #PBS -l walltime=1:00:00 #PBS -j oe module purge module add ansys/17.2 cd $PBS_O_WORKDIR machines=$(uniq -c $PBS_NODEFILE | awk '{print $2\":\"$1}' | tr '\\n' :) for node in `uniq $PBS_NODEFILE` do ssh $node \"sleep 5\" ssh $node \"cp input.txt $TMPDIR\" done cd $TMPDIR ansys172 -dir $TMPDIR -j EXAMPLE -s read -l en-us -b -i input.txt -o output.txt -dis -machines $machines -usessh for node in `uniq $PBS_NODEFILE` do ssh $node \"cp -r $TMPDIR/* $PBS_O_WORKDIR\" done In the batch script job.sh : The following line extracts the nodes (machines) available for this job as well as the number of CPU cores allocated for each node: ~~~ machines=$(uniq -c $PBS_NODEFILE | awk '{print $2\":\"$1}' | tr '\\n' :) ~~~ For ANSYS jobs, you should always use $TMPDIR ( /local_scratch ) as the working directory. The following lines ensure that $TMPDIR is created on each node: ~~~ do ssh $node \"sleep 5\" ssh $node \"cp input.txt $TMPDIR\" done ~~~ The following line runs the ANSYS program, specifying various options such as the path to the input.txt file, the scratch directory to use, etc., ~~~ ansys172 -dir $TMPDIR -j EXAMPLE -s read -l en-us -b -i input.txt -o output.txt -dis -machines $machines -usessh ~~~ Finally, the following lines copy all the data from $TMPDIR : ~~~ do ssh $node \"cp -r $TMPDIR/* $PBS_O_WORKDIR\" done ~~~ To submit the job: $ qsub job.sh 9752784 After job completion, you will see the job submission directory ( /scratch2/username/ANSYS ) populated with various files: $ ls ANSYSdis.o9752784 EXAMPLE0.stat EXAMPLE2.err EXAMPLE3.esav EXAMPLE4.full EXAMPLE5.out EXAMPLE6.rst EXAMPLE.DSP input.txt EXAMPLE0.err EXAMPLE1.err EXAMPLE2.esav EXAMPLE3.full EXAMPLE4.out EXAMPLE5.rst EXAMPLE7.err EXAMPLE.esav job.sh EXAMPLE0.esav EXAMPLE1.esav EXAMPLE2.full EXAMPLE3.out EXAMPLE4.rst EXAMPLE6.err EXAMPLE7.esav EXAMPLE.mntr mpd.hosts EXAMPLE0.full EXAMPLE1.full EXAMPLE2.out EXAMPLE3.rst EXAMPLE5.err EXAMPLE6.esav EXAMPLE7.full EXAMPLE.rst mpd.hosts.bak EXAMPLE0.log EXAMPLE1.out EXAMPLE2.rst EXAMPLE4.err EXAMPLE5.esav EXAMPLE6.full EXAMPLE7.out host.list output.txt EXAMPLE0.rst EXAMPLE1.rst EXAMPLE3.err EXAMPLE4.esav EXAMPLE5.full EXAMPLE6.out EXAMPLE7.rst host.list.bak If everything went well, the job output file ( ANSYSdis.o9752784 ) should look like this: +------------------------------------------+ | PALMETTO CLUSTER PBS RESOURCES REQUESTED | +------------------------------------------+ mem=22gb,ncpus=8,walltime=01:00:00 +-------------------------------------+ | PALMETTO CLUSTER PBS RESOURCES USED | +-------------------------------------+ cpupercent=27,cput=00:00:17,mem=3964kb,ncpus=8,vmem=327820kb,walltime=00:01:07 The results file ( EXAMPLE.rst ) contains the results of the simulation which can be viewed using the ANSYS APDL graphical interface:","title":"Batch Mode"},{"location":"software/packages/comsol/","text":"COMSOL COMSOL is an application for solving Multiphysics problems. To see the available COMSOL modules on Palmetto: $ module avail comsol comsol/4.3b comsol/4.4 comsol/5.0 comsol/5.1 comsol/5.2 comsol/5.3 To see license usage of COMSOL-related packages, you can use the lmstat command: /software/USR_LOCAL/flexlm/lmstat -a -c /software/USR_LOCAL/flexlm/licenses/comsol.dat Graphical Interface To run the COMSOL graphical interface, you must log-in with tunneling enabled , and then ask for an interactive session: $ qsub -I -X -l select=1:ncpus=8:mpiprocs=8:mem=6gb:interconnect=1g,walltime=00:15:00 Once logged-in to an interactive compute node, to launch the interactive viewer, you can use the comsol command to run COMSOL: $ module add comsol/5.2 $ comsol -np 8 -tmpdir $TMPDIR The -np option can be used to specify the number of CPU cores to use. Remember to always use $TMPDIR as the working directory for COMSOL jobs. Batch Mode To run COMSOL in batch mode on Palmetto cluster, you can use the example batch scripts below as a template. The first example demonstrates running COMSOL using multiple cores on a single node, while the second demonstrates running COMSOL across multiple nodes using MPI. You can obtain the files required to run this example using the following commands: $ module add examples $ example get COMSOL $ cd COMSOL && ls job.sh job_mpi.sh Both of these examples run the \"Heat Transfer by Free Convection\" application described here . In addition to the job.sh and job_mpi.sh scripts, to run the examples and reproduce the results, you will need to download the file free_convection.mph (choose the correct version) provided with the description (login required). COMSOL batch job on a single node, using multiple cores: #!/bin/bash #PBS -N COMSOL #PBS -l select=1:ncpus=8:mem=32gb,walltime=01:30:00 #PBS -j oe module purge module add comsol/5.2 cd $PBS_O_WORKDIR comsol batch -np 8 -tmpdir $TMPDIR -inputfile free_convection.mph -outputfile free_convection_output.mph COMSOL batch job across several nodes #!/bin/bash #PBS -N COMSOL #PBS -l select=2:ncpus=8:mpiprocs=8:mem=32gb,walltime=01:30:00 #PBS -j oe module purge module add comsol/5.2 cd $PBS_O_WORKDIR uniq $PBS_NODEFILE > comsol_nodefile comsol batch -clustersimple -f comsol_nodefile -tmpdir $TMPDIR -inputfile free_convection.mph -outputfile free_convection_output.mph","title":"Comsol"},{"location":"software/packages/comsol/#comsol","text":"COMSOL is an application for solving Multiphysics problems. To see the available COMSOL modules on Palmetto: $ module avail comsol comsol/4.3b comsol/4.4 comsol/5.0 comsol/5.1 comsol/5.2 comsol/5.3 To see license usage of COMSOL-related packages, you can use the lmstat command: /software/USR_LOCAL/flexlm/lmstat -a -c /software/USR_LOCAL/flexlm/licenses/comsol.dat","title":"COMSOL"},{"location":"software/packages/comsol/#graphical-interface","text":"To run the COMSOL graphical interface, you must log-in with tunneling enabled , and then ask for an interactive session: $ qsub -I -X -l select=1:ncpus=8:mpiprocs=8:mem=6gb:interconnect=1g,walltime=00:15:00 Once logged-in to an interactive compute node, to launch the interactive viewer, you can use the comsol command to run COMSOL: $ module add comsol/5.2 $ comsol -np 8 -tmpdir $TMPDIR The -np option can be used to specify the number of CPU cores to use. Remember to always use $TMPDIR as the working directory for COMSOL jobs.","title":"Graphical Interface"},{"location":"software/packages/comsol/#batch-mode","text":"To run COMSOL in batch mode on Palmetto cluster, you can use the example batch scripts below as a template. The first example demonstrates running COMSOL using multiple cores on a single node, while the second demonstrates running COMSOL across multiple nodes using MPI. You can obtain the files required to run this example using the following commands: $ module add examples $ example get COMSOL $ cd COMSOL && ls job.sh job_mpi.sh Both of these examples run the \"Heat Transfer by Free Convection\" application described here . In addition to the job.sh and job_mpi.sh scripts, to run the examples and reproduce the results, you will need to download the file free_convection.mph (choose the correct version) provided with the description (login required).","title":"Batch Mode"},{"location":"software/packages/comsol/#comsol-batch-job-on-a-single-node-using-multiple-cores","text":"#!/bin/bash #PBS -N COMSOL #PBS -l select=1:ncpus=8:mem=32gb,walltime=01:30:00 #PBS -j oe module purge module add comsol/5.2 cd $PBS_O_WORKDIR comsol batch -np 8 -tmpdir $TMPDIR -inputfile free_convection.mph -outputfile free_convection_output.mph","title":"COMSOL batch job on a single node, using multiple cores:"},{"location":"software/packages/comsol/#comsol-batch-job-across-several-nodes","text":"#!/bin/bash #PBS -N COMSOL #PBS -l select=2:ncpus=8:mpiprocs=8:mem=32gb,walltime=01:30:00 #PBS -j oe module purge module add comsol/5.2 cd $PBS_O_WORKDIR uniq $PBS_NODEFILE > comsol_nodefile comsol batch -clustersimple -f comsol_nodefile -tmpdir $TMPDIR -inputfile free_convection.mph -outputfile free_convection_output.mph","title":"COMSOL batch job across several nodes"},{"location":"software/packages/gephi/","text":"Gephi Gephi is an interactive visualization and exploration platform for all kinds of networks and complex systems, dynamic and hierarchical graphs. At this time, we don't have Gephi setup as a software module on Palmetto, but downloading and installing it in your own /home or /newscratch directory (and running it from that location) is easy to do. Below are the procedures I used to do this: Download and Setup the Gephi Binary wget https://launchpad.net/gephi/0.8/0.8.2beta/+download/gephi-0.8.2-beta.tar.gz tar -zxf gephi-0.8.2-beta.tar.gz Now, the gephi binary is installed in ~/gephi/bin . Running Gephi Connect to Palmetto with X11 tunneling enabled so you can \"tunnel\" the GUI running on Palmetto through your SSH connection to your desktop: ssh -X galen@login.palmetto.clemson.edu Once you're logged-in to Palmetto, you'll need to launch an interactive job with X11 tunneling enabled. Be sure to request an allocation with enough memory for Gephi to handle your input data properly (here, I'm requesing 31 GB RAM): qsub -I -X -l select=1:ncpus=4:mem=31gb,walltime=6:00:00 If you are using a Windows system, you'll need to launch a local Xserver (like Xming) so your system can display the tunneled GUI information. Once your local Xserver is ready, launch the Gephi GUI in your interactive session on Palmetto: /home/galen/gephi/bin/gephi Now, I can browse for data I have stored in my directories on Palmetto. If your data is stored on your local workstation, you must move that data to Palmetto so you can open it with Gephi. You may experience a short delay as the GUI is generated and tunneled to your desktop. Responsiveness between your local workstation and the tunneled GUI may be a bit slow/laggy, but the compute operations taking place on that Palmetto compute node are not delayed.","title":"Gephi"},{"location":"software/packages/gephi/#gephi","text":"Gephi is an interactive visualization and exploration platform for all kinds of networks and complex systems, dynamic and hierarchical graphs. At this time, we don't have Gephi setup as a software module on Palmetto, but downloading and installing it in your own /home or /newscratch directory (and running it from that location) is easy to do. Below are the procedures I used to do this:","title":"Gephi"},{"location":"software/packages/gephi/#download-and-setup-the-gephi-binary","text":"wget https://launchpad.net/gephi/0.8/0.8.2beta/+download/gephi-0.8.2-beta.tar.gz tar -zxf gephi-0.8.2-beta.tar.gz Now, the gephi binary is installed in ~/gephi/bin .","title":"Download and Setup the Gephi Binary"},{"location":"software/packages/gephi/#running-gephi","text":"Connect to Palmetto with X11 tunneling enabled so you can \"tunnel\" the GUI running on Palmetto through your SSH connection to your desktop: ssh -X galen@login.palmetto.clemson.edu Once you're logged-in to Palmetto, you'll need to launch an interactive job with X11 tunneling enabled. Be sure to request an allocation with enough memory for Gephi to handle your input data properly (here, I'm requesing 31 GB RAM): qsub -I -X -l select=1:ncpus=4:mem=31gb,walltime=6:00:00 If you are using a Windows system, you'll need to launch a local Xserver (like Xming) so your system can display the tunneled GUI information. Once your local Xserver is ready, launch the Gephi GUI in your interactive session on Palmetto: /home/galen/gephi/bin/gephi Now, I can browse for data I have stored in my directories on Palmetto. If your data is stored on your local workstation, you must move that data to Palmetto so you can open it with Gephi. You may experience a short delay as the GUI is generated and tunneled to your desktop. Responsiveness between your local workstation and the tunneled GUI may be a bit slow/laggy, but the compute operations taking place on that Palmetto compute node are not delayed.","title":"Running Gephi"},{"location":"software/packages/gromacs/","text":"GROMACS Various installations of GROMACS are available on the cluster. Different modules are provided for GPU-enabled and non-GPU versions of GROMACS: $ module avail gromacs ---------------------------------------------------------------- /software/modulefiles ----------------------------------------------------------------- gromacs/4.5.4-sp gromacs/4.6.5-sp-k20-ompi gromacs/5.0.1-sp-k20-g481-o181 gromacs/5.0.5-nogpu gromacs/4.6.5-dp-ompi gromacs/5.0.1-dp-g481-o181 gromacs/5.0.5-gpu ------------------------------------------------------------ /usr/share/Modules/modulefiles ------------------------------------------------------------ gromacs/4.5.4-sp Running GROMACS without GPU To use the non-GPU version of GROMACS, here is an example interactive job: $ qsub -I -l select=2:ncpus=20:phase=10:mem=100gb:mpiprocs=20,walltime=1: 00:00 $ module load gromacs/5.0.5-nogpu $ mpirun mdrun_mpi ... where ... specifies the input files and options of Gromacs. And an example of a PBS batch script for submitting a GROMACS batch script: #!/bin/bash #PBS -l select=2:ncpus=20:phase=10:mem=100gb:mpiprocs=20 #PBS -l walltime=1:00:00 module load gromacs/5.0.5-nogpu cd $PBS_O_WORKDIR export OMP_NUM_THREADS=1 mpirun mdrun_mpi ... Running GPU-enabled GROMACS For the GPU version #!/bin/bash #PBS -l select=2:ncpus=20:ngpus=2:phase=10:mem=100gb:mpiprocs=2 #PBS -l walltime=1:00:00 module load gromacs/5.0.5-gpu cd $PBS_O_WORKDIR export OMP_NUM_THREADS=10 mpirun mdrun_mpi ... Note that the number of MPI processes per chunk is set as 2 when using the GPU. Normally, the number of MPI processes is the same as the number of CPU cores requested. But because there are only 2 GPUs per node, we launch only two MPI processes. In the above example, each MPI process can also use 10 CPU cores in addition to a GPU (this is enabled by setting the variable OMP_NUM_THREADS to 10). GPU-enabled GROMACS using Singularity and NGC The NVIDIA GPU cloud provides images for GPU-enabled GROMACS that can be downloaded and run using Singularity on the Palmetto cluster. This is the recommended way to run GROMACS on Palmetto. Downloading the image Before downloading images from NGC, you will need to obtain an NVIDIA NGC API key, instructions for which can be found here . Start an interactive job, and create a directory for storing Singularity images (if you don't have one already): $ qsub -I -l select=1:ngpus=2:ncpus=16:mem=20gb:gpu_model=p100,walltime=5:00:00 $ mkdir -p singularity-images Set the required environment variables (you will need the API key for this step): $ export SINGULARITY_DOCKER_USERNAME='$oauthtoken' $ export SINGULARITY_DOCKER_PASSWORD=<NVIDA NGC API key> Navigate to the singularity-images directory and pull the GROMACS image (choose from the images listed here : $ cd ~/singularity-images $ module load singularity $ singularity pull docker://nvcr.io/hpc/gromacs:2016.4 The above should take a few seconds to complete. Running GROMACS interactively As an example, we'll consider running the GROMACS ADH benchmark. First, request an interactive job: $ qsub -I -l select=1:ngpus=2:ncpus=16:mpiprocs=16:mem=120gb:gpu_model=p100,walltime=5:00:00 Load the singularity module: $ module load singularity Prepare the input and output directories: In this case, we use /scratch3/$USER/gromacs_ADH as both the input directory (containing the data) and the output directory (where output will be stored). Remember that $TMPDIR is cleaned up after the job completes, so it's important to move any data out of $TMPDIR after GROMACS completes running. Also, in this case, we are downloading the input data from the web, but if your input is stored in the /home /, /scratch or some other directory, then you can copy it to $TMPDIR : $ mkdir -p /scratch3/$USER/gromacs_ADH_benchmark $ cd /scratch3/$USER/gromacs_ADH_benchmark $ wget ftp://ftp.gromacs.org/pub/benchmarks/ADH_bench_systems.tar.gz $ tar -xvf ADH_bench_systems.tar.gz Use singularity shell to interactively run a container from the downloaded GROMACS image. The -B switch is used to \"bind\" directories on the host (Palmetto compute node) to directories in the container. The different bindings used are: /scratch3/$USER/gromacs_ADH_benchmark is bound to /input /scratch3/$USER/gromacs_ADH_benchmark is bound to /output $TMPDIR is bound to /work $ singularity shell --nv \\ -B /scratch3/$USER/gromacs_ADH_benchmark:/input \\ -B /scratch3/$USER/gromacs_ADH_benchmark:/output \\ -B $TMPDIR:/work \\ --pwd /work \\ ~/singularity-images/gromacs-2016.4.simg \\ Running the benchmark: $ source /opt/gromacs/install/2016.4/bin/GMXRC $ gmx_mpi grompp -f /input/adh_cubic/pme_verlet.mdp -c /input/adh_cubic/conf.gro -p /input/adh_cubic/topol.top $ export OMP_NUM_THREADS=8 $ mpirun -np 2 gmx_mpi mdrun -g adh_cubic.log -pin on -resethway -v -noconfout -nsteps 10000 -s topol.tpr -ntomp $OMP_NUM_THREADS After the last command above completes, the .edr and .log files produced by GROMACS should be visible. Typically, the next step is to copy these results to the output directory: $ cp *.log *.edr /output $ exit Upon exiting the container, the .log and .edr files will be found in the output directory, /scratch3/$USER/gromacs_ADH_benchmark . Running GROMACS in batch mode The same benchmark can be run in batch mode by encapsulating the commands in a script run_adh.sh , and running it non-interactively using singularity exec . # run_adh.sh source /opt/gromacs/install/2016.4/bin/GMXRC gmx_mpi grompp -f /input/adh_cubic/pme_verlet.mdp -c /input/adh_cubic/conf.gro -p /input/adh_cubic/topol.top export OMP_NUM_THREADS=8 mpirun -np 2 gmx_mpi mdrun -g adh_cubic.log -pin on -resethway -v -noconfout -nsteps 10000 -s topol.tpr -ntomp $OMP_NUM_THREADS cp *.log *.edr /output The PBS batch script for submitting the above (assuming that the /scratch3/$USER/gromacs_ADH_benchmark directory already contains the input files): #PBS -N adh_cubic #PBS -l select=1:ngpus=2:ncpus=16:mem=20gb:gpu_model=p100,walltime=5:00:00 module load singularity cd $PBS_O_WORKDIR cp run_adh.sh $TMPDIR singularity exec --nv \\ -B /scratch3/$USER/gromacs_ADH_benchmark:/input \\ -B /scratch3/$USER/gromacs_ADH_benchmark:/output \\ -B $TMPDIR:/work \\ --pwd /work \\ ~/singularity-images/gromacs-2016.4.simg bash run_adh.sh","title":"Gromacs"},{"location":"software/packages/gromacs/#gromacs","text":"Various installations of GROMACS are available on the cluster. Different modules are provided for GPU-enabled and non-GPU versions of GROMACS: $ module avail gromacs ---------------------------------------------------------------- /software/modulefiles ----------------------------------------------------------------- gromacs/4.5.4-sp gromacs/4.6.5-sp-k20-ompi gromacs/5.0.1-sp-k20-g481-o181 gromacs/5.0.5-nogpu gromacs/4.6.5-dp-ompi gromacs/5.0.1-dp-g481-o181 gromacs/5.0.5-gpu ------------------------------------------------------------ /usr/share/Modules/modulefiles ------------------------------------------------------------ gromacs/4.5.4-sp","title":"GROMACS"},{"location":"software/packages/gromacs/#running-gromacs-without-gpu","text":"To use the non-GPU version of GROMACS, here is an example interactive job: $ qsub -I -l select=2:ncpus=20:phase=10:mem=100gb:mpiprocs=20,walltime=1: 00:00 $ module load gromacs/5.0.5-nogpu $ mpirun mdrun_mpi ... where ... specifies the input files and options of Gromacs. And an example of a PBS batch script for submitting a GROMACS batch script: #!/bin/bash #PBS -l select=2:ncpus=20:phase=10:mem=100gb:mpiprocs=20 #PBS -l walltime=1:00:00 module load gromacs/5.0.5-nogpu cd $PBS_O_WORKDIR export OMP_NUM_THREADS=1 mpirun mdrun_mpi ...","title":"Running GROMACS without GPU"},{"location":"software/packages/gromacs/#running-gpu-enabled-gromacs","text":"For the GPU version #!/bin/bash #PBS -l select=2:ncpus=20:ngpus=2:phase=10:mem=100gb:mpiprocs=2 #PBS -l walltime=1:00:00 module load gromacs/5.0.5-gpu cd $PBS_O_WORKDIR export OMP_NUM_THREADS=10 mpirun mdrun_mpi ... Note that the number of MPI processes per chunk is set as 2 when using the GPU. Normally, the number of MPI processes is the same as the number of CPU cores requested. But because there are only 2 GPUs per node, we launch only two MPI processes. In the above example, each MPI process can also use 10 CPU cores in addition to a GPU (this is enabled by setting the variable OMP_NUM_THREADS to 10).","title":"Running GPU-enabled GROMACS"},{"location":"software/packages/gromacs/#gpu-enabled-gromacs-using-singularity-and-ngc","text":"The NVIDIA GPU cloud provides images for GPU-enabled GROMACS that can be downloaded and run using Singularity on the Palmetto cluster. This is the recommended way to run GROMACS on Palmetto.","title":"GPU-enabled GROMACS using Singularity and NGC"},{"location":"software/packages/gromacs/#downloading-the-image","text":"Before downloading images from NGC, you will need to obtain an NVIDIA NGC API key, instructions for which can be found here . Start an interactive job, and create a directory for storing Singularity images (if you don't have one already): $ qsub -I -l select=1:ngpus=2:ncpus=16:mem=20gb:gpu_model=p100,walltime=5:00:00 $ mkdir -p singularity-images Set the required environment variables (you will need the API key for this step): $ export SINGULARITY_DOCKER_USERNAME='$oauthtoken' $ export SINGULARITY_DOCKER_PASSWORD=<NVIDA NGC API key> Navigate to the singularity-images directory and pull the GROMACS image (choose from the images listed here : $ cd ~/singularity-images $ module load singularity $ singularity pull docker://nvcr.io/hpc/gromacs:2016.4 The above should take a few seconds to complete.","title":"Downloading the image"},{"location":"software/packages/gromacs/#running-gromacs-interactively","text":"As an example, we'll consider running the GROMACS ADH benchmark. First, request an interactive job: $ qsub -I -l select=1:ngpus=2:ncpus=16:mpiprocs=16:mem=120gb:gpu_model=p100,walltime=5:00:00 Load the singularity module: $ module load singularity Prepare the input and output directories: In this case, we use /scratch3/$USER/gromacs_ADH as both the input directory (containing the data) and the output directory (where output will be stored). Remember that $TMPDIR is cleaned up after the job completes, so it's important to move any data out of $TMPDIR after GROMACS completes running. Also, in this case, we are downloading the input data from the web, but if your input is stored in the /home /, /scratch or some other directory, then you can copy it to $TMPDIR : $ mkdir -p /scratch3/$USER/gromacs_ADH_benchmark $ cd /scratch3/$USER/gromacs_ADH_benchmark $ wget ftp://ftp.gromacs.org/pub/benchmarks/ADH_bench_systems.tar.gz $ tar -xvf ADH_bench_systems.tar.gz Use singularity shell to interactively run a container from the downloaded GROMACS image. The -B switch is used to \"bind\" directories on the host (Palmetto compute node) to directories in the container. The different bindings used are: /scratch3/$USER/gromacs_ADH_benchmark is bound to /input /scratch3/$USER/gromacs_ADH_benchmark is bound to /output $TMPDIR is bound to /work $ singularity shell --nv \\ -B /scratch3/$USER/gromacs_ADH_benchmark:/input \\ -B /scratch3/$USER/gromacs_ADH_benchmark:/output \\ -B $TMPDIR:/work \\ --pwd /work \\ ~/singularity-images/gromacs-2016.4.simg \\ Running the benchmark: $ source /opt/gromacs/install/2016.4/bin/GMXRC $ gmx_mpi grompp -f /input/adh_cubic/pme_verlet.mdp -c /input/adh_cubic/conf.gro -p /input/adh_cubic/topol.top $ export OMP_NUM_THREADS=8 $ mpirun -np 2 gmx_mpi mdrun -g adh_cubic.log -pin on -resethway -v -noconfout -nsteps 10000 -s topol.tpr -ntomp $OMP_NUM_THREADS After the last command above completes, the .edr and .log files produced by GROMACS should be visible. Typically, the next step is to copy these results to the output directory: $ cp *.log *.edr /output $ exit Upon exiting the container, the .log and .edr files will be found in the output directory, /scratch3/$USER/gromacs_ADH_benchmark .","title":"Running GROMACS interactively"},{"location":"software/packages/gromacs/#running-gromacs-in-batch-mode","text":"The same benchmark can be run in batch mode by encapsulating the commands in a script run_adh.sh , and running it non-interactively using singularity exec . # run_adh.sh source /opt/gromacs/install/2016.4/bin/GMXRC gmx_mpi grompp -f /input/adh_cubic/pme_verlet.mdp -c /input/adh_cubic/conf.gro -p /input/adh_cubic/topol.top export OMP_NUM_THREADS=8 mpirun -np 2 gmx_mpi mdrun -g adh_cubic.log -pin on -resethway -v -noconfout -nsteps 10000 -s topol.tpr -ntomp $OMP_NUM_THREADS cp *.log *.edr /output The PBS batch script for submitting the above (assuming that the /scratch3/$USER/gromacs_ADH_benchmark directory already contains the input files): #PBS -N adh_cubic #PBS -l select=1:ngpus=2:ncpus=16:mem=20gb:gpu_model=p100,walltime=5:00:00 module load singularity cd $PBS_O_WORKDIR cp run_adh.sh $TMPDIR singularity exec --nv \\ -B /scratch3/$USER/gromacs_ADH_benchmark:/input \\ -B /scratch3/$USER/gromacs_ADH_benchmark:/output \\ -B $TMPDIR:/work \\ --pwd /work \\ ~/singularity-images/gromacs-2016.4.simg bash run_adh.sh","title":"Running GROMACS in batch mode"},{"location":"software/packages/hoomd/","text":"HOOMD Run HOOMD Now the HOOMD-BLUE v2.3.5 has been installed. Create a simple python file \u201ctest_hoomd.py\u201d to run HOOMD import hoomd import hoomd.md hoomd.context.initialize(\"\"); hoomd.init.create_lattice(unitcell=hoomd.lattice.sc(a=2.0), n=5); nl = hoomd.md.nlist.cell(); lj = hoomd.md.pair.lj(r_cut=2.5, nlist=nl); lj.pair_coeff.set('A', 'A', epsilon=1.0, sigma=1.0); hoomd.md.integrate.mode_standard(dt=0.005); all = hoomd.group.all(); hoomd.md.integrate.langevin(group=all, kT=0.2, seed=42);hoomd.analyze.log(filename=\"log-output.log\", quantities=['potential_energy', 'temperature'], period=100, overwrite=True); hoomd.dump.gsd(\"trajectory.gsd\", period=2e3, group=all, overwrite=True); hoomd.run(1e4); If you have logged out of the node, request an interactive session on a GPU node and add required modules: $ qsub -I -l select=1:ncpus=16:mem=64gb:ngpus=2:gpu_model=p100:mpiprocs=16:interconnect=fdr,walltime=2:00:00 $ module add anaconda3/5.1.0 gcc/5.4.0 cuda-toolkit/9.0.176 Run the script interactively: $ python test_hoomd.py Alternatively, you can setup a PBS job script to run HOOMD in batch mode. A sample is below for Test_Hoomd.sh : #PBS -N HOOMD #PBS -l select=1:ncpus=16:mem=64gb:ngpus=2:gpu_model=p100:mpiprocs=16:interconnect=fdr,walltime=02:00:00 #PBS -j oe module purge module add anaconda3/5.1.0 gcc/5.4.0 cuda-toolkit/9.0.176 cd $PBS_O_WORKDIR python test_hoomd.py Submit the job: $ qsub Test_Hoomd.sh This is it.","title":"Hoomd"},{"location":"software/packages/hoomd/#hoomd","text":"","title":"HOOMD"},{"location":"software/packages/hoomd/#run-hoomd","text":"Now the HOOMD-BLUE v2.3.5 has been installed. Create a simple python file \u201ctest_hoomd.py\u201d to run HOOMD import hoomd import hoomd.md hoomd.context.initialize(\"\"); hoomd.init.create_lattice(unitcell=hoomd.lattice.sc(a=2.0), n=5); nl = hoomd.md.nlist.cell(); lj = hoomd.md.pair.lj(r_cut=2.5, nlist=nl); lj.pair_coeff.set('A', 'A', epsilon=1.0, sigma=1.0); hoomd.md.integrate.mode_standard(dt=0.005); all = hoomd.group.all(); hoomd.md.integrate.langevin(group=all, kT=0.2, seed=42);hoomd.analyze.log(filename=\"log-output.log\", quantities=['potential_energy', 'temperature'], period=100, overwrite=True); hoomd.dump.gsd(\"trajectory.gsd\", period=2e3, group=all, overwrite=True); hoomd.run(1e4); If you have logged out of the node, request an interactive session on a GPU node and add required modules: $ qsub -I -l select=1:ncpus=16:mem=64gb:ngpus=2:gpu_model=p100:mpiprocs=16:interconnect=fdr,walltime=2:00:00 $ module add anaconda3/5.1.0 gcc/5.4.0 cuda-toolkit/9.0.176 Run the script interactively: $ python test_hoomd.py Alternatively, you can setup a PBS job script to run HOOMD in batch mode. A sample is below for Test_Hoomd.sh : #PBS -N HOOMD #PBS -l select=1:ncpus=16:mem=64gb:ngpus=2:gpu_model=p100:mpiprocs=16:interconnect=fdr,walltime=02:00:00 #PBS -j oe module purge module add anaconda3/5.1.0 gcc/5.4.0 cuda-toolkit/9.0.176 cd $PBS_O_WORKDIR python test_hoomd.py Submit the job: $ qsub Test_Hoomd.sh This is it.","title":"Run HOOMD"},{"location":"software/packages/java/","text":"Java The Java Runtime Environment (JRE) version 1.6.0_11 is currently available cluster-wide on Palmetto. If a user needs a different version of Java, or if the Java Development Kit (JDK, which includes the JRE) is needed, that user is encouraged to download and install Java (JRE or JDK) for herself. Below is a brief overview of installing the JDK in a user's /home directory. JRE vs. JDK The JRE is basically the Java Virtual Machine (Java VM) that provides a platform for running your Java programs. The JDK is the fully featured Software Development Kit for Java, including the JRE, compilers, and tools like JavaDoc and Java Debugger used to create and compile programs. Usually, when you only care about running Java programs, the JRE is all you'll need. If you are planning to do some Java programming, you will need the JDK. Downloading the JDK The JDK cannot be downloaded directly using the wget utility because a user must agree to Oracle's Java license ageement when downloading. So, download the JDK using a web browser and transfer the downloaded jdk-7uXX-linux-x64.tar.gz file to your /home directory on Palmetto using scp , sftp , or FileZilla: scp jdk-7u45-linux-x64.tar.gz galen@login.palmetto.clemson.edu:/home/galen jdk-7u45-linux-x64.tar.gz 100% 132MB 57.7KB/s 38:58 Installing the JDK The JDK is distributed in a Linux x86_64 compatible binary format, so once it has been unpacked, it is ready to use (no need to compile). However, you will need to setup your environment for using this new package by adding lines similar to the following at the end of your ~/.bashrc file: export JAVA_HOME=/home/galen/jdk1.7.0_45 export PATH=$JAVA_HOME/bin:$PATH export MANPATH=$JAVA_HOME/man:$MANPATH Once this is done, you can log-out and log-in again or simply source your ~/.bashrc file and then you'll be ready to begin using your new Java installation.","title":"Java"},{"location":"software/packages/java/#java","text":"The Java Runtime Environment (JRE) version 1.6.0_11 is currently available cluster-wide on Palmetto. If a user needs a different version of Java, or if the Java Development Kit (JDK, which includes the JRE) is needed, that user is encouraged to download and install Java (JRE or JDK) for herself. Below is a brief overview of installing the JDK in a user's /home directory.","title":"Java"},{"location":"software/packages/java/#jre-vs-jdk","text":"The JRE is basically the Java Virtual Machine (Java VM) that provides a platform for running your Java programs. The JDK is the fully featured Software Development Kit for Java, including the JRE, compilers, and tools like JavaDoc and Java Debugger used to create and compile programs. Usually, when you only care about running Java programs, the JRE is all you'll need. If you are planning to do some Java programming, you will need the JDK.","title":"JRE vs. JDK"},{"location":"software/packages/java/#downloading-the-jdk","text":"The JDK cannot be downloaded directly using the wget utility because a user must agree to Oracle's Java license ageement when downloading. So, download the JDK using a web browser and transfer the downloaded jdk-7uXX-linux-x64.tar.gz file to your /home directory on Palmetto using scp , sftp , or FileZilla: scp jdk-7u45-linux-x64.tar.gz galen@login.palmetto.clemson.edu:/home/galen jdk-7u45-linux-x64.tar.gz 100% 132MB 57.7KB/s 38:58","title":"Downloading the JDK"},{"location":"software/packages/java/#installing-the-jdk","text":"The JDK is distributed in a Linux x86_64 compatible binary format, so once it has been unpacked, it is ready to use (no need to compile). However, you will need to setup your environment for using this new package by adding lines similar to the following at the end of your ~/.bashrc file: export JAVA_HOME=/home/galen/jdk1.7.0_45 export PATH=$JAVA_HOME/bin:$PATH export MANPATH=$JAVA_HOME/man:$MANPATH Once this is done, you can log-out and log-in again or simply source your ~/.bashrc file and then you'll be ready to begin using your new Java installation.","title":"Installing the JDK"},{"location":"software/packages/julia/","text":"Julia Julia: high-level dynamic programming language that was originally designed to address the needs of high-performance numerical analysis and computational science. Run Julia in Palmetto: Interactive There are a few different versions of Julia available on the cluster. $ module avail julia --------------------------------------------- /software/modulefiles --------------------------------------------- julia/0.6.2 julia/0.7.0 julia/1.0.4 julia/1.1.1 Let demonstrate how to use julia/1.1.1 in the Palmetto cluster together with Gurobi Optimizer (a commercial optimization solver for linear programming), quadratic programming, etc. Clemson University has different version of licenses for Gurobi solver. In this example, I would like to use Julia and Gurobi solver to solve a linear math problem using Palmetto HPC Problems: Maximize x+y Given the following constrains: 50 x + 24 y <= 2400 30 x + 33 y <= 2100 x >= 5, y >= 45 Let prepare a script to solve this problem, named: jump_gurobi.jl. You can save this file to: /scratch1/$username/Julia/ # Request for a compute node: $ qsub -I -l select=1:ncpus=8:mem=16gb:interconnect=fdr,walltime=01:00:00 # Go to working folder: $ cd /scratch1/$username/Julia $ nano jump_gurobi.jl Then type/copy the following code to the file jump_gurobi.jl import Pkg using JuMP using Gurobi m = Model(with_optimizer(Gurobi.Optimizer)) @variable(m, x >= 5) @variable(m, y >= 45) @objective(m, Max, x + y) @constraint(m, 50x + 24y <= 2400) @constraint(m, 30x + 33y <= 2100) status = optimize!(m) println(\" x = \", JuMP.value(x), \" y = \", JuMP.value(y)) Save the jump_gurobi.jl file then you are ready to run julia: $ module add julia/1.1.1 gurobi/7.0.2 $ julia # the julia prompt appears: julia> # Next install Package: JuMP and Gurobi julia> using Pkg julia> Pkg.add(\"JuMP\") julia> Pkg.add(\"Gurobi\") julia> exit() Run the julia_gurobi.jl script: $ julia jump_gurobi.jl Run Julia in Palmetto: Batch mode Alternatively, you can setup a PBS job script to run Julia in batch mode. A sample is below for submit_julia.sh : You must install the JuMP and Gurobi package first (one time installation) #!/bin/bash #PBS -N Julia #PBS -l select=1:ncpus=8:mem=16gb:interconnect=fdr #PBS -l walltime=02:00:00 #PBS -j oe module purge module add julia/1.1.1 gurobi/7.0.2 cd $PBS_O_WORKDIR julia jump_gurobi.jl > output_JuMP.txt Submit the job: $ qsub submit_julia.sh The output file can be found at the same folder: output_JuMP.txt Install your own Julia package using conda environment and running in Jupyterhub In addition to traditional compilation of Julia, it is possible to install your own version of Julia and setup kernel to work using Jupterhub. # Request for a compute node: $ qsub -I -l select=1:ncpus=8:mem=16gb:interconnect=fdr,walltime=01:00:00 $ module add anaconda3/5.1.0 # Create conda environment with the name as \"Julia\" $ conda create -n Julia -c conda-forge julia $ source activate Julia (Julia) [$username@node1234 ~]$ (Julia) [$username@node1234 ~]$ julia julia> julia> using Pkg julia> Pkg.add(\"IJulia\") julia> exit Exit Julia and Start Jupyterhub in Palmetto After spawning, Click on New kernel in Jupyterhub, you will see Julia 1.1.1 kernel available for use Type in the follwing code to test: println(\"Hello world\")","title":"Julia"},{"location":"software/packages/julia/#julia","text":"Julia: high-level dynamic programming language that was originally designed to address the needs of high-performance numerical analysis and computational science.","title":"Julia"},{"location":"software/packages/julia/#run-julia-in-palmetto-interactive","text":"There are a few different versions of Julia available on the cluster. $ module avail julia --------------------------------------------- /software/modulefiles --------------------------------------------- julia/0.6.2 julia/0.7.0 julia/1.0.4 julia/1.1.1 Let demonstrate how to use julia/1.1.1 in the Palmetto cluster together with Gurobi Optimizer (a commercial optimization solver for linear programming), quadratic programming, etc. Clemson University has different version of licenses for Gurobi solver. In this example, I would like to use Julia and Gurobi solver to solve a linear math problem using Palmetto HPC Problems: Maximize x+y Given the following constrains: 50 x + 24 y <= 2400 30 x + 33 y <= 2100 x >= 5, y >= 45 Let prepare a script to solve this problem, named: jump_gurobi.jl. You can save this file to: /scratch1/$username/Julia/ # Request for a compute node: $ qsub -I -l select=1:ncpus=8:mem=16gb:interconnect=fdr,walltime=01:00:00 # Go to working folder: $ cd /scratch1/$username/Julia $ nano jump_gurobi.jl Then type/copy the following code to the file jump_gurobi.jl import Pkg using JuMP using Gurobi m = Model(with_optimizer(Gurobi.Optimizer)) @variable(m, x >= 5) @variable(m, y >= 45) @objective(m, Max, x + y) @constraint(m, 50x + 24y <= 2400) @constraint(m, 30x + 33y <= 2100) status = optimize!(m) println(\" x = \", JuMP.value(x), \" y = \", JuMP.value(y)) Save the jump_gurobi.jl file then you are ready to run julia: $ module add julia/1.1.1 gurobi/7.0.2 $ julia # the julia prompt appears: julia> # Next install Package: JuMP and Gurobi julia> using Pkg julia> Pkg.add(\"JuMP\") julia> Pkg.add(\"Gurobi\") julia> exit() Run the julia_gurobi.jl script: $ julia jump_gurobi.jl","title":"Run Julia in Palmetto: Interactive"},{"location":"software/packages/julia/#run-julia-in-palmetto-batch-mode","text":"Alternatively, you can setup a PBS job script to run Julia in batch mode. A sample is below for submit_julia.sh : You must install the JuMP and Gurobi package first (one time installation) #!/bin/bash #PBS -N Julia #PBS -l select=1:ncpus=8:mem=16gb:interconnect=fdr #PBS -l walltime=02:00:00 #PBS -j oe module purge module add julia/1.1.1 gurobi/7.0.2 cd $PBS_O_WORKDIR julia jump_gurobi.jl > output_JuMP.txt Submit the job: $ qsub submit_julia.sh The output file can be found at the same folder: output_JuMP.txt","title":"Run Julia in Palmetto: Batch mode"},{"location":"software/packages/julia/#install-your-own-julia-package-using-conda-environment-and-running-in-jupyterhub","text":"In addition to traditional compilation of Julia, it is possible to install your own version of Julia and setup kernel to work using Jupterhub. # Request for a compute node: $ qsub -I -l select=1:ncpus=8:mem=16gb:interconnect=fdr,walltime=01:00:00 $ module add anaconda3/5.1.0 # Create conda environment with the name as \"Julia\" $ conda create -n Julia -c conda-forge julia $ source activate Julia (Julia) [$username@node1234 ~]$ (Julia) [$username@node1234 ~]$ julia julia> julia> using Pkg julia> Pkg.add(\"IJulia\") julia> exit Exit Julia and Start Jupyterhub in Palmetto After spawning, Click on New kernel in Jupyterhub, you will see Julia 1.1.1 kernel available for use Type in the follwing code to test: println(\"Hello world\")","title":"Install your own Julia package using conda environment and running in Jupyterhub"},{"location":"software/packages/lammps/","text":"LAMMPS There are a few different versions of LAMMPS available on the cluster, but users are encouraged to install their own version of LAMMPS in case newer versions different configurations are desired. In particular, there are two main components of LAMMPS which can be run with CPUs (lmp_mpi) or with GPUs (lmp) $ module avail lammps lammps/10Jan15-dp lammps/17Dec13-dp lammps/17Dec13-dp-k20 lammps/2018Dec12 lammps/29Aug14-sp-k20 Installing LAMMPS on Palmetto cluster In this example, we will demonstrate installing LAMMPS to run with CPUs and GPUs (version 7 Aug 2019 ). Detail information can be found here: https://lammps.sandia.gov/doc/Install.html After logging in, ask for an interactive session (with GPU): ~~~ $ qsub -I -l select=1:ncpus=8:mpiprocs=8:ngpus=1:mem=64gb:gpu_model=v100,walltime=8:00:00 ~~~ Load the required modules. Specifically, note that we are loading the CUDA-enabled module: ~~~~ $ module load openmpi/3.1.4-gcc71-ucx fftw/3.3.4-g481 cuda-toolkit/9.2 cmake/3.13.1 ~~~~ Download the LAMMPS source code from http://lammps.sandia.gov/download.html. The detailed instruction on usage and compilation options are available at http://lammps.sandia.gov/doc/Manual.html. Unpack the source code and enter the package directory: ~~~ $ tar -xvf lammps-stable.tar.gz $ cd lammps-7Aug19 ~~~ There are two different methods of compiling LAMMPS using make (support CPUs) or cmake (support GPUs). We introduce both methods here: Compile LAMMPS using make with KOKKOS support ~~~ $ cd src $ make yes-kokkos $ make mpi -j8 ~~~ A new executable file lmp_mpi will be produced in the src folder. You can copy this executable over to the working directory of your jobs. Note that the size of the executable may be a hundred MB Note: there are optional packages which can be installed together with the above package using make yes- prior to running make mpi . For example: ~~~ $ make yes-USER-MISC $ make yes-KSPACE $ make yes-MOLECULE $ make yes-MISC $ make yes-manybody $ make yes-dipole $ make yes-class2 $ make yes-asphere $ make yes-replica $ make yes-granular $ make yes-rigid ~~~ Compile LAMMPS using cmake with GPU support: The detailed manual can be found here with more information on the options to be included. In this instruction, we only cover a certain number of options that are enough install cmake with GPU: ~~~ $ mkdir build $ cd build $ cmake ../cmake/ -DPKG_GPU=on -DGPU_API=cuda -DGPU_PREC=double -DGPU_ARCH=sm_70 -DCUDA_CUDA_LIBRARY=/usr/lib64/libcuda.so -DFFMPEG_EXECUTABLE=/software/ffmpeg/3.3.2/bin/ffmpeg -DPKG_MC=on -DPKG_REPLICA=on -DPKG_SNAP=on -DPKG-MANYBODY=on -DPKG_USER-MEAMC=on -DPKG_USER-MISC=on -DPKG_MISC=on $ make -j8 ~~~ In the above script; the -DGPU_ARCH=sm_70 was used because the gpu_model was set at Volta (v100). Please refer to the above guideline for setting particular gpu architecture Note: there are optional packages which can be installed together with the above package using -D[Options]=on . For example: ~~~ $ cmake ../cmake/ -DPKG_MISC=on -DPKG_KSPACE=on -DPKG_MOLECULE=on -DPKG_MANYBODY=on -DPKG_DIPOLE=on -DPKG_CLASS2=on -DPKG_ASPHERE=on -DPKG_REPLICA=on -DPKG_GRANULAR=on -DPKG_RIGID=on ~~~ The executable file lmp will be produced. You can copy this executable over to the working directory of your jobs. Note that the size of the executable may be a hundred MB. Running LAMMPS - an example Several existing examples are in the installed folder: lammps-7Aug19/examples/ Detailes description of all examples are here . In order to run the example, simply copy the executable files created from step 5 and 6 to particular example folder and follow the README for detailed description on how to run. For instance, in order to run an example accelerate using GPU package. Copy lmp to that folder. Here is a sample batch script job.sh for this example: #PBS -N accelerate #PBS -l select=1:ncpus=8:mpiprocs=8:ngpus=1:gpu_model=v100:mem=64gb,walltime=1:00:00 #PBS -j oe module purge module load openmpi/3.1.4-gcc71-ucx mpirun -np 8 lmp -sf gpu < in.lj # 8 MPI, 8 MPI/GPU","title":"Lammps"},{"location":"software/packages/lammps/#lammps","text":"There are a few different versions of LAMMPS available on the cluster, but users are encouraged to install their own version of LAMMPS in case newer versions different configurations are desired. In particular, there are two main components of LAMMPS which can be run with CPUs (lmp_mpi) or with GPUs (lmp) $ module avail lammps lammps/10Jan15-dp lammps/17Dec13-dp lammps/17Dec13-dp-k20 lammps/2018Dec12 lammps/29Aug14-sp-k20","title":"LAMMPS"},{"location":"software/packages/lammps/#installing-lammps-on-palmetto-cluster","text":"In this example, we will demonstrate installing LAMMPS to run with CPUs and GPUs (version 7 Aug 2019 ). Detail information can be found here: https://lammps.sandia.gov/doc/Install.html After logging in, ask for an interactive session (with GPU): ~~~ $ qsub -I -l select=1:ncpus=8:mpiprocs=8:ngpus=1:mem=64gb:gpu_model=v100,walltime=8:00:00 ~~~ Load the required modules. Specifically, note that we are loading the CUDA-enabled module: ~~~~ $ module load openmpi/3.1.4-gcc71-ucx fftw/3.3.4-g481 cuda-toolkit/9.2 cmake/3.13.1 ~~~~ Download the LAMMPS source code from http://lammps.sandia.gov/download.html. The detailed instruction on usage and compilation options are available at http://lammps.sandia.gov/doc/Manual.html. Unpack the source code and enter the package directory: ~~~ $ tar -xvf lammps-stable.tar.gz $ cd lammps-7Aug19 ~~~ There are two different methods of compiling LAMMPS using make (support CPUs) or cmake (support GPUs). We introduce both methods here: Compile LAMMPS using make with KOKKOS support ~~~ $ cd src $ make yes-kokkos $ make mpi -j8 ~~~ A new executable file lmp_mpi will be produced in the src folder. You can copy this executable over to the working directory of your jobs. Note that the size of the executable may be a hundred MB Note: there are optional packages which can be installed together with the above package using make yes- prior to running make mpi . For example: ~~~ $ make yes-USER-MISC $ make yes-KSPACE $ make yes-MOLECULE $ make yes-MISC $ make yes-manybody $ make yes-dipole $ make yes-class2 $ make yes-asphere $ make yes-replica $ make yes-granular $ make yes-rigid ~~~ Compile LAMMPS using cmake with GPU support: The detailed manual can be found here with more information on the options to be included. In this instruction, we only cover a certain number of options that are enough install cmake with GPU: ~~~ $ mkdir build $ cd build $ cmake ../cmake/ -DPKG_GPU=on -DGPU_API=cuda -DGPU_PREC=double -DGPU_ARCH=sm_70 -DCUDA_CUDA_LIBRARY=/usr/lib64/libcuda.so -DFFMPEG_EXECUTABLE=/software/ffmpeg/3.3.2/bin/ffmpeg -DPKG_MC=on -DPKG_REPLICA=on -DPKG_SNAP=on -DPKG-MANYBODY=on -DPKG_USER-MEAMC=on -DPKG_USER-MISC=on -DPKG_MISC=on $ make -j8 ~~~ In the above script; the -DGPU_ARCH=sm_70 was used because the gpu_model was set at Volta (v100). Please refer to the above guideline for setting particular gpu architecture Note: there are optional packages which can be installed together with the above package using -D[Options]=on . For example: ~~~ $ cmake ../cmake/ -DPKG_MISC=on -DPKG_KSPACE=on -DPKG_MOLECULE=on -DPKG_MANYBODY=on -DPKG_DIPOLE=on -DPKG_CLASS2=on -DPKG_ASPHERE=on -DPKG_REPLICA=on -DPKG_GRANULAR=on -DPKG_RIGID=on ~~~ The executable file lmp will be produced. You can copy this executable over to the working directory of your jobs. Note that the size of the executable may be a hundred MB.","title":"Installing LAMMPS on Palmetto cluster"},{"location":"software/packages/lammps/#running-lammps-an-example","text":"Several existing examples are in the installed folder: lammps-7Aug19/examples/ Detailes description of all examples are here . In order to run the example, simply copy the executable files created from step 5 and 6 to particular example folder and follow the README for detailed description on how to run. For instance, in order to run an example accelerate using GPU package. Copy lmp to that folder. Here is a sample batch script job.sh for this example: #PBS -N accelerate #PBS -l select=1:ncpus=8:mpiprocs=8:ngpus=1:gpu_model=v100:mem=64gb,walltime=1:00:00 #PBS -j oe module purge module load openmpi/3.1.4-gcc71-ucx mpirun -np 8 lmp -sf gpu < in.lj # 8 MPI, 8 MPI/GPU","title":"Running LAMMPS - an example"},{"location":"software/packages/matlab/","text":"MATLAB Checking license usage for MATLAB You can check the availability of MATLAB licenses using the lmstat command: $ /software/USR_LOCAL/flexlm/lmstat -a -c /software/USR_LOCAL/flexlm/licenses/matlab.dat Running the MATLAB graphical interface To launch the MATLAB graphical interface, you must first you must log-in with tunneling enabled , and then ask for an interactive session: $ qsub -I -X -l select=1:ncpus=2:mem=24gb,walltime=1:00:00 Once logged-in, you must load one of the MATLAB modules: $ module add matlab/2018b And then launch the MATLAB program: $ matlab Warning : DO NOT attempt to run MATLAB right after logging-in (i.e., on the login001 node). Always ask for an interactive job first. MATLAB sessions are automatically killed on the login node. Running the MATLAB command line without graphics To use the MATLAB command-line interface without graphics, you can additionally use the -nodisplay and -nosplash options: $ matlab -nodisplay -nosplash To quit matlab command-line interface, type: $ exit MATLAB in batch jobs To use MATLAB in your batch jobs, you can use the -r switch provided by MATLAB, which lets you r un commands specified on the command-line. For example: $ matlab -nodisplay -nosplash -r myscript will run the MATLAB script myscript.m , Or: $ matlab -nodisplay -nosplash < myscript.m > myscript_results.txt will run the MATLAB script myscript.m and write the output to myscript_results.txt file. Thus, an example batch job using MATLAB could have a batch script as follows: #!/bin/bash # #PBS -N test_matlab #PBS -l select=1:ncpus=1:mem=5gb #PBS -l walltime=1:00:00 module add matlab/2018b cd $PBS_O_WORKDIR taskset -c 0-$(($OMP_NUM_THREADS-1)) matlab -nodisplay -nosplash < myscript.m > myscript_results.txt Note : MATLAB will sometimes attemps to use all available CPU cores on the node it is running on. If you haven't reserved all cores on the node, your job may be killed if this happens. To avoid this, you can use the taskset utility to set the \"core affinity\" (as shown above). As an example: $ taskset 0-2 <application> will limit application to using 3 CPU cores. On Palmetto, the variable OMP_NUM_THREADS is automatically set to be the number of cores requested for a job. Thus, you can use 0-$((OMP_NUM_THREADS-1)) as shown in the above batch script to use all the cores you requested. Compiling MATLAB code to create an executable Often, you need to run a large number of MATLAB jobs concurrently (e.g,m each job operating on different data). In such cases, you can avoid over-utilizing MATLAB licenses by compiling your MATLAB code into an executable. This can be done from within the MATLAB command-line as follows: $ matlab -nodisplay -nosplash >> mcc -R -nodisplay -R -singleCompThread -R -nojvm -m mycode.m Note: MATLAB will try to use all the available CPU cores on the system where it is running, and this presents a problem when your compiled executable on the cluster where available cores on a single node might be shared amongst mulitple users. You can disable this \"feature\" when you compile your code by adding the -R -singleCompThread option, as shown above. The above command will produce the executable mycode , corresponding to the M-file mycode.m . If you have multiple M-files in your project and want to create a single excutable, you can use a command like the following: >> mcc -R -nodisplay -R -singleCompThread -R -nojvm -m my_main_code.m myfunction1.m myfunction2.m myfunction3.m Once the executable is produced, you can run it like any other executable in your batch jobs. Of course, you'll also need the same matlab and (optional) GCC module loaded for your job's runtime environment.","title":"Matlab"},{"location":"software/packages/matlab/#matlab","text":"","title":"MATLAB"},{"location":"software/packages/matlab/#checking-license-usage-for-matlab","text":"You can check the availability of MATLAB licenses using the lmstat command: $ /software/USR_LOCAL/flexlm/lmstat -a -c /software/USR_LOCAL/flexlm/licenses/matlab.dat","title":"Checking license usage for MATLAB"},{"location":"software/packages/matlab/#running-the-matlab-graphical-interface","text":"To launch the MATLAB graphical interface, you must first you must log-in with tunneling enabled , and then ask for an interactive session: $ qsub -I -X -l select=1:ncpus=2:mem=24gb,walltime=1:00:00 Once logged-in, you must load one of the MATLAB modules: $ module add matlab/2018b And then launch the MATLAB program: $ matlab Warning : DO NOT attempt to run MATLAB right after logging-in (i.e., on the login001 node). Always ask for an interactive job first. MATLAB sessions are automatically killed on the login node.","title":"Running the MATLAB graphical interface"},{"location":"software/packages/matlab/#running-the-matlab-command-line-without-graphics","text":"To use the MATLAB command-line interface without graphics, you can additionally use the -nodisplay and -nosplash options: $ matlab -nodisplay -nosplash To quit matlab command-line interface, type: $ exit","title":"Running the MATLAB command line without graphics"},{"location":"software/packages/matlab/#matlab-in-batch-jobs","text":"To use MATLAB in your batch jobs, you can use the -r switch provided by MATLAB, which lets you r un commands specified on the command-line. For example: $ matlab -nodisplay -nosplash -r myscript will run the MATLAB script myscript.m , Or: $ matlab -nodisplay -nosplash < myscript.m > myscript_results.txt will run the MATLAB script myscript.m and write the output to myscript_results.txt file. Thus, an example batch job using MATLAB could have a batch script as follows: #!/bin/bash # #PBS -N test_matlab #PBS -l select=1:ncpus=1:mem=5gb #PBS -l walltime=1:00:00 module add matlab/2018b cd $PBS_O_WORKDIR taskset -c 0-$(($OMP_NUM_THREADS-1)) matlab -nodisplay -nosplash < myscript.m > myscript_results.txt Note : MATLAB will sometimes attemps to use all available CPU cores on the node it is running on. If you haven't reserved all cores on the node, your job may be killed if this happens. To avoid this, you can use the taskset utility to set the \"core affinity\" (as shown above). As an example: $ taskset 0-2 <application> will limit application to using 3 CPU cores. On Palmetto, the variable OMP_NUM_THREADS is automatically set to be the number of cores requested for a job. Thus, you can use 0-$((OMP_NUM_THREADS-1)) as shown in the above batch script to use all the cores you requested.","title":"MATLAB in batch jobs"},{"location":"software/packages/matlab/#compiling-matlab-code-to-create-an-executable","text":"Often, you need to run a large number of MATLAB jobs concurrently (e.g,m each job operating on different data). In such cases, you can avoid over-utilizing MATLAB licenses by compiling your MATLAB code into an executable. This can be done from within the MATLAB command-line as follows: $ matlab -nodisplay -nosplash >> mcc -R -nodisplay -R -singleCompThread -R -nojvm -m mycode.m Note: MATLAB will try to use all the available CPU cores on the system where it is running, and this presents a problem when your compiled executable on the cluster where available cores on a single node might be shared amongst mulitple users. You can disable this \"feature\" when you compile your code by adding the -R -singleCompThread option, as shown above. The above command will produce the executable mycode , corresponding to the M-file mycode.m . If you have multiple M-files in your project and want to create a single excutable, you can use a command like the following: >> mcc -R -nodisplay -R -singleCompThread -R -nojvm -m my_main_code.m myfunction1.m myfunction2.m myfunction3.m Once the executable is produced, you can run it like any other executable in your batch jobs. Of course, you'll also need the same matlab and (optional) GCC module loaded for your job's runtime environment.","title":"Compiling MATLAB code to create an executable"},{"location":"software/packages/mothur/","text":"mothur This page instructs how to install mothur software to Palmetto The code and the issue tracker can be found here Request an interactive session. For example: $ qsub -I -l select=1:ncpus=6:mem=24gb:interconnect=fdr,walltime=3:00:00 Load the required modules $ module load boost/1.65.1 hdf5/1.10.1 openmpi/1.10.3 gcc/8.2.0 Download mothur from source . Here we download the latest version 1.41.3 $ wget https://github.com/mothur/mothur/archive/v1.41.3.tar.gz Unpack the downloaded file and go into mothur source folder $ tar -xvf v1.41.3.tar.gz $ cd mothur-1.41.3 Modify the Makefile $ nano Makefile #Replace the following lines 22-32 with information: (Note: username should be replaced) OPTIMIZE ?= yes USEREADLINE ?= yes USEBOOST ?= yes USEHDF5 ?= yes LOGFILE_NAME ?= no BOOST_LIBRARY_DIR ?= \"/software/boost/1.65.1/lib\" BOOST_INCLUDE_DIR ?= \"/software/boost/1.65.1/include\" HDF5_LIBRARY_DIR ?= \"/software/hdf5/1.10.1/lib\" HDF5_INCLUDE_DIR ?= \"/software/hdf5/1.10.1/include\" MOTHUR_FILES ?= \"\\\"home/username/application/bin/mothur\\\"\" VERSION = \"\\\"1.41.3\\\"\" Run make file $ make The mothur executable file will be created in the installation folder: /home/username/applications/bin . Make sure you set the correct environment PATH in ~/.bashrc file export PATH=$PATH:$HOME/applications/bin","title":"Mothur"},{"location":"software/packages/mothur/#mothur","text":"This page instructs how to install mothur software to Palmetto The code and the issue tracker can be found here Request an interactive session. For example: $ qsub -I -l select=1:ncpus=6:mem=24gb:interconnect=fdr,walltime=3:00:00 Load the required modules $ module load boost/1.65.1 hdf5/1.10.1 openmpi/1.10.3 gcc/8.2.0 Download mothur from source . Here we download the latest version 1.41.3 $ wget https://github.com/mothur/mothur/archive/v1.41.3.tar.gz Unpack the downloaded file and go into mothur source folder $ tar -xvf v1.41.3.tar.gz $ cd mothur-1.41.3 Modify the Makefile $ nano Makefile #Replace the following lines 22-32 with information: (Note: username should be replaced) OPTIMIZE ?= yes USEREADLINE ?= yes USEBOOST ?= yes USEHDF5 ?= yes LOGFILE_NAME ?= no BOOST_LIBRARY_DIR ?= \"/software/boost/1.65.1/lib\" BOOST_INCLUDE_DIR ?= \"/software/boost/1.65.1/include\" HDF5_LIBRARY_DIR ?= \"/software/hdf5/1.10.1/lib\" HDF5_INCLUDE_DIR ?= \"/software/hdf5/1.10.1/include\" MOTHUR_FILES ?= \"\\\"home/username/application/bin/mothur\\\"\" VERSION = \"\\\"1.41.3\\\"\" Run make file $ make The mothur executable file will be created in the installation folder: /home/username/applications/bin . Make sure you set the correct environment PATH in ~/.bashrc file export PATH=$PATH:$HOME/applications/bin","title":"mothur"},{"location":"software/packages/mrbayes/","text":"MrBayes MrBayes is a program for Bayesian inference and model choice across a wide range of phylogenetic and evolutionary models. MrBayes uses Markov chain Monte Carlo (MCMC) methods to estimate the posterior distribution of model parameters. Installing BEAGLE Library BEAGLE library is supported to dramatic speedups for codon and amino acid models on compatible hardware (NVIDIA graphics cards); To install MrBayes in your /home directory on Palmetto, you'll need to begin by installing the BEAGLE library. Here are the steps, starting with checking-out the source code using Subversion: [galen@login001 ~]$ cd [galen@login001 ~]$ svn checkout http://beagle-lib.googlecode.com/svn/trunk/ beagle-setup [galen@login001 ~]$ cd beagle-setup [galen@login001 beagle-setup]$ ./autogen.sh [galen@login001 beagle-setup]$ ./configure --prefix=/home/galen/beagle-lib [galen@login001 beagle-setup]$ make install Once installed, I also needed to add the location of these new libraries to my LD_LIBRARY_PATH (this can be done in your ~/.bashrc file, or in your PBS job script as I have done at the bottom of this section): [galen@login001 beagle-setup]$ export LD_LIBRARY_PATH=/home/galen/beagle-lib/lib:$LD_LIBRARY_PATH Finally, verify that everything is setup properly: [galen@login001 beagle-setup]$ make check Installing MrBayes You can build a parallel version of MrBayes using OpenMPI or MPICH2. I used MPICH2 and I included the BEAGLE libraries when compiling MrBayes, so those libraries will also be needed whenever I run MrBayes. [galen@login001 src]$ module add gcc/4.4 mpich2/1.4 [galen@login001 ~]$ wget http://downloads.sourceforge.net/project/mrbayes/mrbayes/3.2.1/mrbayes-3.2.1.tar.gz [galen@login001 ~]$ tar -zxf mrbayes-3.2.1.tar.gz [galen@login001 ~]$ cd mrbayes_3.2.1/src [galen@login001 src]$ export PKG_CONFIG_PATH=/home/galen/beagle-lib/lib/pkgconfig:$PKG_CONFIG_PATH [galen@login001 src]$ autoconf [galen@login001 src]$ ./configure --enable-mpi=yes --with-beagle=/home/galen/beagle-lib [galen@login001 src]$ make Here, the mb executable was created in my /home/galen/mrbayes_3.2.1/src directory. Running MrBayes When running MrBayes in parallel, you'll need to use 1 processor core for each Markov chain, and the default number of chains is 4 (3 heated and 1 that's not heated). Below is an example MrBayes job that uses one of the example Nexus files included with my installation package /home/galen/mrbayes_3.2.1/examples/primates.nex . My MrBayes input file (mb_input) contains these commands: begin mrbayes; set autoclose=yes nowarn=yes; execute primates.nex; lset nst=6 rates=gamma; mcmc nruns=1 ngen=10000 samplefreq=10 file=primates.nex; mcmc file=primates.nex2; mcmc file=primates.nex3; end; My PBS job script for running this job in parallel looks like this: #!/bin/bash #PBS -N MrBayes #PBS -l select=1:ncpus=4:mpiprocs=4:mem=6gb:interconnect=1g,walltime=02:00:00 #PBS -j oe source /etc/profile.d/modules.sh module purge module add gcc/4.4 mpich2/1.4 export LD_LIBRARY_PATH=/home/galen/beagle-lib/lib:$LD_LIBRARY_PATH NCORES=`qstat -xf $PBS_JOBID | grep List.ncpus | sed 's/^.\\{26\\}//'` cd $PBS_O_WORKDIR mpiexec -n $NCORES /home/galen/mrbayes_3.2.1/src/mb mb_input > mb.log","title":"Mrbayes"},{"location":"software/packages/mrbayes/#mrbayes","text":"MrBayes is a program for Bayesian inference and model choice across a wide range of phylogenetic and evolutionary models. MrBayes uses Markov chain Monte Carlo (MCMC) methods to estimate the posterior distribution of model parameters.","title":"MrBayes"},{"location":"software/packages/mrbayes/#installing-beagle-library","text":"BEAGLE library is supported to dramatic speedups for codon and amino acid models on compatible hardware (NVIDIA graphics cards); To install MrBayes in your /home directory on Palmetto, you'll need to begin by installing the BEAGLE library. Here are the steps, starting with checking-out the source code using Subversion: [galen@login001 ~]$ cd [galen@login001 ~]$ svn checkout http://beagle-lib.googlecode.com/svn/trunk/ beagle-setup [galen@login001 ~]$ cd beagle-setup [galen@login001 beagle-setup]$ ./autogen.sh [galen@login001 beagle-setup]$ ./configure --prefix=/home/galen/beagle-lib [galen@login001 beagle-setup]$ make install Once installed, I also needed to add the location of these new libraries to my LD_LIBRARY_PATH (this can be done in your ~/.bashrc file, or in your PBS job script as I have done at the bottom of this section): [galen@login001 beagle-setup]$ export LD_LIBRARY_PATH=/home/galen/beagle-lib/lib:$LD_LIBRARY_PATH Finally, verify that everything is setup properly: [galen@login001 beagle-setup]$ make check","title":"Installing BEAGLE Library"},{"location":"software/packages/mrbayes/#installing-mrbayes","text":"You can build a parallel version of MrBayes using OpenMPI or MPICH2. I used MPICH2 and I included the BEAGLE libraries when compiling MrBayes, so those libraries will also be needed whenever I run MrBayes. [galen@login001 src]$ module add gcc/4.4 mpich2/1.4 [galen@login001 ~]$ wget http://downloads.sourceforge.net/project/mrbayes/mrbayes/3.2.1/mrbayes-3.2.1.tar.gz [galen@login001 ~]$ tar -zxf mrbayes-3.2.1.tar.gz [galen@login001 ~]$ cd mrbayes_3.2.1/src [galen@login001 src]$ export PKG_CONFIG_PATH=/home/galen/beagle-lib/lib/pkgconfig:$PKG_CONFIG_PATH [galen@login001 src]$ autoconf [galen@login001 src]$ ./configure --enable-mpi=yes --with-beagle=/home/galen/beagle-lib [galen@login001 src]$ make Here, the mb executable was created in my /home/galen/mrbayes_3.2.1/src directory.","title":"Installing MrBayes"},{"location":"software/packages/mrbayes/#running-mrbayes","text":"When running MrBayes in parallel, you'll need to use 1 processor core for each Markov chain, and the default number of chains is 4 (3 heated and 1 that's not heated). Below is an example MrBayes job that uses one of the example Nexus files included with my installation package /home/galen/mrbayes_3.2.1/examples/primates.nex . My MrBayes input file (mb_input) contains these commands: begin mrbayes; set autoclose=yes nowarn=yes; execute primates.nex; lset nst=6 rates=gamma; mcmc nruns=1 ngen=10000 samplefreq=10 file=primates.nex; mcmc file=primates.nex2; mcmc file=primates.nex3; end; My PBS job script for running this job in parallel looks like this: #!/bin/bash #PBS -N MrBayes #PBS -l select=1:ncpus=4:mpiprocs=4:mem=6gb:interconnect=1g,walltime=02:00:00 #PBS -j oe source /etc/profile.d/modules.sh module purge module add gcc/4.4 mpich2/1.4 export LD_LIBRARY_PATH=/home/galen/beagle-lib/lib:$LD_LIBRARY_PATH NCORES=`qstat -xf $PBS_JOBID | grep List.ncpus | sed 's/^.\\{26\\}//'` cd $PBS_O_WORKDIR mpiexec -n $NCORES /home/galen/mrbayes_3.2.1/src/mb mb_input > mb.log","title":"Running MrBayes"},{"location":"software/packages/network_simulator/","text":"Network Simulator 3 or ns-3 is a discrete-event network simulator for Internet systems, targeted primarily for research and educational use. ns-3 is free software, licensed under the GNU GPLv2 license, and is publicly available for research, development, and use. In this example, we will show how to install ns-3 to your Palmetto home directory. More detail can be found here: [https://www.nsnam.org/wiki/Installation#Installation/] Installing Network Simulator 3 Request an interactive session $ qsub -I -l select=1:ncpus=16:mem=64gb:mpiprocs=16:interconnect=fdr,walltime=2:00:00 Load necessary module. $ module add anaconda3/5.0.1 gcc/6.3.0 Qt/5.9.2 sqlite/3.21.0 Download the latest version of ns-3 (currently 3.29) to your home installed directory: (e.g: ~/source/) As this is The first time you build the ns-3 project you should build using the allinone environment. This will get the project configured for you in the most commonly useful way. cd /source/ $ wget https://www.nsnam.org/releases/ns-allinone-3.29.tar.bz2 Extract the zip file and go to install folder: $ tar -xvf ns-allinone-3.29.tar.bz2 $ cd ns-allinone-3.29 Install the ns-3: $ ./build.py --enable-examples --enable-tests -- --python=/software/anaconda3/5.0.1/bin/python","title":"Network Simulator 3"},{"location":"software/packages/network_simulator/#installing-network-simulator-3","text":"Request an interactive session $ qsub -I -l select=1:ncpus=16:mem=64gb:mpiprocs=16:interconnect=fdr,walltime=2:00:00 Load necessary module. $ module add anaconda3/5.0.1 gcc/6.3.0 Qt/5.9.2 sqlite/3.21.0 Download the latest version of ns-3 (currently 3.29) to your home installed directory: (e.g: ~/source/) As this is The first time you build the ns-3 project you should build using the allinone environment. This will get the project configured for you in the most commonly useful way. cd /source/ $ wget https://www.nsnam.org/releases/ns-allinone-3.29.tar.bz2 Extract the zip file and go to install folder: $ tar -xvf ns-allinone-3.29.tar.bz2 $ cd ns-allinone-3.29 Install the ns-3: $ ./build.py --enable-examples --enable-tests -- --python=/software/anaconda3/5.0.1/bin/python","title":"Installing Network Simulator 3"},{"location":"software/packages/openbabel/","text":"This page instructs how to install Open Babel software to Palmetto The code and the issue tracker can be found here Request an interactive session. For example: $ qsub -I -l select=1:ncpus=8:mem=24gb:interconnect=fdr,walltime=3:00:00 Load the required modules $ module load cmake/3.6.1 gcc/4.8.1 Install Eigen (latest version 3.3.7) $ wget http://bitbucket.org/eigen/eigen/get/3.3.7.tar.gz $ tar -xvf 3.3.7.tar.gz $ cd eigen-eigen-323c052e1731 $ mkdir build $ cd build $ cmake -DCMAKE_INSTALL_PREFIX=$HOME/applications/ .. $ make install Install wxWidgets (latest version 3.1.2) $ wget https://github.com/wxWidgets/wxWidgets/releases/download/v3.1.2/wxWidgets-3.1.2.tar.bz2 $ tar -xvf wxWidgets-3.1.2.tar.bz2 $ cd wxWidgets-3.1.2 $ configure --prefix=$HOME/applications $ make $ make install Download Open Babel from source . Here we download the latest version 2.4.1. Upload the package openbabel-2.4.1.tar.gz to your Palmetto storage Unpack the downloaded file and go into openbabel source folder $ tar -xvf openbabel-2.4.1.tar.gz $ cd openbabel-2.4.1 $ mkdir build $ cd build $ cmake ../openbabel-2.4.1 -DCMAKE_INSTALL_PREFIX=$HOME/applications/ -DLIB_INSTALL_DIR=$HOME/applications/lib -DEIGEN3_INCLUDE_DIR=$HOME/applications/include/eigen3 -DBUILD_GUI=ON .. $ make -j4 $ make test #make sure that all tests should be passed $ make install The obabel executable file will be created in the installation folder: /home/username/applications/bin . Make sure you set the correct environment PATH in ~/.bashrc file export PATH=$PATH:$HOME/applications/bin","title":"Open Babel"},{"location":"software/packages/overview/","text":"Overview Modules A large number of popular software packages are installed on Palmetto and can be used without any setup or configuration. These include: Compilers (such as gcc , Intel, and PGI) Libraries (such as OpenMPI, HDF5, Boost) Programming languages (such as Python, MATLAB, R) Scientific applications (such as LAMMPS, Paraview, ANSYS) Others (e.g., Git, PostgreSQL, Singularity) These packages are available as modules on Palmetto. The following commands can be used to inspect, activate and deactivate modules: Command Purpose module avail List all packages available (on current system) module add package/version Add a package to your current shell environment module list List packages you have loaded module rm package/version Remove a currently loaded package module purge Remove all currently loaded packages See the Quick Start Guide for more details about modules. Licensed software Many site-licensed software packages are available on Palmetto cluster (e.g., MATLAB, ANSYS, COMSOL, etc.,). There are limitations on the number of jobs that can run using these packages. See this section of the User's Guide on how to check license usage. Individual-owned or group-owned licensed software can also be run on Palmetto. Software with graphical applications See this section of the User's Guide on how to use software with graphical user interface (GUI). Installing your own software See this section of the User's Guide on how to check license usage.","title":"Overview"},{"location":"software/packages/overview/#overview","text":"","title":"Overview"},{"location":"software/packages/overview/#modules","text":"A large number of popular software packages are installed on Palmetto and can be used without any setup or configuration. These include: Compilers (such as gcc , Intel, and PGI) Libraries (such as OpenMPI, HDF5, Boost) Programming languages (such as Python, MATLAB, R) Scientific applications (such as LAMMPS, Paraview, ANSYS) Others (e.g., Git, PostgreSQL, Singularity) These packages are available as modules on Palmetto. The following commands can be used to inspect, activate and deactivate modules: Command Purpose module avail List all packages available (on current system) module add package/version Add a package to your current shell environment module list List packages you have loaded module rm package/version Remove a currently loaded package module purge Remove all currently loaded packages See the Quick Start Guide for more details about modules.","title":"Modules"},{"location":"software/packages/overview/#licensed-software","text":"Many site-licensed software packages are available on Palmetto cluster (e.g., MATLAB, ANSYS, COMSOL, etc.,). There are limitations on the number of jobs that can run using these packages. See this section of the User's Guide on how to check license usage. Individual-owned or group-owned licensed software can also be run on Palmetto.","title":"Licensed software"},{"location":"software/packages/overview/#software-with-graphical-applications","text":"See this section of the User's Guide on how to use software with graphical user interface (GUI).","title":"Software with graphical applications"},{"location":"software/packages/overview/#installing-your-own-software","text":"See this section of the User's Guide on how to check license usage.","title":"Installing your own software"},{"location":"software/packages/paraview/","text":"Paraview Using Paraview+GPUs to visualize very large datasets Paraview can also use multiple GPUs on Palmetto cluster to visualize very large datasets. For this, Paraview must be run in client-server mode. The \"client\" is your local machine on which Paraview must be installed, and the \"server\" is the Palmetto cluster on which the computations/rendering is done. The version of Paraview on the client needs to match exactly the version of Paraview on the server. The client must be running Linux. You can obtain the source code used for installation of Paraview 5.0.1 on Palmetto from /software/paraview/ParaView-v5.0.1-source.tar.gz . Copy this file to the client, extract it and compile Paraview. Compilation instructions can be found in the Paraview documentation . You will need to run the Paraview server on Palmetto cluster. First, log-in with X11 tunneling enabled, and request an interactive session: $ qsub -I -X -l select=4:ncpus=2:mpiprocs=2:ngpus=2:mem=32gb,walltime=1:00:00 In the above example, we request 4 nodes with 2 GPUs each. Next, launch the Paraview server: $ module add paraview/5.0 $ export DISPLAY=:0 $ mpiexec -n 8 pvserver -display :0 The server will be serving on a specific port number (like 11111) on this node. Note this number down. Next, you will need to set up \"port-forwarding\" from the lead node (the node your interactive session is running one) to your local machine. This can be done by opening a terminal running on the local machine, and typing the following: $ ssh -L 11111:nodeXYZ:11111 username@login.palmetto.clemson.edu Once port-forwarding is set up, you can launch Paraview on your local machine,","title":"Paraview"},{"location":"software/packages/paraview/#paraview","text":"","title":"Paraview"},{"location":"software/packages/paraview/#using-paraviewgpus-to-visualize-very-large-datasets","text":"Paraview can also use multiple GPUs on Palmetto cluster to visualize very large datasets. For this, Paraview must be run in client-server mode. The \"client\" is your local machine on which Paraview must be installed, and the \"server\" is the Palmetto cluster on which the computations/rendering is done. The version of Paraview on the client needs to match exactly the version of Paraview on the server. The client must be running Linux. You can obtain the source code used for installation of Paraview 5.0.1 on Palmetto from /software/paraview/ParaView-v5.0.1-source.tar.gz . Copy this file to the client, extract it and compile Paraview. Compilation instructions can be found in the Paraview documentation . You will need to run the Paraview server on Palmetto cluster. First, log-in with X11 tunneling enabled, and request an interactive session: $ qsub -I -X -l select=4:ncpus=2:mpiprocs=2:ngpus=2:mem=32gb,walltime=1:00:00 In the above example, we request 4 nodes with 2 GPUs each. Next, launch the Paraview server: $ module add paraview/5.0 $ export DISPLAY=:0 $ mpiexec -n 8 pvserver -display :0 The server will be serving on a specific port number (like 11111) on this node. Note this number down. Next, you will need to set up \"port-forwarding\" from the lead node (the node your interactive session is running one) to your local machine. This can be done by opening a terminal running on the local machine, and typing the following: $ ssh -L 11111:nodeXYZ:11111 username@login.palmetto.clemson.edu Once port-forwarding is set up, you can launch Paraview on your local machine,","title":"Using Paraview+GPUs to visualize very large datasets"},{"location":"software/packages/rclone/","text":"rclone rclone is a command-line program that can be used to sync files and folders to and from cloud services such as Google Drive, Amazon S3, Dropbox, and many others . In this example, we will show how to use rclone to sync files to a Google Drive account, but the official documentation has specific instructions for other services. Setting up rclone for use with Google Drive on Palmetto To use rclone with any of the above cloud storage services, you must perform a one-time configuration. You can configure rclone to work with as many services as you like. For the one-time configuration, you will need to log-in with tunneling enabled . Once logged-in, ask for an interactive job: $ qsub -I -X Once the job starts, load the rclone module: $ module add rclone/1.23 After rclone is loaded, you must set up a \"remote\". In this case, we will configure a remote for Google Drive. You can create and manage a separate remote for each cloud storage service you want to use. Start by entering the following command: $ rclone config n) New remote q) Quit config n/q> Hit n then Enter to create a new remote host name> Provide any name for this remote host. For example: gmaildrive What type of source is it? Choose a number from below 1) amazon cloud drive 2) drive 3) dropbox 4) google cloud storage 5) local 6) s3 7) swift type> Provide any number for the remote source. For example choose number 2 for goolge drive. Google Application Client Id - leave blank normally. client_id> # Enter to leave blank Google Application Client Secret - leave blank normally. client_secret> # Enter to leave blank Remote config Use auto config? * Say Y if not sure * Say N if you are working on a remote or headless machine or Y didn't work y) Yes n) No y/n> Use y if you are not sure If your browser doesn't open automatically go to the following link: http://127.0.0.1:53682/auth Log in and authorize rclone for access Waiting for code... This will open the Firefox web browser, allowing you to log-in to your Google account. Enter your username and password then accept to let rclone access your Goolge drive. Once this is done, the browser will ask you to go back to rclone to continue. Got code -------------------- [gmaildrive] client_id = client_secret = token = {\"access_token\":\"xyz\",\"token_type\":\"Bearer\",\"refresh_token\":\"xyz\",\"expiry\":\"yyyy-mm-ddThh:mm:ss\"} -------------------- y) Yes this is OK e) Edit this remote d) Delete this remote y/e/d> Select y to finish configure this remote host. The gmaildrive host will then be created. Current remotes: Name Type ==== ==== gmaildrive drive e) Edit existing remote n) New remote d) Delete remote q) Quit config e/n/d/q> After this, you can quit the config using q , kill the job and exit this ssh session: $ exit Using rclone Whenever transfering files (using rclone or otherwise), login to the transfer node xfer01-ext.palmetto.clemson.edu . In MobaXterm, create a new ssh session with xfer01-ext.palmetto.clemson.edu as the Remote host In MacOS, open new terminal and ssh user@xfer01-ext.palmetto.clemson.edu Once logged-in, load the rclone module: $ module add rclone/1.23 You can check the content of the remote host gmaildrive : $ rclone ls gmaildrive: $ rclone lsd gmaildrive: You can use rclone to (for example) copy a file from Palmetto to any folder in your Google Drive: $ rclone copy /path/to/file/on/palmetto gmaildrive:/path/to/folder/on/drive Or if you want to copy to a specific destination on Google Drive back to Palmetto: $ rclone copy gmaildrive:/path/to/folder/on/drive /path/to/file/on/palmetto Additional rclone commands can be found here .","title":"Rclone"},{"location":"software/packages/rclone/#rclone","text":"rclone is a command-line program that can be used to sync files and folders to and from cloud services such as Google Drive, Amazon S3, Dropbox, and many others . In this example, we will show how to use rclone to sync files to a Google Drive account, but the official documentation has specific instructions for other services.","title":"rclone"},{"location":"software/packages/rclone/#setting-up-rclone-for-use-with-google-drive-on-palmetto","text":"To use rclone with any of the above cloud storage services, you must perform a one-time configuration. You can configure rclone to work with as many services as you like. For the one-time configuration, you will need to log-in with tunneling enabled . Once logged-in, ask for an interactive job: $ qsub -I -X Once the job starts, load the rclone module: $ module add rclone/1.23 After rclone is loaded, you must set up a \"remote\". In this case, we will configure a remote for Google Drive. You can create and manage a separate remote for each cloud storage service you want to use. Start by entering the following command: $ rclone config n) New remote q) Quit config n/q> Hit n then Enter to create a new remote host name> Provide any name for this remote host. For example: gmaildrive What type of source is it? Choose a number from below 1) amazon cloud drive 2) drive 3) dropbox 4) google cloud storage 5) local 6) s3 7) swift type> Provide any number for the remote source. For example choose number 2 for goolge drive. Google Application Client Id - leave blank normally. client_id> # Enter to leave blank Google Application Client Secret - leave blank normally. client_secret> # Enter to leave blank Remote config Use auto config? * Say Y if not sure * Say N if you are working on a remote or headless machine or Y didn't work y) Yes n) No y/n> Use y if you are not sure If your browser doesn't open automatically go to the following link: http://127.0.0.1:53682/auth Log in and authorize rclone for access Waiting for code... This will open the Firefox web browser, allowing you to log-in to your Google account. Enter your username and password then accept to let rclone access your Goolge drive. Once this is done, the browser will ask you to go back to rclone to continue. Got code -------------------- [gmaildrive] client_id = client_secret = token = {\"access_token\":\"xyz\",\"token_type\":\"Bearer\",\"refresh_token\":\"xyz\",\"expiry\":\"yyyy-mm-ddThh:mm:ss\"} -------------------- y) Yes this is OK e) Edit this remote d) Delete this remote y/e/d> Select y to finish configure this remote host. The gmaildrive host will then be created. Current remotes: Name Type ==== ==== gmaildrive drive e) Edit existing remote n) New remote d) Delete remote q) Quit config e/n/d/q> After this, you can quit the config using q , kill the job and exit this ssh session: $ exit","title":"Setting up rclone for use with Google Drive on Palmetto"},{"location":"software/packages/rclone/#using-rclone","text":"Whenever transfering files (using rclone or otherwise), login to the transfer node xfer01-ext.palmetto.clemson.edu . In MobaXterm, create a new ssh session with xfer01-ext.palmetto.clemson.edu as the Remote host In MacOS, open new terminal and ssh user@xfer01-ext.palmetto.clemson.edu Once logged-in, load the rclone module: $ module add rclone/1.23 You can check the content of the remote host gmaildrive : $ rclone ls gmaildrive: $ rclone lsd gmaildrive: You can use rclone to (for example) copy a file from Palmetto to any folder in your Google Drive: $ rclone copy /path/to/file/on/palmetto gmaildrive:/path/to/folder/on/drive Or if you want to copy to a specific destination on Google Drive back to Palmetto: $ rclone copy gmaildrive:/path/to/folder/on/drive /path/to/file/on/palmetto Additional rclone commands can be found here .","title":"Using rclone"},{"location":"software/packages/singularity/","text":"Singularity Singularity is a tool for creating and running containers on HPC systems, similar to Docker . For further information on Singularity, and on downloading, building and running containers with Singularity, please refer to the Singularity documentation . This page provides information about singularity specific to the Palmetto cluster. Running Singularity Singularity is installed on all of the Palmetto compute nodes and the Palmetto LoginVMs, but it IS NOT present on the login.palmetto.clemson.edu node. To run singularity, you may simply run singularity or more specifically /bin/singularity . e.g. $ singularity --version singularity version 3.5.3-1.el7 An important change for existing singularity users Formerly, Palmetto administrators had installed singularity as a \"software module\" on Palmetto, but that is no longer the case. If your job scripts have any statements that use the singularity module, then those statements will need to be completely removed; otherwise, your job script may error. Remove any statements from your job scripts that resemble the following lines: module <some_command> singularity Where to download containers Containers can be downloaded from DockerHub DockerHub contains containers for various software packages, and Singularity is compatible with Docker images . SingularityHub The NVIDIA GPU Cloud for GPU-optimized images. Many individual projects contain specific instructions for installation via Docker and/or Singularity, and may host pre-built images in other locations. Example: Running OpenFOAM using Singularity As an example, we consider installing and running the OpenFOAM CFD solver using Singularity. OpenFOAM can be quite difficult to install manually, but singularity makes it very easy. This example shows how to use singularity interactively, but singularity containers can be run in batch jobs as well. Start by requesting an interactive job. NOTE: Singularity can only be run on the compute nodes and Palmetto Login VMs: $ qsub -I -l select=1:ngpus=2:ncpus=16:mpiprocs=16:mem=120gb,walltime=5:00:00 We recommend that all users store built singularity images in their /home directories. Singularity images can be quite large, so be sure to delete unused or old images: $ mkdir ~/singularity-images $ cd ~/singularity-images Next, we download the singularity image for OpenFOAM from DockerHub. This takes a few seconds to complete: $ singularity pull docker://openfoam/openfoam6-paraview54 Once the image is downloaded, we are ready to run OpenFOAM. We use singularity shell to start a container, and run a shell in the container. The -B option is used to \"bind\" the /scratch2/$USER directory to a directory named /scratch in the container. We also the --pwd option to specify the working directory in the running container (in this case /scratch ). This is always recommended. Typically, the working directory may be the $TMPDIR directory or one of the scratch directories. $ singularity shell -B /scratch2/atrikut:/scratch --pwd /scratch openfoam6-paraview54.simg Before running OpenFOAM commands, we need to source a few environment variables (this step is specific to OpenFOAM): $ source /opt/openfoam6/etc/bashrc Now, we are ready to run a simple example using OpenFOAM: $ cp -r $FOAM_TUTORIALS/incompressible/simpleFoam/pitzDaily . $ cd pitzDaily $ blockMesh $ simpleFoam The simulation takes a few seconds to complete, and should finish with the following output: smoothSolver: Solving for Ux, Initial residual = 0.00012056, Final residual = 7.8056e-06, No Iterations 6 smoothSolver: Solving for Uy, Initial residual = 0.000959834, Final residual = 6.43909e-05, No Iterations 6 GAMG: Solving for p, Initial residual = 0.00191644, Final residual = 0.000161493, No Iterations 3 time step continuity errors : sum local = 0.00681813, global = -0.000731564, cumulative = 0.941842 smoothSolver: Solving for epsilon, Initial residual = 0.000137225, Final residual = 8.98917e-06, No Iterations 3 smoothSolver: Solving for k, Initial residual = 0.000215144, Final residual = 1.30281e-05, No Iterations 4 ExecutionTime = 10.77 s ClockTime = 11 s SIMPLE solution converged in 288 iterations streamLine streamlines write: seeded 10 particles Tracks:10 Total samples:11980 Writing data to \"/scratch/pitzDaily/postProcessing/sets/streamlines/288\" End We are now ready to exit the container: $ exit Because the directory /scratch was bound to /scratch2/$USER , the simulation output is available in the directory /scratch2/$USER/pitzDaily/postProcessing/ : $ ls /scratch2/$USER/pitzDaily/postProcessing/ sets GPU-enabled software using Singularity containers (NVIDIA GPU Cloud) Palmetto also supports use of images provided by the NVIDIA GPU Cloud (NGC) . The provides GPU-accelerated HPC and deep learning containers for scientific computing. NVIDIA tests HPC container compatibility with the Singularity runtime through a rigorous QA process. Pulling NGC images Singularity images may be pulled directly from the Palmetto GPU compute nodes, an interactive job is most convenient for this. Singularity uses multiple CPU cores when building the image and so it is recommended that a minimum of 4 CPU cores are reserved. For instance to reserve 4 CPU cores, 2 NVIDIA Pascal GPUs, for 20 minutes the following could be used: $ qsub -I -lselect=1:ncpus=4:mem=2gb:ngpus=2:gpu_model=p100,walltime=00:20:00 Wait for the interactive job to give you control over the shell. Before pulling an NGC image, authentication credentials must be set. This is most easily accomplished by setting the following variables in the build environment. $ export SINGULARITY_DOCKER_USERNAME='$oauthtoken' $ export SINGULARITY_DOCKER_PASSWORD=<NVIDIA NGC API key> More information describing how to obtain and use your NVIDIA NGC API key can be found here . Once credentials are set in the environment, we\u2019re ready to pull and convert the NGC image to a local Singularity image file. The general form of this command for NGC HPC images is: $ singularity build <local_image> docker://nvcr.io/<registry>/<app:tag> This singularity build command will download the app:tag NGC Docker image, convert it to Singularity format, and save it to the local file named local_image. For example to pull the namd NGC container tagged with version 2.12-171025 to a local file named namd.simg we can run: $ singularity build ~/namd.simg docker://nvcr.io/hpc/namd:2.12-171025 After this command has finished we'll have a Singularity image file, namd.simg : Running NGC containers Running NGC containers on Palmetto presents few differences from the run instructions provided on NGC for each application. Application-specific information may vary so it is recommended that you follow the container specific documentation before running with Singularity. If the container documentation does not include Singularity information, then the container has not yet been tested under Singularity. As all NGC containers are optimized for NVIDIA GPU acceleration we will always want to add the --nv flag to enable NVIDIA GPU support within the container. The Singularity command below represents the standard form of the Singularity command used on the Palmetto cluster. It will mount the present working directory on the host to /host_pwd in the container process and set the present working directory of the container process to /host_pwd This means that when our process starts it will be effectively running in the host directory the singularity command was launched from. $ singularity exec --nv -B $(pwd):/host_pwd --pwd /host_pwd <image.simg> <cmd>","title":"Singularity"},{"location":"software/packages/singularity/#singularity","text":"Singularity is a tool for creating and running containers on HPC systems, similar to Docker . For further information on Singularity, and on downloading, building and running containers with Singularity, please refer to the Singularity documentation . This page provides information about singularity specific to the Palmetto cluster.","title":"Singularity"},{"location":"software/packages/singularity/#running-singularity","text":"Singularity is installed on all of the Palmetto compute nodes and the Palmetto LoginVMs, but it IS NOT present on the login.palmetto.clemson.edu node. To run singularity, you may simply run singularity or more specifically /bin/singularity . e.g. $ singularity --version singularity version 3.5.3-1.el7","title":"Running Singularity"},{"location":"software/packages/singularity/#an-important-change-for-existing-singularity-users","text":"Formerly, Palmetto administrators had installed singularity as a \"software module\" on Palmetto, but that is no longer the case. If your job scripts have any statements that use the singularity module, then those statements will need to be completely removed; otherwise, your job script may error. Remove any statements from your job scripts that resemble the following lines: module <some_command> singularity","title":"An important change for existing singularity users"},{"location":"software/packages/singularity/#where-to-download-containers","text":"Containers can be downloaded from DockerHub DockerHub contains containers for various software packages, and Singularity is compatible with Docker images . SingularityHub The NVIDIA GPU Cloud for GPU-optimized images. Many individual projects contain specific instructions for installation via Docker and/or Singularity, and may host pre-built images in other locations.","title":"Where to download containers"},{"location":"software/packages/singularity/#example-running-openfoam-using-singularity","text":"As an example, we consider installing and running the OpenFOAM CFD solver using Singularity. OpenFOAM can be quite difficult to install manually, but singularity makes it very easy. This example shows how to use singularity interactively, but singularity containers can be run in batch jobs as well. Start by requesting an interactive job. NOTE: Singularity can only be run on the compute nodes and Palmetto Login VMs: $ qsub -I -l select=1:ngpus=2:ncpus=16:mpiprocs=16:mem=120gb,walltime=5:00:00 We recommend that all users store built singularity images in their /home directories. Singularity images can be quite large, so be sure to delete unused or old images: $ mkdir ~/singularity-images $ cd ~/singularity-images Next, we download the singularity image for OpenFOAM from DockerHub. This takes a few seconds to complete: $ singularity pull docker://openfoam/openfoam6-paraview54 Once the image is downloaded, we are ready to run OpenFOAM. We use singularity shell to start a container, and run a shell in the container. The -B option is used to \"bind\" the /scratch2/$USER directory to a directory named /scratch in the container. We also the --pwd option to specify the working directory in the running container (in this case /scratch ). This is always recommended. Typically, the working directory may be the $TMPDIR directory or one of the scratch directories. $ singularity shell -B /scratch2/atrikut:/scratch --pwd /scratch openfoam6-paraview54.simg Before running OpenFOAM commands, we need to source a few environment variables (this step is specific to OpenFOAM): $ source /opt/openfoam6/etc/bashrc Now, we are ready to run a simple example using OpenFOAM: $ cp -r $FOAM_TUTORIALS/incompressible/simpleFoam/pitzDaily . $ cd pitzDaily $ blockMesh $ simpleFoam The simulation takes a few seconds to complete, and should finish with the following output: smoothSolver: Solving for Ux, Initial residual = 0.00012056, Final residual = 7.8056e-06, No Iterations 6 smoothSolver: Solving for Uy, Initial residual = 0.000959834, Final residual = 6.43909e-05, No Iterations 6 GAMG: Solving for p, Initial residual = 0.00191644, Final residual = 0.000161493, No Iterations 3 time step continuity errors : sum local = 0.00681813, global = -0.000731564, cumulative = 0.941842 smoothSolver: Solving for epsilon, Initial residual = 0.000137225, Final residual = 8.98917e-06, No Iterations 3 smoothSolver: Solving for k, Initial residual = 0.000215144, Final residual = 1.30281e-05, No Iterations 4 ExecutionTime = 10.77 s ClockTime = 11 s SIMPLE solution converged in 288 iterations streamLine streamlines write: seeded 10 particles Tracks:10 Total samples:11980 Writing data to \"/scratch/pitzDaily/postProcessing/sets/streamlines/288\" End We are now ready to exit the container: $ exit Because the directory /scratch was bound to /scratch2/$USER , the simulation output is available in the directory /scratch2/$USER/pitzDaily/postProcessing/ : $ ls /scratch2/$USER/pitzDaily/postProcessing/ sets","title":"Example: Running OpenFOAM using Singularity"},{"location":"software/packages/singularity/#gpu-enabled-software-using-singularity-containers-nvidia-gpu-cloud","text":"Palmetto also supports use of images provided by the NVIDIA GPU Cloud (NGC) . The provides GPU-accelerated HPC and deep learning containers for scientific computing. NVIDIA tests HPC container compatibility with the Singularity runtime through a rigorous QA process.","title":"GPU-enabled software using Singularity containers (NVIDIA GPU Cloud)"},{"location":"software/packages/singularity/#pulling-ngc-images","text":"Singularity images may be pulled directly from the Palmetto GPU compute nodes, an interactive job is most convenient for this. Singularity uses multiple CPU cores when building the image and so it is recommended that a minimum of 4 CPU cores are reserved. For instance to reserve 4 CPU cores, 2 NVIDIA Pascal GPUs, for 20 minutes the following could be used: $ qsub -I -lselect=1:ncpus=4:mem=2gb:ngpus=2:gpu_model=p100,walltime=00:20:00 Wait for the interactive job to give you control over the shell. Before pulling an NGC image, authentication credentials must be set. This is most easily accomplished by setting the following variables in the build environment. $ export SINGULARITY_DOCKER_USERNAME='$oauthtoken' $ export SINGULARITY_DOCKER_PASSWORD=<NVIDIA NGC API key> More information describing how to obtain and use your NVIDIA NGC API key can be found here . Once credentials are set in the environment, we\u2019re ready to pull and convert the NGC image to a local Singularity image file. The general form of this command for NGC HPC images is: $ singularity build <local_image> docker://nvcr.io/<registry>/<app:tag> This singularity build command will download the app:tag NGC Docker image, convert it to Singularity format, and save it to the local file named local_image. For example to pull the namd NGC container tagged with version 2.12-171025 to a local file named namd.simg we can run: $ singularity build ~/namd.simg docker://nvcr.io/hpc/namd:2.12-171025 After this command has finished we'll have a Singularity image file, namd.simg :","title":"Pulling NGC images"},{"location":"software/packages/singularity/#running-ngc-containers","text":"Running NGC containers on Palmetto presents few differences from the run instructions provided on NGC for each application. Application-specific information may vary so it is recommended that you follow the container specific documentation before running with Singularity. If the container documentation does not include Singularity information, then the container has not yet been tested under Singularity. As all NGC containers are optimized for NVIDIA GPU acceleration we will always want to add the --nv flag to enable NVIDIA GPU support within the container. The Singularity command below represents the standard form of the Singularity command used on the Palmetto cluster. It will mount the present working directory on the host to /host_pwd in the container process and set the present working directory of the container process to /host_pwd This means that when our process starts it will be effectively running in the host directory the singularity command was launched from. $ singularity exec --nv -B $(pwd):/host_pwd --pwd /host_pwd <image.simg> <cmd>","title":"Running NGC containers"},{"location":"software/packages/sumo/","text":"SUMO is an open source, highly portable, microscopic and continuous road traffic simulation package designed to handle large road networks. It allows for intermodal simulation including pedestrians and comes with a large set of tools for scenario creation. The code and the issue tracker can be found here This page explains how to install the SUMO package on the cluster. The procedure is contributed by Sakib Mahmud Khan - PhD Candidate at Transportation Cyber-Physical Systems Lab -Glenn Department of Civil Engineering. Installing submodules Request an interactive session. For example: $ qsub -I -l select=1:ncpus=6:mem=24gb:interconnect=fdr,walltime=3:00:00 Load the required modules $ module load gcc/7.0.1 cmake/3.6.1 Install FOX , here we used the version 1.6.57 $ cd ~/source $ wget http://fox-toolkit.org/ftp/fox-1.6.57.tar.gz $ cd fox-1.6.57 $ ./configure --prefix=$HOME/applications $ make $ make install Install PROJ , version 4.9.3 $ cd ~/source $ wget https://download.osgeo.org/proj/proj-4.9.3.tar.gz $ cd proj-4.9.3 $ ./configure --prefix=$HOME/applications $ make $ make install Install GDAL version 2.2.0 $ cd ~/source $ wget https://download.osgeo.org/gdal/2.2.0/gdal-2.2.0.tar.gz $ cd gdal-2.2.0 $ ./configure --prefix=$HOME/applications $ make $ make install Install XCERCES $ cd ~/source $ wget https://archive.apache.org/dist/xerces/c/3/sources/xerces-c-3.1.4.tar.gz $ cd xerces-c-3.1.4 $ ./configure --prefix=$HOME/applications $ make $ make install Installing SUMO After installing all dependencies, start installing SUMO . Download sumo from source then upload the code to installed folder: $ cd ~/source $ tar -xvf sumo-src-1.1.0.tar.gz $ cd sumo-src-1.1.0 $ ./configure --with-fox-config=$HOME/applications/bin/fox-config --with-proj-gdal=$HOME/applications/ --with-xerces=$HOME/applications $ make $ make install","title":"Simulation of Urban Mobility"},{"location":"software/packages/sumo/#installing-submodules","text":"Request an interactive session. For example: $ qsub -I -l select=1:ncpus=6:mem=24gb:interconnect=fdr,walltime=3:00:00 Load the required modules $ module load gcc/7.0.1 cmake/3.6.1 Install FOX , here we used the version 1.6.57 $ cd ~/source $ wget http://fox-toolkit.org/ftp/fox-1.6.57.tar.gz $ cd fox-1.6.57 $ ./configure --prefix=$HOME/applications $ make $ make install Install PROJ , version 4.9.3 $ cd ~/source $ wget https://download.osgeo.org/proj/proj-4.9.3.tar.gz $ cd proj-4.9.3 $ ./configure --prefix=$HOME/applications $ make $ make install Install GDAL version 2.2.0 $ cd ~/source $ wget https://download.osgeo.org/gdal/2.2.0/gdal-2.2.0.tar.gz $ cd gdal-2.2.0 $ ./configure --prefix=$HOME/applications $ make $ make install Install XCERCES $ cd ~/source $ wget https://archive.apache.org/dist/xerces/c/3/sources/xerces-c-3.1.4.tar.gz $ cd xerces-c-3.1.4 $ ./configure --prefix=$HOME/applications $ make $ make install","title":"Installing submodules"},{"location":"software/packages/sumo/#installing-sumo","text":"After installing all dependencies, start installing SUMO . Download sumo from source then upload the code to installed folder: $ cd ~/source $ tar -xvf sumo-src-1.1.0.tar.gz $ cd sumo-src-1.1.0 $ ./configure --with-fox-config=$HOME/applications/bin/fox-config --with-proj-gdal=$HOME/applications/ --with-xerces=$HOME/applications $ make $ make install","title":"Installing SUMO"},{"location":"software/packages/tassel/","text":"TASSEL (Trait Analysis by Association, Evolution, and Linkage) is a software package used to evaluate genotype and trait associations with the tools of population and quantitative genetics. TASSEL can handle datasets commonly encountered in the plant community, e.g. replicated trials, inbred lines, and complex structured pedigrees. Downloading/installing TASSEL 3 Verify your Java version to ensure that you're downloading the correct version of TASSEL (TASSEL 3 is used with Java version 6, TASSEL 4 is used with Java version 7): [galen@login001 ~]$ java -version java version \"1.6.0_11\" Java(TM) SE Runtime Environment (build 1.6.0_11-b03) Java HotSpot(TM) 64-Bit Server VM (build 11.0-b16, mixed mode) Download the latest build of TASSEL using Git: [galen@login001 ~]$ git --version git version 1.7.1 [galen@login001 ~]$ git clone git://git.code.sf.net/p/tassel/tassel3-standalone Initialized empty Git repository in /home/galen/tassel/tassel3-standalone/.git/ remote: Counting objects: 293, done. remote: Compressing objects: 100% (290/290), done. remote: Total 293 (delta 114), reused 0 (delta 0) Receiving objects: 100% (293/293), 42.95 MiB | 1.86 MiB/s, done. Resolving deltas: 100% (114/114), done. Running TASSEL 3 (using the GUI) -Xms is used to set the lower bound for the total heap size, -Xmx is used to set the upper bound. An incorrectly sized Java heap can lead to OutOfMemoryError exceptions or to a reduction in the performance of the Java application. If the Java heap is smaller than the memory requirements of the application, OutOfMemoryError exceptions are generated because of Java heap exhaustion. If the Java heap is slightly larger than the requirements of the application, garbage collection runs very frequently and affects the performance of the application. Size your Java heap so that your application runs with a minimum heap usage of 40%, and a maximum heap usage of 70%. Of course, you'll have to enable X11 tunneling to display the GUI. See details presented here. [galen@login001 ~]$ qsub -I -X -l select=1:ncpus=1:mem=11gb qsub (Warning): Interactive jobs will be treated as not rerunnable qsub: waiting for job 6360074.pbs01 to start qsub: job 6360074.pbs01 ready [galen@node0133 ~]$ export PATH=/home/galen/tassel3-standalone:$PATH [galen@node0133 ~]$ cd tassel3-standalone/ [galen@node0133 tassel3-standalone]$ ./start_tassel.pl -Xms512m -Xmx10g Running TASSEL 3 (from the command line) Please review the user guide for running Tassel 3 from the command line. It's available here: http://www.maizegenetics.net/tassel/docs/TasselPipelineCLI.pdf See note about heap size settings, above. [galen@login001 ~]$ qsub -I -l select=1:ncpus=1:mem=11gb qsub (Warning): Interactive jobs will be treated as not rerunnable qsub: waiting for job 6360074.pbs01 to start qsub: job 6360074.pbs01 ready [galen@node0133 ~]$ export PATH=/home/galen/tassel3-standalone:$PATH [galen@node0133 ~]$ cd tassel3-standalone/ [galen@node0133 tassel3-standalone]$ ./run_pipeline.pl -Xms512m -Xmx10g ./run_pipeline.pl -fork1 -h my_hapmap_file.txt -ld -ldd png -o my_output_file.png -runfork1 Below is a simple example of a TASSEL 3 PBS job script: {% highlight bash %} !/bin/bash PBS -N TAS-TEST PBS -l select=1:ncpus=1:mem=11gb,walltime=12:00:00 PBS -j oe cd $PBS_O_WORKDIR export PATH=/home/galen/tassel3-standalone:$PATH ./run_pipeline.pl -Xms512m -Xmx10g ./run_pipeline.pl -fork1 \\ -h my_hapmap_file.txt -ld -ldd png -o my_output_file.png -runfork1 {% endhighlight %} Updating TASSEL 3 Updating your TASSEL 3 installation to the latest version is easy. Simply enter the TASSEL 3 installation directory and run git pull [galen@login001 ~]$ cd tassel3-standalone [galen@login001 tassel3-standalone]$ git pull Downloading/installing TASSEL 4 Verify your Java version to ensure that you're downloading the correct version of TASSEL (TASSEL 3 is used with Java version 6, TASSEL 4 is used with Java version 7). To install your own build (version) of Java, see the instructions presented here. [galen@login001 ~]$ java -version java version \"1.7.0_45\" Java(TM) SE Runtime Environment (build 1.7.0_45-b18) Java HotSpot(TM) 64-Bit Server VM (build 24.45-b08, mixed mode) Download the latest build of TASSEL using Git: [galen@login001 ~]$ git --version git version 1.7.1 [galen@login001 ~]$ git clone git://git.code.sf.net/p/tassel/tassel4-standalone Initialized empty Git repository in /home/galen/tassel4-standalone/.git/ remote: Counting objects: 361, done. remote: Compressing objects: 100% (358/358), done. remote: Total 361 (delta 176), reused 0 (delta 0) Receiving objects: 100% (361/361), 48.07 MiB | 3.83 MiB/s, done. Resolving deltas: 100% (176/176), done. Running TASSEL 4 (using the GUI) -Xms is used to set the lower bound for the total heap size, -Xmx is used to set the upper bound. An incorrectly sized Java heap can lead to OutOfMemoryError exceptions or to a reduction in the performance of the Java application. If the Java heap is smaller than the memory requirements of the application, OutOfMemoryError exceptions are generated because of Java heap exhaustion. If the Java heap is slightly larger than the requirements of the application, garbage collection runs very frequently and affects the performance of the application. Size your Java heap so that your application runs with a minimum heap usage of 40%, and a maximum heap usage of 70%. Of course, you'll have to enable X11 tunneling to display the GUI. See details presented here. [galen@login001 ~]$ qsub -I -X -l select=1:ncpus=1:mem=11gb qsub (Warning): Interactive jobs will be treated as not rerunnable qsub: waiting for job 6360074.pbs01 to start qsub: job 6360074.pbs01 ready [galen@node0133 ~]$ export PATH=/home/galen/tassel4-standalone:$PATH [galen@node0133 ~]$ cd tassel4-standalone/ [galen@node0133 tassel4-standalone]$ ./start_tassel.pl -Xms512m -Xmx10g Running TASSEL 4 (from the command line) Please review the user guide for running Tassel 4 from the command line. It's available here: http://www.maizegenetics.net/tassel/docs/TasselPipelineCLI.pdf See note about heap size settings, above. [galen@login001 ~]$ qsub -I -l select=1:ncpus=1:mem=11gb qsub (Warning): Interactive jobs will be treated as not rerunnable qsub: waiting for job 6360074.pbs01 to start qsub: job 6360074.pbs01 ready [galen@node0133 ~]$ export PATH=/home/galen/tassel4-standalone:$PATH [galen@node0133 ~]$ cd tassel4-standalone/ [galen@node0133 tassel4-standalone]$ ./run_pipeline.pl -Xms512m -Xmx10g ./run_pipeline.pl -fork1 -h my_hapmap_file.txt -ld -ldd png -o my_output_file.png -runfork1 Below is a simple example of a TASSEL 4 PBS job script: {% highlight bash %} !/bin/bash PBS -N TAS-TEST PBS -l select=1:ncpus=1:mem=11gb,walltime=12:00:00 PBS -j oe cd $PBS_O_WORKDIR export PATH=/home/galen/tassel4-standalone:$PATH ./run_pipeline.pl -Xms512m -Xmx10g ./run_pipeline.pl -fork1 \\ -h my_hapmap_file.txt -ld -ldd png -o my_output_file.png -runfork1 {% endhighlight %} Updating TASSEL 4 Updating your TASSEL 4 installation to the latest version is easy. Simply enter the TASSEL 4 installation directory and run git pull [galen@login001 ~]$ cd tassel4-standalone [galen@login001 tassel4-standalone]$ git pull","title":"Tassel"},{"location":"software/packages/tassel/#downloadinginstalling-tassel-3","text":"Verify your Java version to ensure that you're downloading the correct version of TASSEL (TASSEL 3 is used with Java version 6, TASSEL 4 is used with Java version 7): [galen@login001 ~]$ java -version java version \"1.6.0_11\" Java(TM) SE Runtime Environment (build 1.6.0_11-b03) Java HotSpot(TM) 64-Bit Server VM (build 11.0-b16, mixed mode) Download the latest build of TASSEL using Git: [galen@login001 ~]$ git --version git version 1.7.1 [galen@login001 ~]$ git clone git://git.code.sf.net/p/tassel/tassel3-standalone Initialized empty Git repository in /home/galen/tassel/tassel3-standalone/.git/ remote: Counting objects: 293, done. remote: Compressing objects: 100% (290/290), done. remote: Total 293 (delta 114), reused 0 (delta 0) Receiving objects: 100% (293/293), 42.95 MiB | 1.86 MiB/s, done. Resolving deltas: 100% (114/114), done.","title":"Downloading/installing TASSEL 3"},{"location":"software/packages/tassel/#running-tassel-3-using-the-gui","text":"-Xms is used to set the lower bound for the total heap size, -Xmx is used to set the upper bound. An incorrectly sized Java heap can lead to OutOfMemoryError exceptions or to a reduction in the performance of the Java application. If the Java heap is smaller than the memory requirements of the application, OutOfMemoryError exceptions are generated because of Java heap exhaustion. If the Java heap is slightly larger than the requirements of the application, garbage collection runs very frequently and affects the performance of the application. Size your Java heap so that your application runs with a minimum heap usage of 40%, and a maximum heap usage of 70%. Of course, you'll have to enable X11 tunneling to display the GUI. See details presented here. [galen@login001 ~]$ qsub -I -X -l select=1:ncpus=1:mem=11gb qsub (Warning): Interactive jobs will be treated as not rerunnable qsub: waiting for job 6360074.pbs01 to start qsub: job 6360074.pbs01 ready [galen@node0133 ~]$ export PATH=/home/galen/tassel3-standalone:$PATH [galen@node0133 ~]$ cd tassel3-standalone/ [galen@node0133 tassel3-standalone]$ ./start_tassel.pl -Xms512m -Xmx10g","title":"Running TASSEL 3 (using the GUI)"},{"location":"software/packages/tassel/#running-tassel-3-from-the-command-line","text":"Please review the user guide for running Tassel 3 from the command line. It's available here: http://www.maizegenetics.net/tassel/docs/TasselPipelineCLI.pdf See note about heap size settings, above. [galen@login001 ~]$ qsub -I -l select=1:ncpus=1:mem=11gb qsub (Warning): Interactive jobs will be treated as not rerunnable qsub: waiting for job 6360074.pbs01 to start qsub: job 6360074.pbs01 ready [galen@node0133 ~]$ export PATH=/home/galen/tassel3-standalone:$PATH [galen@node0133 ~]$ cd tassel3-standalone/ [galen@node0133 tassel3-standalone]$ ./run_pipeline.pl -Xms512m -Xmx10g ./run_pipeline.pl -fork1 -h my_hapmap_file.txt -ld -ldd png -o my_output_file.png -runfork1 Below is a simple example of a TASSEL 3 PBS job script: {% highlight bash %}","title":"Running TASSEL 3 (from the command line)"},{"location":"software/packages/tassel/#binbash","text":"","title":"!/bin/bash"},{"location":"software/packages/tassel/#pbs-n-tas-test","text":"","title":"PBS -N TAS-TEST"},{"location":"software/packages/tassel/#pbs-l-select1ncpus1mem11gbwalltime120000","text":"","title":"PBS -l select=1:ncpus=1:mem=11gb,walltime=12:00:00"},{"location":"software/packages/tassel/#pbs-j-oe","text":"cd $PBS_O_WORKDIR export PATH=/home/galen/tassel3-standalone:$PATH ./run_pipeline.pl -Xms512m -Xmx10g ./run_pipeline.pl -fork1 \\ -h my_hapmap_file.txt -ld -ldd png -o my_output_file.png -runfork1 {% endhighlight %}","title":"PBS -j oe"},{"location":"software/packages/tassel/#updating-tassel-3","text":"Updating your TASSEL 3 installation to the latest version is easy. Simply enter the TASSEL 3 installation directory and run git pull [galen@login001 ~]$ cd tassel3-standalone [galen@login001 tassel3-standalone]$ git pull","title":"Updating TASSEL 3"},{"location":"software/packages/tassel/#downloadinginstalling-tassel-4","text":"Verify your Java version to ensure that you're downloading the correct version of TASSEL (TASSEL 3 is used with Java version 6, TASSEL 4 is used with Java version 7). To install your own build (version) of Java, see the instructions presented here. [galen@login001 ~]$ java -version java version \"1.7.0_45\" Java(TM) SE Runtime Environment (build 1.7.0_45-b18) Java HotSpot(TM) 64-Bit Server VM (build 24.45-b08, mixed mode) Download the latest build of TASSEL using Git: [galen@login001 ~]$ git --version git version 1.7.1 [galen@login001 ~]$ git clone git://git.code.sf.net/p/tassel/tassel4-standalone Initialized empty Git repository in /home/galen/tassel4-standalone/.git/ remote: Counting objects: 361, done. remote: Compressing objects: 100% (358/358), done. remote: Total 361 (delta 176), reused 0 (delta 0) Receiving objects: 100% (361/361), 48.07 MiB | 3.83 MiB/s, done. Resolving deltas: 100% (176/176), done.","title":"Downloading/installing TASSEL 4"},{"location":"software/packages/tassel/#running-tassel-4-using-the-gui","text":"-Xms is used to set the lower bound for the total heap size, -Xmx is used to set the upper bound. An incorrectly sized Java heap can lead to OutOfMemoryError exceptions or to a reduction in the performance of the Java application. If the Java heap is smaller than the memory requirements of the application, OutOfMemoryError exceptions are generated because of Java heap exhaustion. If the Java heap is slightly larger than the requirements of the application, garbage collection runs very frequently and affects the performance of the application. Size your Java heap so that your application runs with a minimum heap usage of 40%, and a maximum heap usage of 70%. Of course, you'll have to enable X11 tunneling to display the GUI. See details presented here. [galen@login001 ~]$ qsub -I -X -l select=1:ncpus=1:mem=11gb qsub (Warning): Interactive jobs will be treated as not rerunnable qsub: waiting for job 6360074.pbs01 to start qsub: job 6360074.pbs01 ready [galen@node0133 ~]$ export PATH=/home/galen/tassel4-standalone:$PATH [galen@node0133 ~]$ cd tassel4-standalone/ [galen@node0133 tassel4-standalone]$ ./start_tassel.pl -Xms512m -Xmx10g","title":"Running TASSEL 4 (using the GUI)"},{"location":"software/packages/tassel/#running-tassel-4-from-the-command-line","text":"Please review the user guide for running Tassel 4 from the command line. It's available here: http://www.maizegenetics.net/tassel/docs/TasselPipelineCLI.pdf See note about heap size settings, above. [galen@login001 ~]$ qsub -I -l select=1:ncpus=1:mem=11gb qsub (Warning): Interactive jobs will be treated as not rerunnable qsub: waiting for job 6360074.pbs01 to start qsub: job 6360074.pbs01 ready [galen@node0133 ~]$ export PATH=/home/galen/tassel4-standalone:$PATH [galen@node0133 ~]$ cd tassel4-standalone/ [galen@node0133 tassel4-standalone]$ ./run_pipeline.pl -Xms512m -Xmx10g ./run_pipeline.pl -fork1 -h my_hapmap_file.txt -ld -ldd png -o my_output_file.png -runfork1 Below is a simple example of a TASSEL 4 PBS job script: {% highlight bash %}","title":"Running TASSEL 4 (from the command line)"},{"location":"software/packages/tassel/#binbash_1","text":"","title":"!/bin/bash"},{"location":"software/packages/tassel/#pbs-n-tas-test_1","text":"","title":"PBS -N TAS-TEST"},{"location":"software/packages/tassel/#pbs-l-select1ncpus1mem11gbwalltime120000_1","text":"","title":"PBS -l select=1:ncpus=1:mem=11gb,walltime=12:00:00"},{"location":"software/packages/tassel/#pbs-j-oe_1","text":"cd $PBS_O_WORKDIR export PATH=/home/galen/tassel4-standalone:$PATH ./run_pipeline.pl -Xms512m -Xmx10g ./run_pipeline.pl -fork1 \\ -h my_hapmap_file.txt -ld -ldd png -o my_output_file.png -runfork1 {% endhighlight %}","title":"PBS -j oe"},{"location":"software/packages/tassel/#updating-tassel-4","text":"Updating your TASSEL 4 installation to the latest version is easy. Simply enter the TASSEL 4 installation directory and run git pull [galen@login001 ~]$ cd tassel4-standalone [galen@login001 tassel4-standalone]$ git pull","title":"Updating TASSEL 4"},{"location":"software/packages/tensorflow/","text":"Installing Tensorflow and setting up the corresponding JupyterHub kernel This page explains how to install the Tensorflow package for use with GPUs on the cluster, and how to use it from Jupyter Notebook via JupyterHub . Installing Tensorflow for GPU node Request an interactive session on a GPU node. For example: $ qsub -I -l select=1:ncpus=16:mem=20gb:ngpus=1:gpu_model=p100,walltime=3:00:00 Load the Anaconda module: $ module load cuda-toolkit/9.0.176 cuDNN/9.0v7 anaconda3/5.0.1 Create a conda environment called tf_env (or any name you like): $ conda create -n tf_env Activate the conda environment: $ source activate tf_env Install Tensorflow with GPU support from the anaconda channel: $ conda install -c anaconda tensorflow-gpu=2.0.0 This will automatically install some packages that are required for Tensorflow, like SciPy or NumPy. To see the list of installed packages, type $ conda list If you need additional packages (for example, Pandas), you can type $ conda install pandas You can now run Python and test the install: ``` $ python import tensorflow as tf ``` Each time you login, you will first need to load the required modules and also activate the tf_env conda environment before running Python: $ module load cuda-toolkit/9.0.176 cuDNN/9.0v7 anaconda3/5.0.1 $ source activate tf_env Installing Tensorflow for non-GPU node Request an interactive session for non-GPU node. For example: $ qsub -I -l select=1:ncpus=16:mem=20gb,walltime=3:00:00 Load the required modules $ module load cuda-toolkit/9.0.176 cuDNN/9.0v7 anaconda3/5.0.1 Create a conda environment called tf_env_cpu : $ conda create -n tf_env_cpu Activate the conda environment: $ source activate tf_env_cpu Install Tensorflow from the anaconda channel: $ conda install -c anaconda tensorflow=2.0.0 This will automatically install some packages that are required for Tensorflow, like SciPy or NumPy. To see the list of installed packages, type $ conda list If you need additional packages (for example, Pandas), you can type $ conda install pandas You can now run Python and test the install: ``` $ python import tensorflow as tf ``` Setting up the kernel for JupyterHub: If you would like to use Tensorflow from Jupyter Notebook on Palmetto via JupyterHub , you need the following additional steps: After you have installed Tensorflow, install Jupyter in the same conda environment: $ conda install jupyter Now, set up a Notebook kernel called \"Tensorflow\". For Tensorflow with GPU support, do: $ python -m ipykernel install --user --name tf_env --display-name Tensorflow For Tensorflow without GPU support, do: $ python -m ipykernel install --user --name tf_env_cpu --display-name Tensorflow Create/edit the file .jhubrc in your home directory: $ cd $ nano .jhubrc Add the following two lines to the .jhubrc file, then exit. module load cuda-toolkit/9.0.176 module load cuDNN/9.0v7 Log into JupyterHub . Recommended settings: Number of chunks: 1 CPU cores: 16 Memory: 62gb Number of GPUs: 1 or 2 (Note: if GPU node is acquired, at least 2 CPU cores are required) Queue: workq Once your JupyterHub has started, you should see the Tensorflow kernel in your list of kernels when you click \"New\". Setting up the kernel for non-GPU node is the same as GPU, except using tf_env_cpu","title":"Tensorflow"},{"location":"software/packages/tensorflow/#installing-tensorflow-and-setting-up-the-corresponding-jupyterhub-kernel","text":"This page explains how to install the Tensorflow package for use with GPUs on the cluster, and how to use it from Jupyter Notebook via JupyterHub .","title":"Installing Tensorflow and setting up the corresponding JupyterHub kernel"},{"location":"software/packages/tensorflow/#installing-tensorflow-for-gpu-node","text":"Request an interactive session on a GPU node. For example: $ qsub -I -l select=1:ncpus=16:mem=20gb:ngpus=1:gpu_model=p100,walltime=3:00:00 Load the Anaconda module: $ module load cuda-toolkit/9.0.176 cuDNN/9.0v7 anaconda3/5.0.1 Create a conda environment called tf_env (or any name you like): $ conda create -n tf_env Activate the conda environment: $ source activate tf_env Install Tensorflow with GPU support from the anaconda channel: $ conda install -c anaconda tensorflow-gpu=2.0.0 This will automatically install some packages that are required for Tensorflow, like SciPy or NumPy. To see the list of installed packages, type $ conda list If you need additional packages (for example, Pandas), you can type $ conda install pandas You can now run Python and test the install: ``` $ python import tensorflow as tf ``` Each time you login, you will first need to load the required modules and also activate the tf_env conda environment before running Python: $ module load cuda-toolkit/9.0.176 cuDNN/9.0v7 anaconda3/5.0.1 $ source activate tf_env","title":"Installing Tensorflow for GPU node"},{"location":"software/packages/tensorflow/#installing-tensorflow-for-non-gpu-node","text":"Request an interactive session for non-GPU node. For example: $ qsub -I -l select=1:ncpus=16:mem=20gb,walltime=3:00:00 Load the required modules $ module load cuda-toolkit/9.0.176 cuDNN/9.0v7 anaconda3/5.0.1 Create a conda environment called tf_env_cpu : $ conda create -n tf_env_cpu Activate the conda environment: $ source activate tf_env_cpu Install Tensorflow from the anaconda channel: $ conda install -c anaconda tensorflow=2.0.0 This will automatically install some packages that are required for Tensorflow, like SciPy or NumPy. To see the list of installed packages, type $ conda list If you need additional packages (for example, Pandas), you can type $ conda install pandas You can now run Python and test the install: ``` $ python import tensorflow as tf ```","title":"Installing Tensorflow for non-GPU node"},{"location":"software/packages/tensorflow/#setting-up-the-kernel-for-jupyterhub","text":"If you would like to use Tensorflow from Jupyter Notebook on Palmetto via JupyterHub , you need the following additional steps: After you have installed Tensorflow, install Jupyter in the same conda environment: $ conda install jupyter Now, set up a Notebook kernel called \"Tensorflow\". For Tensorflow with GPU support, do: $ python -m ipykernel install --user --name tf_env --display-name Tensorflow For Tensorflow without GPU support, do: $ python -m ipykernel install --user --name tf_env_cpu --display-name Tensorflow Create/edit the file .jhubrc in your home directory: $ cd $ nano .jhubrc Add the following two lines to the .jhubrc file, then exit. module load cuda-toolkit/9.0.176 module load cuDNN/9.0v7 Log into JupyterHub . Recommended settings: Number of chunks: 1 CPU cores: 16 Memory: 62gb Number of GPUs: 1 or 2 (Note: if GPU node is acquired, at least 2 CPU cores are required) Queue: workq Once your JupyterHub has started, you should see the Tensorflow kernel in your list of kernels when you click \"New\".","title":"Setting up the kernel for JupyterHub:"},{"location":"software/packages/tensorflow/#setting-up-the-kernel-for-non-gpu-node-is-the-same-as-gpu-except-using-tf_env_cpu","text":"","title":"Setting up the kernel for non-GPU node is the same as GPU, except using tf_env_cpu"},{"location":"software/packages/trinity/","text":"Trinity represents a novel method for the efficient and robust de novo reconstruction of transcriptomes from RNA-seq data. Trinity combines three independent software modules: Inchworm, Chrysalis, and Butterfly, applied sequentially to process large volumes of RNA-seq reads. Trinity partitions the sequence data into many individual de Bruijn graphs, each representing the transcriptional complexity at at a given gene or locus, and then processes each graph independently to extract full-length splicing isoforms and to tease apart transcripts derived from paralogous genes. Download Trinity wget http://downloads.sourceforge.net/project/trinityrnaseq/trinityrnaseq_r20131110.tgz tar -zxf trinityrnaseq_r20131110.tgz cd trinityrnaseq_r20131110 Build/compile Trinity You can build/compile Trinity by simply running make in the installation directory. This should build Inchworm and Chrysalis, both written in C++. Butterfly should not require any special compilation, as its written in Java and provided as portable precompiled software. module add gcc/4.8.1 make You'll also need to install Bowtie (version 1, not Bowtie2) for running Trinity, wget http://downloads.sourceforge.net/project/bowtie-bio/bowtie/1.0.0/bowtie-1.0.0-src.zip unzip bowtie-1.0.0-src.zip cd bowtie-1.0.0 module add gcc/4.8.1 make ...and you may also need to install SAMtools, wget http://downloads.sourceforge.net/project/samtools/samtools/0.1.19/samtools-0.1.19.tar.bz2 tar -jxf samtools-0.1.19.tar.bz2 cd samtools-0.1.19 module add gcc/4.8.1 make Running Trinity A complete Trinity job runs in a series of stages, so you can run Trinity as a single job or as a series of jobs. The specific settings you choose when running your Trinity job can vary so please review the Trinity documentation http://trinityrnaseq.sourceforge.net/#running_trinity to ensure that you've chosen the appropriate settings for your job. The following are just examples for running Trinity on Palmetto. There are a varitey of ways to do this. For any Trinity job, the number of CPU cores and amount of memory you need will vary depending on your job's needs. Please consult the Trinity documentation to try to determine the appropriate hardware for your job. For a complete, standalone Trinity job, the PBS job script may look something like this: {% highlight bash %} !/bin/bash PBS -N fasta2q PBS -l select=1:ncpus=16:mpiprocs=16:mem=62gb:ssd=true,walltime=72:00:00 PBS -j oe source /etc/profile.d/modules.sh module purge module add gcc/4.8.1 cd $PBS_O_WORKDIR mkdir -p /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/trinityrnaseq_r20131110 /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/bowtie-1.0.0 /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/samtools-0.1.19 /local_scratch/pbs.${PBS_JOBID} export PATH=/local_scratch/pbs.${PBS_JOBID}/samtools-0.1.19:$PATH export PATH=/local_scratch/pbs.${PBS_JOBID}/bowtie-1.0.0:$PATH cp /home/galen/job1/XXTL.fasta /home/galen/job1/XXTR.fasta /local_scratch/pbs.${PBS_JOBID} /local_scratch/pbs.${PBS_JOBID}/trinityrnaseq_r20131110/Trinity.pl --seqType fa --JM 60G --left XXTL.fasta --right XXTR.fasta --output /local_scratch/pbs.${PBS_JOBID}/trinity_out_dir --CPU 16 --inchworm_cpu 16 --min_kmer_cov 2 cp -r /local_scratch/pbs.${PBS_JOBID}/trinity_out_dir /home/galen/job1 rm -rf /local_scratch/pbs.${PBS_JOBID}/* {% endhighlight %} If you choose to break-up your Trinity stages into separate jobs, those individual stage job scripts may look something like these (below). Note: the last option for the Trinity.pl command in each stage (colored red, below) is how you will instruct Trinity to complete only the current stage. The Trinity.pl script will check to see where the previous job ended and it will resume processing tasks from that point. Trinity stage 1 , generate the kmer-catalog and run Inchworm, {% highlight bash %} !/bin/bash PBS -N Tstage1 PBS -l select=1:ncpus=16:mpiprocs=16:mem=62gb:ssd=true,walltime=72:00:00 PBS -j oe source /etc/profile.d/modules.sh module purge module add gcc/4.8.1 cd $PBS_O_WORKDIR mkdir -p /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/trinityrnaseq_r20131110 /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/bowtie-1.0.0 /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/samtools-0.1.19 /local_scratch/pbs.${PBS_JOBID} export PATH=/local_scratch/pbs.${PBS_JOBID}/samtools-0.1.19:$PATH export PATH=/local_scratch/pbs.${PBS_JOBID}/bowtie-1.0.0:$PATH cp /home/galen/job1/XXTL.fasta /home/galen/job1/XXTR.fasta /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/trinity_out_dir /local_scratch/pbs.${PBS_JOBID} /local_scratch/pbs.${PBS_JOBID}/trinityrnaseq_r20131110/Trinity.pl --seqType fa --JM 60G --left XXTL.fasta --right XXTR.fasta --output /local_scratch/pbs.${PBS_JOBID}/trinity_out_dir --CPU 16 --inchworm_cpu 16 --min_kmer_cov 2 --no_run_chrysalis cp -r /local_scratch/pbs.${PBS_JOBID}/trinity_out_dir /home/galen/job1 rm -rf /local_scratch/pbs.${PBS_JOBID}/* {% endhighlight %} Trinity stage 2 , Chrysalis clustering of inchworm contigs and mapping reads, {% highlight bash %} !/bin/bash PBS -N Tstage2 PBS -l select=1:ncpus=16:mpiprocs=16:mem=62gb:ssd=true,walltime=72:00:00 PBS -j oe source /etc/profile.d/modules.sh module purge module add gcc/4.8.1 cd $PBS_O_WORKDIR mkdir -p /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/trinityrnaseq_r20131110 /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/bowtie-1.0.0 /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/samtools-0.1.19 /local_scratch/pbs.${PBS_JOBID} export PATH=/local_scratch/pbs.${PBS_JOBID}/samtools-0.1.19:$PATH export PATH=/local_scratch/pbs.${PBS_JOBID}/bowtie-1.0.0:$PATH cp /home/galen/job1/XXTL.fasta /home/galen/job1/XXTR.fasta /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/trinity_out_dir /local_scratch/pbs.${PBS_JOBID} /local_scratch/pbs.${PBS_JOBID}/trinityrnaseq_r20131110/Trinity.pl --seqType fa --JM 60G --left XXTL.fasta --right XXTR.fasta --output /local_scratch/pbs.${PBS_JOBID}/trinity_out_dir --CPU 16 --inchworm_cpu 16 --min_kmer_cov 2 --no_run_quantifygraph cp -r /local_scratch/pbs.${PBS_JOBID}/trinity_out_dir /home/galen/job1 rm -rf /local_scratch/pbs.${PBS_JOBID}/* {% endhighlight %} Trinity stage 3 , Chrysalis deBruijn graph construction, {% highlight bash %} !/bin/bash PBS -N Tstage1 PBS -l select=1:ncpus=16:mpiprocs=16:mem=62gb:ssd=true,walltime=72:00:00 PBS -j oe source /etc/profile.d/modules.sh module purge module add gcc/4.8.1 cd $PBS_O_WORKDIR mkdir -p /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/trinityrnaseq_r20131110 /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/bowtie-1.0.0 /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/samtools-0.1.19 /local_scratch/pbs.${PBS_JOBID} export PATH=/local_scratch/pbs.${PBS_JOBID}/samtools-0.1.19:$PATH export PATH=/local_scratch/pbs.${PBS_JOBID}/bowtie-1.0.0:$PATH cp /home/galen/job1/XXTL.fasta /home/galen/job1/XXTR.fasta /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/trinity_out_dir /local_scratch/pbs.${PBS_JOBID} /local_scratch/pbs.${PBS_JOBID}/trinityrnaseq_r20131110/Trinity.pl --seqType fa --JM 60G --left XXTL.fasta --right XXTR.fasta --output /local_scratch/pbs.${PBS_JOBID}/trinity_out_dir --CPU 16 --inchworm_cpu 16 --min_kmer_cov 2 --no_run_butterfly cp -r /local_scratch/pbs.${PBS_JOBID}/trinity_out_dir /home/galen/job1 rm -rf /local_scratch/pbs.${PBS_JOBID}/* {% endhighlight %} Trinity stage 4, run Butterfly and generate the final Trinity.fasta file, {% highlight bash %} !/bin/bash PBS -N Tstage1 PBS -l select=1:ncpus=16:mpiprocs=16:mem=62gb:ssd=true,walltime=72:00:00 PBS -j oe source /etc/profile.d/modules.sh module purge module add gcc/4.8.1 cd $PBS_O_WORKDIR mkdir -p /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/trinityrnaseq_r20131110 /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/bowtie-1.0.0 /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/samtools-0.1.19 /local_scratch/pbs.${PBS_JOBID} export PATH=/local_scratch/pbs.${PBS_JOBID}/samtools-0.1.19:$PATH export PATH=/local_scratch/pbs.${PBS_JOBID}/bowtie-1.0.0:$PATH cp /home/galen/job1/XXTL.fasta /home/galen/job1/XXTR.fasta /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/trinity_out_dir /local_scratch/pbs.${PBS_JOBID} /local_scratch/pbs.${PBS_JOBID}/trinityrnaseq_r20131110/Trinity.pl --seqType fa --JM 60G --left XXTL.fasta --right XXTR.fasta --output /local_scratch/pbs.${PBS_JOBID}/trinity_out_dir --CPU 16 --inchworm_cpu 16 --min_kmer_cov 2 --bflyHeapSpaceMax 60G --bflyCPU 4 cp -r /local_scratch/pbs.${PBS_JOBID}/trinity_out_dir /home/galen/job1 rm -rf /local_scratch/pbs.${PBS_JOBID}/* {% endhighlight %}","title":"Trinity"},{"location":"software/packages/trinity/#download-trinity","text":"wget http://downloads.sourceforge.net/project/trinityrnaseq/trinityrnaseq_r20131110.tgz tar -zxf trinityrnaseq_r20131110.tgz cd trinityrnaseq_r20131110","title":"Download Trinity"},{"location":"software/packages/trinity/#buildcompile-trinity","text":"You can build/compile Trinity by simply running make in the installation directory. This should build Inchworm and Chrysalis, both written in C++. Butterfly should not require any special compilation, as its written in Java and provided as portable precompiled software. module add gcc/4.8.1 make You'll also need to install Bowtie (version 1, not Bowtie2) for running Trinity, wget http://downloads.sourceforge.net/project/bowtie-bio/bowtie/1.0.0/bowtie-1.0.0-src.zip unzip bowtie-1.0.0-src.zip cd bowtie-1.0.0 module add gcc/4.8.1 make ...and you may also need to install SAMtools, wget http://downloads.sourceforge.net/project/samtools/samtools/0.1.19/samtools-0.1.19.tar.bz2 tar -jxf samtools-0.1.19.tar.bz2 cd samtools-0.1.19 module add gcc/4.8.1 make","title":"Build/compile Trinity"},{"location":"software/packages/trinity/#running-trinity","text":"A complete Trinity job runs in a series of stages, so you can run Trinity as a single job or as a series of jobs. The specific settings you choose when running your Trinity job can vary so please review the Trinity documentation http://trinityrnaseq.sourceforge.net/#running_trinity to ensure that you've chosen the appropriate settings for your job. The following are just examples for running Trinity on Palmetto. There are a varitey of ways to do this. For any Trinity job, the number of CPU cores and amount of memory you need will vary depending on your job's needs. Please consult the Trinity documentation to try to determine the appropriate hardware for your job. For a complete, standalone Trinity job, the PBS job script may look something like this: {% highlight bash %}","title":"Running Trinity"},{"location":"software/packages/trinity/#binbash","text":"","title":"!/bin/bash"},{"location":"software/packages/trinity/#pbs-n-fasta2q","text":"","title":"PBS -N fasta2q"},{"location":"software/packages/trinity/#pbs-l-select1ncpus16mpiprocs16mem62gbssdtruewalltime720000","text":"","title":"PBS -l select=1:ncpus=16:mpiprocs=16:mem=62gb:ssd=true,walltime=72:00:00"},{"location":"software/packages/trinity/#pbs-j-oe","text":"source /etc/profile.d/modules.sh module purge module add gcc/4.8.1 cd $PBS_O_WORKDIR mkdir -p /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/trinityrnaseq_r20131110 /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/bowtie-1.0.0 /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/samtools-0.1.19 /local_scratch/pbs.${PBS_JOBID} export PATH=/local_scratch/pbs.${PBS_JOBID}/samtools-0.1.19:$PATH export PATH=/local_scratch/pbs.${PBS_JOBID}/bowtie-1.0.0:$PATH cp /home/galen/job1/XXTL.fasta /home/galen/job1/XXTR.fasta /local_scratch/pbs.${PBS_JOBID} /local_scratch/pbs.${PBS_JOBID}/trinityrnaseq_r20131110/Trinity.pl --seqType fa --JM 60G --left XXTL.fasta --right XXTR.fasta --output /local_scratch/pbs.${PBS_JOBID}/trinity_out_dir --CPU 16 --inchworm_cpu 16 --min_kmer_cov 2 cp -r /local_scratch/pbs.${PBS_JOBID}/trinity_out_dir /home/galen/job1 rm -rf /local_scratch/pbs.${PBS_JOBID}/* {% endhighlight %} If you choose to break-up your Trinity stages into separate jobs, those individual stage job scripts may look something like these (below). Note: the last option for the Trinity.pl command in each stage (colored red, below) is how you will instruct Trinity to complete only the current stage. The Trinity.pl script will check to see where the previous job ended and it will resume processing tasks from that point. Trinity stage 1 , generate the kmer-catalog and run Inchworm, {% highlight bash %}","title":"PBS -j oe"},{"location":"software/packages/trinity/#binbash_1","text":"","title":"!/bin/bash"},{"location":"software/packages/trinity/#pbs-n-tstage1","text":"","title":"PBS -N Tstage1"},{"location":"software/packages/trinity/#pbs-l-select1ncpus16mpiprocs16mem62gbssdtruewalltime720000_1","text":"","title":"PBS -l select=1:ncpus=16:mpiprocs=16:mem=62gb:ssd=true,walltime=72:00:00"},{"location":"software/packages/trinity/#pbs-j-oe_1","text":"source /etc/profile.d/modules.sh module purge module add gcc/4.8.1 cd $PBS_O_WORKDIR mkdir -p /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/trinityrnaseq_r20131110 /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/bowtie-1.0.0 /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/samtools-0.1.19 /local_scratch/pbs.${PBS_JOBID} export PATH=/local_scratch/pbs.${PBS_JOBID}/samtools-0.1.19:$PATH export PATH=/local_scratch/pbs.${PBS_JOBID}/bowtie-1.0.0:$PATH cp /home/galen/job1/XXTL.fasta /home/galen/job1/XXTR.fasta /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/trinity_out_dir /local_scratch/pbs.${PBS_JOBID} /local_scratch/pbs.${PBS_JOBID}/trinityrnaseq_r20131110/Trinity.pl --seqType fa --JM 60G --left XXTL.fasta --right XXTR.fasta --output /local_scratch/pbs.${PBS_JOBID}/trinity_out_dir --CPU 16 --inchworm_cpu 16 --min_kmer_cov 2 --no_run_chrysalis cp -r /local_scratch/pbs.${PBS_JOBID}/trinity_out_dir /home/galen/job1 rm -rf /local_scratch/pbs.${PBS_JOBID}/* {% endhighlight %} Trinity stage 2 , Chrysalis clustering of inchworm contigs and mapping reads, {% highlight bash %}","title":"PBS -j oe"},{"location":"software/packages/trinity/#binbash_2","text":"","title":"!/bin/bash"},{"location":"software/packages/trinity/#pbs-n-tstage2","text":"","title":"PBS -N Tstage2"},{"location":"software/packages/trinity/#pbs-l-select1ncpus16mpiprocs16mem62gbssdtruewalltime720000_2","text":"","title":"PBS -l select=1:ncpus=16:mpiprocs=16:mem=62gb:ssd=true,walltime=72:00:00"},{"location":"software/packages/trinity/#pbs-j-oe_2","text":"source /etc/profile.d/modules.sh module purge module add gcc/4.8.1 cd $PBS_O_WORKDIR mkdir -p /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/trinityrnaseq_r20131110 /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/bowtie-1.0.0 /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/samtools-0.1.19 /local_scratch/pbs.${PBS_JOBID} export PATH=/local_scratch/pbs.${PBS_JOBID}/samtools-0.1.19:$PATH export PATH=/local_scratch/pbs.${PBS_JOBID}/bowtie-1.0.0:$PATH cp /home/galen/job1/XXTL.fasta /home/galen/job1/XXTR.fasta /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/trinity_out_dir /local_scratch/pbs.${PBS_JOBID} /local_scratch/pbs.${PBS_JOBID}/trinityrnaseq_r20131110/Trinity.pl --seqType fa --JM 60G --left XXTL.fasta --right XXTR.fasta --output /local_scratch/pbs.${PBS_JOBID}/trinity_out_dir --CPU 16 --inchworm_cpu 16 --min_kmer_cov 2 --no_run_quantifygraph cp -r /local_scratch/pbs.${PBS_JOBID}/trinity_out_dir /home/galen/job1 rm -rf /local_scratch/pbs.${PBS_JOBID}/* {% endhighlight %} Trinity stage 3 , Chrysalis deBruijn graph construction, {% highlight bash %}","title":"PBS -j oe"},{"location":"software/packages/trinity/#binbash_3","text":"","title":"!/bin/bash"},{"location":"software/packages/trinity/#pbs-n-tstage1_1","text":"","title":"PBS -N Tstage1"},{"location":"software/packages/trinity/#pbs-l-select1ncpus16mpiprocs16mem62gbssdtruewalltime720000_3","text":"","title":"PBS -l select=1:ncpus=16:mpiprocs=16:mem=62gb:ssd=true,walltime=72:00:00"},{"location":"software/packages/trinity/#pbs-j-oe_3","text":"source /etc/profile.d/modules.sh module purge module add gcc/4.8.1 cd $PBS_O_WORKDIR mkdir -p /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/trinityrnaseq_r20131110 /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/bowtie-1.0.0 /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/samtools-0.1.19 /local_scratch/pbs.${PBS_JOBID} export PATH=/local_scratch/pbs.${PBS_JOBID}/samtools-0.1.19:$PATH export PATH=/local_scratch/pbs.${PBS_JOBID}/bowtie-1.0.0:$PATH cp /home/galen/job1/XXTL.fasta /home/galen/job1/XXTR.fasta /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/trinity_out_dir /local_scratch/pbs.${PBS_JOBID} /local_scratch/pbs.${PBS_JOBID}/trinityrnaseq_r20131110/Trinity.pl --seqType fa --JM 60G --left XXTL.fasta --right XXTR.fasta --output /local_scratch/pbs.${PBS_JOBID}/trinity_out_dir --CPU 16 --inchworm_cpu 16 --min_kmer_cov 2 --no_run_butterfly cp -r /local_scratch/pbs.${PBS_JOBID}/trinity_out_dir /home/galen/job1 rm -rf /local_scratch/pbs.${PBS_JOBID}/* {% endhighlight %} Trinity stage 4, run Butterfly and generate the final Trinity.fasta file, {% highlight bash %}","title":"PBS -j oe"},{"location":"software/packages/trinity/#binbash_4","text":"","title":"!/bin/bash"},{"location":"software/packages/trinity/#pbs-n-tstage1_2","text":"","title":"PBS -N Tstage1"},{"location":"software/packages/trinity/#pbs-l-select1ncpus16mpiprocs16mem62gbssdtruewalltime720000_4","text":"","title":"PBS -l select=1:ncpus=16:mpiprocs=16:mem=62gb:ssd=true,walltime=72:00:00"},{"location":"software/packages/trinity/#pbs-j-oe_4","text":"source /etc/profile.d/modules.sh module purge module add gcc/4.8.1 cd $PBS_O_WORKDIR mkdir -p /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/trinityrnaseq_r20131110 /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/bowtie-1.0.0 /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/samtools-0.1.19 /local_scratch/pbs.${PBS_JOBID} export PATH=/local_scratch/pbs.${PBS_JOBID}/samtools-0.1.19:$PATH export PATH=/local_scratch/pbs.${PBS_JOBID}/bowtie-1.0.0:$PATH cp /home/galen/job1/XXTL.fasta /home/galen/job1/XXTR.fasta /local_scratch/pbs.${PBS_JOBID} cp -r /home/galen/trinity_out_dir /local_scratch/pbs.${PBS_JOBID} /local_scratch/pbs.${PBS_JOBID}/trinityrnaseq_r20131110/Trinity.pl --seqType fa --JM 60G --left XXTL.fasta --right XXTR.fasta --output /local_scratch/pbs.${PBS_JOBID}/trinity_out_dir --CPU 16 --inchworm_cpu 16 --min_kmer_cov 2 --bflyHeapSpaceMax 60G --bflyCPU 4 cp -r /local_scratch/pbs.${PBS_JOBID}/trinity_out_dir /home/galen/job1 rm -rf /local_scratch/pbs.${PBS_JOBID}/* {% endhighlight %}","title":"PBS -j oe"},{"location":"training/schedule/","text":"","title":"Training Schedule"},{"location":"training/workshop/","text":"","title":"Workshop Descriptions"}]}