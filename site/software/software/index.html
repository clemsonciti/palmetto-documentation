<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Software on Palmetto - Palmetto Documentation</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../../css/theme.css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  <link href="../../css/extra.css" rel="stylesheet" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Software on Palmetto";
    var mkdocs_page_input_path = "software/software.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Palmetto Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../about/">About</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../news/">News</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../owner/">Owner</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Basic</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../basic/login/">Login</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../../basic/jupyter/">JupyterLab</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Software</span></p>
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">Software on Palmetto</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#modules">Modules</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#licensed-software">Licensed software</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#software-with-graphical-applications">Software with graphical applications</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#installing-your-own-software">Installing your own software</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#abaqus">ABAQUS</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#running-abaqus-interactive-viewer">Running ABAQUS interactive viewer</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#running-abaqus-in-batch-mode">Running ABAQUS in batch mode</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#amber">Amber</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#ansys">ANSYS</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#graphical-interfaces">Graphical Interfaces</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#batch-mode">Batch Mode</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#comsol">COMSOL</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#graphical-interface">Graphical Interface</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#batch-mode_1">Batch Mode</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#comsol-batch-job-on-a-single-node-using-multiple-cores">COMSOL batch job on a single node, using multiple cores:</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#comsol-batch-job-across-several-nodes">COMSOL batch job across several nodes</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#gephi">Gephi</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#download-and-setup-the-gephi-binary">Download and Setup the Gephi Binary</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#running-gephi">Running Gephi</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#gromacs">GROMACS</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#running-gromacs-without-gpu">Running GROMACS without GPU</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#running-gpu-enabled-gromacs">Running GPU-enabled GROMACS</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#gpu-enabled-gromacs-using-singularity-and-ngc">GPU-enabled GROMACS using Singularity and NGC</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#downloading-the-image">Downloading the image</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#running-gromacs-interactively">Running GROMACS interactively</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#running-gromacs-in-batch-mode">Running GROMACS in batch mode</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#hoomd">HOOMD</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#run-hoomd">Run HOOMD</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#java">Java</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#jre-vs-jdk">JRE vs. JDK</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#downloading-the-jdk">Downloading the JDK</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#installing-the-jdk">Installing the JDK</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#julia">Julia</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#run-julia-in-palmetto-interactive">Run Julia in Palmetto: Interactive</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#run-julia-in-palmetto-batch-mode">Run Julia in Palmetto: Batch mode</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#install-your-own-julia-package-using-conda-environment-and-running-in-jupyterhub">Install your own Julia package using conda environment and running in Jupyterhub</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#lammps">LAMMPS</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#installing-lammps-on-palmetto-cluster">Installing LAMMPS on Palmetto cluster</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#running-lammps-an-example">Running LAMMPS - an example</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#matlab">MATLAB</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#checking-license-usage-for-matlab">Checking license usage for MATLAB</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#running-the-matlab-graphical-interface">Running the MATLAB graphical interface</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#running-the-matlab-command-line-without-graphics">Running the MATLAB command line without graphics</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#matlab-in-batch-jobs">MATLAB in batch jobs</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#compiling-matlab-code-to-create-an-executable">Compiling MATLAB code to create an executable</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#mothur">mothur</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#mrbayes">MrBayes</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#installing-beagle-library">Installing BEAGLE Library</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#installing-mrbayes">Installing MrBayes</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#running-mrbayes">Running MrBayes</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#paraview">Paraview</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#using-paraviewgpus-to-visualize-very-large-datasets">Using Paraview+GPUs to visualize very large datasets</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#rclone">rclone</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#setting-up-rclone-for-use-with-google-drive-on-palmetto">Setting up rclone for use with Google Drive on Palmetto</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#using-rclone">Using rclone</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#singularity">Singularity</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#running-singularity">Running Singularity</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#an-important-change-for-existing-singularity-users">An important change for existing singularity users</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#where-to-download-containers">Where to download containers</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#example-running-openfoam-using-singularity">Example: Running OpenFOAM using Singularity</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#gpu-enabled-software-using-singularity-containers-nvidia-gpu-cloud">GPU-enabled software using Singularity containers (NVIDIA GPU Cloud)</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#pulling-ngc-images">Pulling NGC images</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#running-ngc-containers">Running NGC containers</a>
    </li>
        </ul>
    </li>
    </ul>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../spack/">User Software Installation</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../programming/">Software Development</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Training and Outreach</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../training/schedule/">Training Schedule</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../../training/workshop/">Workshop Descriptions</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Misc</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../faq/common/">Common Issues</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../../faq/faq/">FAQ</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Palmetto Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Software &raquo;</li>
        
      
    
    <li>Software on Palmetto</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="software-installed-on-palmetto">Software installed on Palmetto</h1>
<h2 id="overview">Overview</h2>
<h3 id="modules">Modules</h3>
<p>A large number of popular software packages
are installed on Palmetto and can be used without
any setup or configuration.
These include:</p>
<ul>
<li>Compilers (such as <code>gcc</code>, Intel, and PGI)</li>
<li>Libraries (such as OpenMPI, HDF5, Boost)</li>
<li>Programming languages (such as Python, MATLAB, R)</li>
<li>Scientific applications (such as LAMMPS, Paraview, ANSYS)</li>
<li>Others (e.g., Git, PostgreSQL, Singularity)</li>
</ul>
<p>These packages are available as modules on Palmetto.
The following commands can be used to inspect, activate
and deactivate modules:</p>
<table>
<thead>
<tr>
<th>Command</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>module avail</code></td>
<td>List all packages available (on current system)</td>
</tr>
<tr>
<td><code>module add package/version</code></td>
<td>Add a package to your current shell environment</td>
</tr>
<tr>
<td><code>module list</code></td>
<td>List packages you have loaded</td>
</tr>
<tr>
<td><code>module rm package/version</code></td>
<td>Remove a currently loaded package</td>
</tr>
<tr>
<td><code>module purge</code></td>
<td>Remove <em>all</em> currently loaded packages</td>
</tr>
</tbody>
</table>
<p>See the <a href="{{site.baseurl}}/userguide_quickstart.html">Quick Start Guide</a>
for more details about modules.</p>
<h3 id="licensed-software">Licensed software</h3>
<p>Many site-licensed software packages are available on Palmetto cluster
(e.g., MATLAB, ANSYS, COMSOL, etc.,).
There are limitations on the number of jobs that can run using these packages.
See <a href="{{site.baseurl}}/userguide_howto_check_license_usage.html">this</a> section
of the User's Guide on how to check license usage.</p>
<p>Individual-owned or group-owned licensed software can also be run on Palmetto.</p>
<h3 id="software-with-graphical-applications">Software with graphical applications</h3>
<p>See <a href="{{site.baseurl}}/userguide_howto_run_graphical_applications.html">this</a> section
of the User's Guide on how to use software with graphical user interface (GUI).</p>
<h3 id="installing-your-own-software">Installing your own software</h3>
<p>See <a href="{{site.baseurl}}/userguide_howto_install_software.html">this</a> section
of the User's Guide on how to check license usage.</p>
<h2 id="abaqus">ABAQUS</h2>
<p>ABAQUS is a Finite Element Analysis software used
for engineering simulations.
Currently, ABAQUS versions 6.10, 6.13, 6.14 are available on Palmetto cluster
as modules.</p>
<pre><code>$ module avail abaqus

abaqus/6.10 abaqus/6.13 abaqus/6.14
</code></pre>

<p>To see license usage of ABAQUS-related packages,
you can use the <code>lmstat</code> command:</p>
<pre><code>/software/USR_LOCAL/flexlm/lmstat -a -c /software/USR_LOCAL/flexlm/licenses/abaqus.dat
</code></pre>

<h3 id="running-abaqus-interactive-viewer">Running ABAQUS interactive viewer</h3>
<p>To run the interactive viewer,
you must <a href="{{site.baseurl}}/userguide_howto_run_graphical_applications.html">log-in with tunneling enabled</a>,
and then ask for an interactive session:</p>
<pre><code>$ qsub -I -X -l select=1:ncpus=8:mpiprocs=8:mem=6gb:interconnect=1g,walltime=00:15:00
</code></pre>

<p>Once logged-in to an interactive compute node,
to launch the interactive viewer,
load the <code>abaqus</code> module, and run the <code>abaqus</code> executable with the <code>viewer</code> and <code>-mesa</code> options:</p>
<pre><code>$ module add abaqus/6.14
$ abaqus viewer -mesa
</code></pre>

<p>Similarly,
to launch the ABAQUS CAE graphical interface:</p>
<pre><code>$ abaqus cae -mesa
</code></pre>

<h3 id="running-abaqus-in-batch-mode">Running ABAQUS in batch mode</h3>
<p>To run ABAQUS in batch mode on Palmetto cluster,
you can use the job script in the following example as a template.
This example shows how to run ABAQUS in parallel using MPI.
This demonstration runs the "Axisymmetric analysis of bolted pipe flange connections"
example provided in the ABAQUS documentation <a href="http://bobcat.nus.edu.sg:2080/v6.14/books/exa/default.htm">here</a>.
Please see the documentation for the physics and simulation details.
You can obtain the files required to run this example
using the following commands:</p>
<pre><code>$ cd /scratch2/username
$ module add examples
$ example get ABAQUS
$ cd ABAQUS &amp;&amp; ls

abaqus_v6.env  boltpipeflange_axi_element.inp  boltpipeflange_axi_node.inp  boltpipeflange_axi_solidgask.inp  job.sh
</code></pre>

<p>The <code>.inp</code> files describe the model and simulation to be performed - see
the documentation for details.
The batch script <code>job.sh</code> submits the job to the cluster.
The <code>.env</code> file is a configuration file that <strong>must</strong> be included in all
ABAQUS job submission folders on Palmetto.</p>
<pre><code>#!/bin/bash
#PBS -N AbaqusDemo
#PBS -l select=2:ncpus=8:mpiprocs=8:mem=6gb:interconnect=1g,walltime=00:15:00
#PBS -j oe

module purge
module add abaqus/6.14

pbsdsh sleep 20

NCORES=`wc -l $PBS_NODEFILE | gawk '{print $1}'`
cd $PBS_O_WORKDIR

SCRATCH=$TMPDIR

# copy all input files into the scratch directory
for node in `uniq $PBS_NODEFILE`
do
    ssh $node &quot;cp $PBS_O_WORKDIR/*.inp $SCRATCH&quot;
done

cd $SCRATCH

# run the abaqus program, providing the .inp file as input
abaqus job=abdemo double input=$SCRATCH/boltpipeflange_axi_solidgask.inp scratch=$SCRATCH cpus=$NCORES mp_mode=mpi interactive 

# copy results back from scratch directory to $PBS_O_WORKDIR
for node in `uniq $PBS_NODEFILE`
do
    ssh $node &quot;cp -r $SCRATCH/* $PBS_O_WORKDIR&quot;
done
</code></pre>

<p>In the batch script <code>job.sh</code>:</p>
<ol>
<li>The following line extracts the total number of CPU cores available across
   all the nodes requested by the job:</li>
</ol>
<p>~~~
   NCORES=<code>wc -l $PBS_NODEFILE | gawk '{print $1}'</code>
   ~~~  </p>
<ol>
<li>The following line runs the ABAQUS program, specifying various options
   such as the path to the <code>.inp</code> file, the scratch directory to use, etc.,</li>
</ol>
<p>~~~
   abaqus job=abdemo double input=/scratch2/$USER/ABAQUS/boltpipeflange_axi_solidgask.inp scratch=$SCRATCH cpus=$NCORES mp_mode=mpi interactive
   ~~~  </p>
<p>To submit the job:</p>
<pre><code>$ qsub job.sh
9668628
</code></pre>

<p>After job completion, you will see the job submission directory (<code>/scratch2/username/ABAQUS</code>)
populated with various files:</p>
<pre><code>$ ls

AbaqusDemo.o9668628  abdemo.dat  abdemo.msg  abdemo.res  abdemo.stt                      boltpipeflange_axi_solidgask.inp
abaqus_v6.env        abdemo.fil  abdemo.odb  abdemo.sim  boltpipeflange_axi_element.inp  job.sh
abdemo.com           abdemo.mdl  abdemo.prt  abdemo.sta  boltpipeflange_axi_node.inp
</code></pre>

<p>If everything went well, the job output file (<code>AbaqusDemo.o9668628</code>) should look like this:</p>
<pre><code>[atrikut@login001 ABAQUS]$ cat AbaqusDemo.o9668628
Abaqus JOB abdemo
Abaqus 6.14-1
Abaqus License Manager checked out the following licenses:
Abaqus/Standard checked out 16 tokens from Flexnet server licensevm4.clemson.edu.
&lt;567 out of 602 licenses remain available&gt;.
Begin Analysis Input File Processor
Mon 13 Feb 2017 12:35:29 PM EST
Run pre
Mon 13 Feb 2017 12:35:31 PM EST
End Analysis Input File Processor
Begin Abaqus/Standard Analysis
Mon 13 Feb 2017 12:35:31 PM EST
Run standard
Mon 13 Feb 2017 12:35:35 PM EST
End Abaqus/Standard Analysis
Abaqus JOB abdemo COMPLETED


+------------------------------------------+
| PALMETTO CLUSTER PBS RESOURCES REQUESTED |
+------------------------------------------+

mem=12gb,ncpus=16,walltime=00:15:00


+-------------------------------------+
| PALMETTO CLUSTER PBS RESOURCES USED |
+-------------------------------------+

cpupercent=90,cput=00:00:10,mem=636kb,ncpus=16,vmem=12612kb,walltime=00:00:13
</code></pre>

<p>The output database (<code>.odb</code>) file
contains the results of the simulation which can be viewed
using the ABAQUS viewer:</p>
<p><img src="{{site.baseurl}}/images/abaqus-screenshot-results.png" style="width:650px"></p>
<h2 id="amber">Amber</h2>
<p>In this example we will use "alp" test from Amber's test suite.
To get the files relevant to this example:</p>
<pre><code>$ module add examples
$ example get Amber
$ cd Amber &amp;&amp; ls

amber.pbs  coords  md.in  prmtop  README.md
</code></pre>

<p>Examine the batch script <code>amber.pbs</code>, and adjust
the names of the files i.e. input and coordinates files.
Also, adjust the number of nodes, processors per node and walltime.
To submit the job:</p>
<pre><code>qsub amber.pbs 
</code></pre>

<p>Results will be saved in the <code>md.out</code> file (in our example).</p>
<h2 id="ansys">ANSYS</h2>
<h3 id="graphical-interfaces">Graphical Interfaces</h3>
<p>To run the various ANSYS graphical programs,
you must <a href="{{site.baseurl}}/userguide_howto_run_graphical_applications.html">log-in with tunneling enabled</a>
and then ask for an interactive session:</p>
<pre><code>$ qsub -I -X -l select=1:ncpus=8:mpiprocs=8:mem=6gb:interconnect=1g,walltime=01:00:00
</code></pre>

<p>Once logged-in to an interactive compute node,
you must first load the ANSYS module along with the Intel module:</p>
<pre><code>$ module add ansys/17.2
$ module add intel/12.1
</code></pre>

<p>And then launch the required program:</p>
<p><strong>For ANSYS APDL</strong></p>
<pre><code>$ ansys172 -g
</code></pre>

<p>If you are using e.g., ANSYS 17.0 instead, then the executable is called <code>ansys170</code>.</p>
<p><strong>For CFX</strong></p>
<pre><code>$ cfxlaunch
</code></pre>

<p><strong>For ANSYS Workbench</strong></p>
<pre><code>$ runwb2
</code></pre>

<p><strong>For Fluent</strong></p>
<pre><code>$ fluent
</code></pre>

<p><strong>For ANSYS Electromagnetics</strong></p>
<pre><code>$ ansysedt
</code></pre>

<h3 id="batch-mode">Batch Mode</h3>
<p>To run ANSYS in batch mode on Palmetto cluster,
you can use the job script in the following example as a template.
This example shows how to run ANSYS in parallel (using multiple cores/nodes).
In this demonstration, we model the strain in a 2-D flat plate.
You can obtain the files required to run this example
using the following commands:</p>
<pre><code>$ cd /scratch2/username
$ module add examples
$ example get ANSYS
$ cd ANSYS &amp;&amp; ls

input.txt job.sh
</code></pre>

<p>The <code>input.txt</code> batch file is generated for the model using the ANSYS APDL interface.
The batch script <code>job.sh</code> submits the batch job to the cluster:</p>
<pre><code>#!/bin/bash
#PBS -N ANSYSdis
#PBS -l select=2:ncpus=4:mpiprocs=4:mem=11gb:interconnect=1g
#PBS -l walltime=1:00:00
#PBS -j oe

module purge
module add ansys/17.2

cd $PBS_O_WORKDIR

machines=$(uniq -c $PBS_NODEFILE | awk '{print $2&quot;:&quot;$1}' | tr '\n' :)

for node in `uniq $PBS_NODEFILE`
do
    ssh $node &quot;sleep 5&quot;
    ssh $node &quot;cp input.txt $TMPDIR&quot;
done

cd $TMPDIR
ansys172 -dir $TMPDIR -j EXAMPLE -s read -l en-us -b -i input.txt -o output.txt -dis -machines $machines -usessh

for node in `uniq $PBS_NODEFILE`
do
    ssh $node &quot;cp -r $TMPDIR/* $PBS_O_WORKDIR&quot;
done
</code></pre>

<p>In the batch script <code>job.sh</code>:</p>
<ol>
<li>The following line extracts the nodes (machines) available for this job
   as well as the number of CPU cores allocated for each node:</li>
</ol>
<p>~~~
   machines=$(uniq -c $PBS_NODEFILE | awk '{print $2":"$1}' | tr '\n' :)
   ~~~  </p>
<ol>
<li>For ANSYS jobs, you should always use <code>$TMPDIR</code> (<code>/local_scratch</code>) as the working directory.
   The following lines ensure that <code>$TMPDIR</code> is created on each node:</li>
</ol>
<p>~~~
   do
       ssh $node "sleep 5"
       ssh $node "cp input.txt $TMPDIR"
   done
   ~~~  </p>
<ol>
<li>The following line runs the ANSYS program, specifying various options
   such as the path to the <code>input.txt</code> file, the scratch directory to use, etc.,</li>
</ol>
<p>~~~
   ansys172 -dir $TMPDIR -j EXAMPLE -s read -l en-us -b -i input.txt -o output.txt -dis -machines $machines -usessh
   ~~~  </p>
<ol>
<li>Finally, the following lines copy all the data
   from <code>$TMPDIR</code>:</li>
</ol>
<p>~~~
    do
        ssh $node "cp -r $TMPDIR/* $PBS_O_WORKDIR"
    done
   ~~~</p>
<p>To submit the job:</p>
<pre><code>$ qsub job.sh
9752784
</code></pre>

<p>After job completion, you will see the job submission directory (<code>/scratch2/username/ANSYS</code>)
populated with various files:</p>
<pre><code>$ ls

ANSYSdis.o9752784  EXAMPLE0.stat  EXAMPLE2.err   EXAMPLE3.esav  EXAMPLE4.full  EXAMPLE5.out   EXAMPLE6.rst   EXAMPLE.DSP    input.txt
EXAMPLE0.err       EXAMPLE1.err   EXAMPLE2.esav  EXAMPLE3.full  EXAMPLE4.out   EXAMPLE5.rst   EXAMPLE7.err   EXAMPLE.esav   job.sh
EXAMPLE0.esav      EXAMPLE1.esav  EXAMPLE2.full  EXAMPLE3.out   EXAMPLE4.rst   EXAMPLE6.err   EXAMPLE7.esav  EXAMPLE.mntr   mpd.hosts
EXAMPLE0.full      EXAMPLE1.full  EXAMPLE2.out   EXAMPLE3.rst   EXAMPLE5.err   EXAMPLE6.esav  EXAMPLE7.full  EXAMPLE.rst    mpd.hosts.bak
EXAMPLE0.log       EXAMPLE1.out   EXAMPLE2.rst   EXAMPLE4.err   EXAMPLE5.esav  EXAMPLE6.full  EXAMPLE7.out   host.list      output.txt
EXAMPLE0.rst       EXAMPLE1.rst   EXAMPLE3.err   EXAMPLE4.esav  EXAMPLE5.full  EXAMPLE6.out   EXAMPLE7.rst   host.list.bak
</code></pre>

<p>If everything went well, the job output file (<code>ANSYSdis.o9752784</code>) should look like this:</p>
<pre><code>+------------------------------------------+
| PALMETTO CLUSTER PBS RESOURCES REQUESTED |
+------------------------------------------+

mem=22gb,ncpus=8,walltime=01:00:00


+-------------------------------------+
| PALMETTO CLUSTER PBS RESOURCES USED |
+-------------------------------------+

cpupercent=27,cput=00:00:17,mem=3964kb,ncpus=8,vmem=327820kb,walltime=00:01:07
</code></pre>

<p>The results file (<code>EXAMPLE.rst</code>)
contains the results of the simulation which can be viewed
using the ANSYS APDL graphical interface:</p>
<p><img src="{{site.baseurl}}/images/ansys-screenshot-results.png" style="width:650px"></p>
<h2 id="comsol">COMSOL</h2>
<p>COMSOL is an application for solving Multiphysics problems.
To see the available COMSOL modules on Palmetto:</p>
<pre><code>$ module avail comsol

comsol/4.3b comsol/4.4  comsol/5.0  comsol/5.1  comsol/5.2  comsol/5.3
</code></pre>

<p>To see license usage of COMSOL-related packages,
you can use the <code>lmstat</code> command:</p>
<pre><code>/software/USR_LOCAL/flexlm/lmstat -a -c /software/USR_LOCAL/flexlm/licenses/comsol.dat 
</code></pre>

<h3 id="graphical-interface">Graphical Interface</h3>
<p>To run the COMSOL graphical interface,
you must <a href="{{site.baseurl}}/userguide_howto_run_graphical_applications.html">log-in with tunneling enabled</a>,
and then ask for an interactive session:</p>
<pre><code>$ qsub -I -X -l select=1:ncpus=8:mpiprocs=8:mem=6gb:interconnect=1g,walltime=00:15:00
</code></pre>

<p>Once logged-in to an interactive compute node,
to launch the interactive viewer,
you can use the <code>comsol</code> command to run COMSOL:</p>
<pre><code>$ module add comsol/5.2
$ comsol -np 8 -tmpdir $TMPDIR
</code></pre>

<p>The <code>-np</code> option can be used to specify the number of
CPU cores to use.
Remember to <strong>always</strong> use <code>$TMPDIR</code> as
the working directory for COMSOL jobs.</p>
<h3 id="batch-mode_1">Batch Mode</h3>
<p>To run COMSOL in batch mode on Palmetto cluster,
you can use the example batch scripts below as a template.
The first example demonstrates running COMSOL using multiple cores
on a single node,
while the second demonstrates running COMSOL across multiple nodes
using MPI.
You can obtain the files required to run this example
using the following commands:</p>
<pre><code>$ module add examples
$ example get COMSOL
$ cd COMSOL &amp;&amp; ls

job.sh  job_mpi.sh
</code></pre>

<p>Both of these examples run the
"Heat Transfer by Free Convection" application described
<a href="https://www.comsol.com/model/heat-transfer-by-free-convection-122">here</a>.
In addition to the <code>job.sh</code> and <code>job_mpi.sh</code> scripts, to run the examples and reproduce the results,
you will need to download the file <code>free_convection.mph</code> (choose the correct version) provided
with the description (login required).</p>
<h4 id="comsol-batch-job-on-a-single-node-using-multiple-cores">COMSOL batch job on a single node, using multiple cores:</h4>
<pre><code>#!/bin/bash
#PBS -N COMSOL
#PBS -l select=1:ncpus=8:mem=32gb,walltime=01:30:00
#PBS -j oe

module purge
module add comsol/5.2

cd $PBS_O_WORKDIR

comsol batch -np 8 -tmpdir $TMPDIR -inputfile free_convection.mph -outputfile free_convection_output.mph
</code></pre>

<h4 id="comsol-batch-job-across-several-nodes">COMSOL batch job across several nodes</h4>
<pre><code>#!/bin/bash
#PBS -N COMSOL
#PBS -l select=2:ncpus=8:mpiprocs=8:mem=32gb,walltime=01:30:00
#PBS -j oe

module purge
module add comsol/5.2

cd $PBS_O_WORKDIR

uniq $PBS_NODEFILE &gt; comsol_nodefile
comsol batch -clustersimple -f comsol_nodefile -tmpdir $TMPDIR -inputfile free_convection.mph -outputfile free_convection_output.mph
</code></pre>

<h2 id="gephi">Gephi</h2>
<p>Gephi is an interactive visualization and exploration platform for all kinds of networks and 
complex systems, dynamic and hierarchical graphs.  At this time, we don't have Gephi setup as a 
software module on Palmetto, but downloading and installing it in your own <code>/home</code> or <code>/newscratch</code> 
directory (and running it from that location) is easy to do.  Below are the procedures I used to do this:</p>
<h3 id="download-and-setup-the-gephi-binary">Download and Setup the Gephi Binary</h3>
<pre><code>wget https://launchpad.net/gephi/0.8/0.8.2beta/+download/gephi-0.8.2-beta.tar.gz
tar -zxf gephi-0.8.2-beta.tar.gz
</code></pre>
<p>Now, the gephi binary is installed in <code>~/gephi/bin</code>.</p>
<h3 id="running-gephi">Running Gephi</h3>
<p>Connect to Palmetto with X11 tunneling enabled so you can "tunnel" the GUI running on Palmetto 
through your SSH connection to your desktop:</p>
<pre><code>ssh -X galen@login.palmetto.clemson.edu
</code></pre>
<p>Once you're logged-in to Palmetto, you'll need to launch an interactive job with X11 tunneling 
enabled.  Be sure to request an allocation with enough memory for Gephi to handle your input data 
properly (here, I'm requesing 31 GB RAM):</p>
<pre><code>qsub -I -X -l select=1:ncpus=4:mem=31gb,walltime=6:00:00
</code></pre>
<p>If you are using a Windows system, you'll need to launch a local Xserver (like Xming) so your 
system can display the tunneled GUI information.</p>
<p>Once your local Xserver is ready, launch the Gephi GUI in your interactive session on Palmetto:</p>
<pre><code>/home/galen/gephi/bin/gephi
</code></pre>
<p>Now, I can browse for data I have stored in my directories on Palmetto.  If your data is stored on 
your local workstation, you must move that data to Palmetto so you can open it with Gephi.</p>
<p>You may experience a short delay as the GUI is generated and tunneled to your desktop.  Responsiveness 
between your local workstation and the tunneled GUI may be a bit slow/laggy, but the compute operations 
taking place on that Palmetto compute node are not delayed.</p>
<p><img alt="Gephi" src="{{site.baseurl}}/images/gephi.1.png" /></p>
<h2 id="gromacs">GROMACS</h2>
<p>Various installations of GROMACS are available on the cluster.
Different modules are provided for GPU-enabled and non-GPU versions
of GROMACS:</p>
<pre><code>$ module avail gromacs

---------------------------------------------------------------- /software/modulefiles -----------------------------------------------------------------
gromacs/4.5.4-sp               gromacs/4.6.5-sp-k20-ompi      gromacs/5.0.1-sp-k20-g481-o181 gromacs/5.0.5-nogpu
gromacs/4.6.5-dp-ompi          gromacs/5.0.1-dp-g481-o181     gromacs/5.0.5-gpu

------------------------------------------------------------ /usr/share/Modules/modulefiles ------------------------------------------------------------
gromacs/4.5.4-sp
</code></pre>

<h3 id="running-gromacs-without-gpu">Running GROMACS without GPU</h3>
<p>To use the non-GPU version of GROMACS,
here is an example interactive job:</p>
<pre><code>$ qsub -I -l select=2:ncpus=20:phase=10:mem=100gb:mpiprocs=20,walltime=1:
00:00
$ module load gromacs/5.0.5-nogpu
$ mpirun mdrun_mpi ...
</code></pre>

<p>where <code>...</code> specifies the input files and options of Gromacs.</p>
<p>And an example of a PBS batch script for submitting a GROMACS batch script:</p>
<pre><code>#!/bin/bash
#PBS -l select=2:ncpus=20:phase=10:mem=100gb:mpiprocs=20
#PBS -l walltime=1:00:00

module load gromacs/5.0.5-nogpu
cd $PBS_O_WORKDIR

export OMP_NUM_THREADS=1

mpirun mdrun_mpi ...
</code></pre>

<h3 id="running-gpu-enabled-gromacs">Running GPU-enabled GROMACS</h3>
<p>For the GPU version</p>
<pre><code>#!/bin/bash
#PBS -l select=2:ncpus=20:ngpus=2:phase=10:mem=100gb:mpiprocs=2
#PBS -l walltime=1:00:00

module load gromacs/5.0.5-gpu
cd $PBS_O_WORKDIR

export OMP_NUM_THREADS=10

mpirun mdrun_mpi ...
</code></pre>

<p>Note that the number of MPI processes per chunk is set as 2 when using the GPU.
Normally, the number of MPI processes is the same as the number of CPU cores requested.
But because there are only 2 GPUs per node, we launch only two MPI processes.
In the above example, each MPI process can also use 10 CPU cores
in addition to a GPU (this is enabled by setting the variable <code>OMP_NUM_THREADS</code> to 10).</p>
<h3 id="gpu-enabled-gromacs-using-singularity-and-ngc">GPU-enabled GROMACS using Singularity and NGC</h3>
<p>The NVIDIA GPU cloud provides images for GPU-enabled GROMACS
that can be downloaded and run using Singularity on the Palmetto cluster.
This is the recommended way to run GROMACS on Palmetto.</p>
<h3 id="downloading-the-image">Downloading the image</h3>
<p>Before downloading images from NGC,
you will need to obtain an NVIDIA NGC API key,
instructions for which can be found
<a href="https://docs.nvidia.com/ngc/ngc-getting-started-guide/index.html">here</a>.</p>
<p>Start an interactive job,
and create a directory for storing Singularity images
(if you don't have one already):</p>
<pre><code>$ qsub -I -l select=1:ngpus=2:ncpus=16:mem=20gb:gpu_model=p100,walltime=5:00:00
$ mkdir -p singularity-images
</code></pre>

<p>Set the required environment variables (you will need the API key for this step):</p>
<pre><code>$ export SINGULARITY_DOCKER_USERNAME='$oauthtoken'
$ export SINGULARITY_DOCKER_PASSWORD=&lt;NVIDA NGC API key&gt;
</code></pre>

<p>Navigate to the <code>singularity-images</code> directory and pull
the GROMACS image (choose from the images listed
<a href="https://ngc.nvidia.com/registry/hpc-gromacs">here</a>:</p>
<pre><code>$ cd ~/singularity-images
$ module load singularity
$ singularity pull docker://nvcr.io/hpc/gromacs:2016.4
</code></pre>

<p>The above should take a few seconds to complete.</p>
<h3 id="running-gromacs-interactively">Running GROMACS interactively</h3>
<p>As an example,
we'll consider running the GROMACS ADH benchmark.</p>
<p>First, request an interactive job:</p>
<pre><code>$ qsub -I -l select=1:ngpus=2:ncpus=16:mpiprocs=16:mem=120gb:gpu_model=p100,walltime=5:00:00
</code></pre>

<p>Load the singularity module:</p>
<pre><code>$ module load singularity
</code></pre>

<p>Prepare the input and output directories:</p>
<p>In this case,
we use <code>/scratch3/$USER/gromacs_ADH</code>
as both the input directory (containing the data)
and the output directory (where output will be stored).</p>
<p>Remember that <code>$TMPDIR</code> is cleaned up after the job completes,
so it's important to move any data out of <code>$TMPDIR</code> after GROMACS completes running.
Also, in this case, we are downloading the input data from the web,
but if your input is stored in the <code>/home</code>/, <code>/scratch</code> or some other directory,
then you can copy it to <code>$TMPDIR</code>:</p>
<pre><code>$ mkdir -p /scratch3/$USER/gromacs_ADH_benchmark
$ cd /scratch3/$USER/gromacs_ADH_benchmark
$ wget ftp://ftp.gromacs.org/pub/benchmarks/ADH_bench_systems.tar.gz
$ tar -xvf ADH_bench_systems.tar.gz
</code></pre>

<p>Use <code>singularity shell</code> to interactively run a container
from the downloaded GROMACS image.
The <code>-B</code> switch is used to "bind" directories on the host (Palmetto compute node)
to directories in the container.
The different bindings used are:</p>
<ul>
<li><code>/scratch3/$USER/gromacs_ADH_benchmark</code> is bound to <code>/input</code></li>
<li><code>/scratch3/$USER/gromacs_ADH_benchmark</code> is bound to <code>/output</code></li>
<li><code>$TMPDIR</code> is bound to <code>/work</code></li>
</ul>
<pre><code>$ singularity shell --nv \
    -B /scratch3/$USER/gromacs_ADH_benchmark:/input \
    -B /scratch3/$USER/gromacs_ADH_benchmark:/output \
    -B $TMPDIR:/work \
    --pwd /work \
    ~/singularity-images/gromacs-2016.4.simg \
</code></pre>

<p>Running the benchmark:</p>
<pre><code>$ source /opt/gromacs/install/2016.4/bin/GMXRC
$ gmx_mpi grompp -f /input/adh_cubic/pme_verlet.mdp -c /input/adh_cubic/conf.gro -p /input/adh_cubic/topol.top 
$ export OMP_NUM_THREADS=8
$ mpirun -np 2 gmx_mpi mdrun -g adh_cubic.log -pin on -resethway -v -noconfout -nsteps 10000 -s topol.tpr -ntomp $OMP_NUM_THREADS
</code></pre>

<p>After the last command above completes,
the <code>.edr</code> and <code>.log</code> files produced by GROMACS should be visible.
Typically, the next step is to copy these results to the 
output directory:</p>
<pre><code>$ cp *.log *.edr /output
$ exit
</code></pre>

<p>Upon exiting the container,
the <code>.log</code> and <code>.edr</code> files will be found in the output directory,
<code>/scratch3/$USER/gromacs_ADH_benchmark</code>.</p>
<h3 id="running-gromacs-in-batch-mode">Running GROMACS in batch mode</h3>
<p>The same benchmark can be run in batch mode by
encapsulating the commands in a script <code>run_adh.sh</code>,
and running it non-interactively using <code>singularity exec</code>.</p>
<pre><code># run_adh.sh

source /opt/gromacs/install/2016.4/bin/GMXRC
gmx_mpi grompp -f /input/adh_cubic/pme_verlet.mdp -c /input/adh_cubic/conf.gro -p /input/adh_cubic/topol.top 
export OMP_NUM_THREADS=8
mpirun -np 2 gmx_mpi mdrun -g adh_cubic.log -pin on -resethway -v -noconfout -nsteps 10000 -s topol.tpr -ntomp $OMP_NUM_THREADS
cp *.log *.edr /output
</code></pre>

<p>The PBS batch script for submitting the above
(assuming that the <code>/scratch3/$USER/gromacs_ADH_benchmark</code> directory already contains the input files):</p>
<pre><code>#PBS -N adh_cubic
#PBS -l select=1:ngpus=2:ncpus=16:mem=20gb:gpu_model=p100,walltime=5:00:00

module load singularity

cd $PBS_O_WORKDIR

cp run_adh.sh $TMPDIR

singularity exec --nv \
    -B /scratch3/$USER/gromacs_ADH_benchmark:/input \
    -B /scratch3/$USER/gromacs_ADH_benchmark:/output \
    -B $TMPDIR:/work \
    --pwd /work \
    ~/singularity-images/gromacs-2016.4.simg bash run_adh.sh
</code></pre>

<h2 id="hoomd">HOOMD</h2>
<h3 id="run-hoomd">Run HOOMD</h3>
<p>Now the HOOMD-BLUE v2.3.5 has been installed. Create a simple python file “test_hoomd.py” to run HOOMD </p>
<pre><code>import hoomd
import hoomd.md
hoomd.context.initialize(&quot;&quot;);
hoomd.init.create_lattice(unitcell=hoomd.lattice.sc(a=2.0), n=5);
nl = hoomd.md.nlist.cell();
lj = hoomd.md.pair.lj(r_cut=2.5, nlist=nl);
lj.pair_coeff.set('A', 'A', epsilon=1.0, sigma=1.0);
hoomd.md.integrate.mode_standard(dt=0.005);
all = hoomd.group.all();
hoomd.md.integrate.langevin(group=all, kT=0.2, seed=42);hoomd.analyze.log(filename=&quot;log-output.log&quot;,
                  quantities=['potential_energy', 'temperature'],
                  period=100,
                  overwrite=True);
hoomd.dump.gsd(&quot;trajectory.gsd&quot;, period=2e3, group=all, overwrite=True);
hoomd.run(1e4);
</code></pre>

<p>If you have logged out of the node, request an interactive session on a GPU node and add required modules:</p>
<pre><code>$ qsub -I -l select=1:ncpus=16:mem=64gb:ngpus=2:gpu_model=p100:mpiprocs=16:interconnect=fdr,walltime=2:00:00
$ module add anaconda3/5.1.0 gcc/5.4.0 cuda-toolkit/9.0.176
</code></pre>

<p>Run the script interactively:</p>
<pre><code>$ python test_hoomd.py
</code></pre>

<p>Alternatively, you can setup a PBS job script to run HOOMD in batch mode. A sample is below for <em>Test_Hoomd.sh</em>:</p>
<pre><code>#PBS -N HOOMD
#PBS -l select=1:ncpus=16:mem=64gb:ngpus=2:gpu_model=p100:mpiprocs=16:interconnect=fdr,walltime=02:00:00
#PBS -j oe

module purge
module add anaconda3/5.1.0 gcc/5.4.0 cuda-toolkit/9.0.176

cd $PBS_O_WORKDIR
python test_hoomd.py
</code></pre>

<p>Submit the job:</p>
<pre><code>$ qsub Test_Hoomd.sh
</code></pre>

<p>This is it</p>
<h2 id="java">Java</h2>
<p>The Java Runtime Environment (JRE) version 1.6.0_11 is currently available cluster-wide on Palmetto. 
If a user needs a different version of Java, or if the Java Development Kit (JDK, which includes the JRE) 
is needed, that user is encouraged to download and install Java (JRE or JDK) for herself. Below is a brief 
overview of installing the JDK in a user's /home directory.</p>
<h3 id="jre-vs-jdk">JRE vs. JDK</h3>
<p>The JRE is basically the Java Virtual Machine (Java VM) that provides a platform for running your 
Java programs. The JDK is the fully featured Software Development Kit for Java, including the JRE, 
compilers, and tools like JavaDoc and Java Debugger used to create and compile programs.</p>
<p>Usually, when you only care about running Java programs, the JRE is all you'll need. If you are planning 
to do some Java programming, you will need the JDK.</p>
<h3 id="downloading-the-jdk">Downloading the JDK</h3>
<p>The JDK cannot be downloaded directly using the wget utility because a user must agree to Oracle's 
Java license ageement when downloading.  So, download the JDK using a web browser and transfer the 
downloaded  <code>jdk-7uXX-linux-x64.tar.gz</code>  file to your <code>/home</code> directory on Palmetto using <code>scp</code>, <code>sftp</code>, 
or FileZilla:</p>
<pre><code>scp  jdk-7u45-linux-x64.tar.gz galen@login.palmetto.clemson.edu:/home/galen
jdk-7u45-linux-x64.tar.gz                                                             100%  132MB  57.7KB/s   38:58
</code></pre>
<h3 id="installing-the-jdk">Installing the JDK</h3>
<p>The JDK is distributed in a Linux x86_64 compatible binary format, so once it has been unpacked, it 
is ready to use (no need to compile).  However, you will need to setup your environment for using this 
new package by adding lines similar to the following at the end of your <code>~/.bashrc</code> file:</p>
<pre><code>export JAVA_HOME=/home/galen/jdk1.7.0_45
export PATH=$JAVA_HOME/bin:$PATH
export MANPATH=$JAVA_HOME/man:$MANPATH
</code></pre>
<p>Once this is done, you can log-out and log-in again or simply source your <code>~/.bashrc</code> file and then 
you'll be ready to begin using your new Java installation.</p>
<h2 id="julia">Julia</h2>
<p>Julia: high-level dynamic programming language that was originally designed to address the needs of high-performance numerical analysis and computational science.</p>
<h3 id="run-julia-in-palmetto-interactive">Run Julia in Palmetto: Interactive</h3>
<p>There are a few different versions of Julia available on the cluster.</p>
<pre><code>$ module avail julia
--------------------------------------------- /software/modulefiles ---------------------------------------------
julia/0.6.2 julia/0.7.0 julia/1.0.4 julia/1.1.1
</code></pre>

<p>Let demonstrate how to use julia/1.1.1 in the Palmetto cluster together with Gurobi Optimizer (a commercial optimization solver for linear programming),
quadratic programming, etc. Clemson University has different version of licenses for Gurobi solver.
In this example, I would like to use Julia and Gurobi solver to solve a linear math problem using Palmetto HPC</p>
<pre><code>Problems: Maximize x+y
Given the following constrains:

50 x + 24 y &lt;= 2400
30 x + 33 y &lt;= 2100
x &gt;= 5, y &gt;= 45
</code></pre>

<p>Let prepare a script to solve this problem, named: jump_gurobi.jl.
You can save this file to: /scratch1/$username/Julia/</p>
<pre><code># Request for a compute node:
$ qsub -I -l select=1:ncpus=8:mem=16gb:interconnect=fdr,walltime=01:00:00
# Go to working folder:
$ cd /scratch1/$username/Julia
$ nano jump_gurobi.jl
</code></pre>

<p>Then type/copy the following code to the file jump_gurobi.jl</p>
<pre><code>import Pkg
using JuMP
using Gurobi

m = Model(with_optimizer(Gurobi.Optimizer))

@variable(m, x &gt;= 5)
@variable(m, y &gt;= 45)

@objective(m, Max, x + y)
@constraint(m, 50x + 24y &lt;= 2400)
@constraint(m, 30x + 33y &lt;= 2100)

status = optimize!(m)
println(&quot; x = &quot;, JuMP.value(x), &quot; y = &quot;, JuMP.value(y))
</code></pre>

<p>Save the jump_gurobi.jl file then you are ready to run julia:</p>
<pre><code>$ module add julia/1.1.1 gurobi/7.0.2
$ julia
# the julia prompt appears:
julia&gt; 

# Next install Package: JuMP and Gurobi

julia&gt; using Pkg
julia&gt; Pkg.add(&quot;JuMP&quot;)
julia&gt; Pkg.add(&quot;Gurobi&quot;)
julia&gt; exit()

Run the julia_gurobi.jl script:

$ julia jump_gurobi.jl
</code></pre>

<h3 id="run-julia-in-palmetto-batch-mode">Run Julia in Palmetto: Batch mode</h3>
<ul>
<li>Alternatively, you can setup a PBS job script to run Julia in batch mode. A sample is below for <em>submit_julia.sh</em>:
<strong>You must install the JuMP and Gurobi package first (one time installation)</strong></li>
</ul>
<pre><code>#!/bin/bash
#PBS -N Julia
#PBS -l select=1:ncpus=8:mem=16gb:interconnect=fdr
#PBS -l walltime=02:00:00
#PBS -j oe

module purge
module add julia/1.1.1 gurobi/7.0.2

cd $PBS_O_WORKDIR
julia jump_gurobi.jl &gt; output_JuMP.txt
</code></pre>

<p>Submit the job:</p>
<p><code>$ qsub submit_julia.sh</code></p>
<p>The output file can be found at the same folder: output_JuMP.txt</p>
<h3 id="install-your-own-julia-package-using-conda-environment-and-running-in-jupyterhub">Install your own Julia package using conda environment and running in Jupyterhub</h3>
<p>In addition to traditional compilation of Julia, it is possible to install your own version of Julia and setup kernel to work using Jupterhub.</p>
<pre><code># Request for a compute node:
$ qsub -I -l select=1:ncpus=8:mem=16gb:interconnect=fdr,walltime=01:00:00
$ module add anaconda3/5.1.0
# Create conda environment with the name as &quot;Julia&quot;
$ conda create -n Julia -c conda-forge julia
$ source activate Julia
(Julia) [$username@node1234 ~]$
(Julia) [$username@node1234 ~]$ julia
julia&gt; 
julia&gt; using Pkg
julia&gt; Pkg.add(&quot;IJulia&quot;)
julia&gt; exit
</code></pre>

<p>Exit Julia and Start Jupyterhub in Palmetto
After spawning, Click on New kernel in Jupyterhub, you will see <strong>Julia 1.1.1</strong> kernel available for use</p>
<p>Type in the follwing code to test:</p>
<pre><code>println(&quot;Hello world&quot;)
</code></pre>

<h2 id="lammps">LAMMPS</h2>
<p>There are a few different versions of LAMMPS available on the cluster,
but users are encouraged to install their own version of LAMMPS
in case newer versions different configurations are desired.
In particular, there are two main components of LAMMPS which can be run with CPUs (lmp_mpi) or with GPUs (lmp)</p>
<pre><code>$ module avail lammps

lammps/10Jan15-dp     lammps/17Dec13-dp     lammps/17Dec13-dp-k20 lammps/2018Dec12      lammps/29Aug14-sp-k20
</code></pre>

<h3 id="installing-lammps-on-palmetto-cluster">Installing LAMMPS on Palmetto cluster</h3>
<p>In this example, we will demonstrate installing LAMMPS to run with CPUs and GPUs (version <code>7 Aug 2019</code>).
Detail information can be found here: https://lammps.sandia.gov/doc/Install.html</p>
<ol>
<li>
<p>After logging in, ask for an interactive session (with GPU):</p>
<p>~~~
$ qsub -I -l select=1:ncpus=8:mpiprocs=8:ngpus=1:mem=64gb:gpu_model=v100,walltime=8:00:00
~~~</p>
</li>
<li>
<p>Load the required modules. Specifically, note that we are loading the CUDA-enabled
module:</p>
<p>~~~~
$ module load openmpi/3.1.4-gcc71-ucx fftw/3.3.4-g481 cuda-toolkit/9.2 cmake/3.13.1
~~~~</p>
</li>
<li>
<p>Download the LAMMPS source code from http://lammps.sandia.gov/download.html.
The detailed instruction on usage and compilation options are available 
at http://lammps.sandia.gov/doc/Manual.html.</p>
</li>
<li>
<p>Unpack the source code and enter the package directory:</p>
<p>~~~
$ tar -xvf lammps-stable.tar.gz
$ cd lammps-7Aug19
~~~</p>
</li>
</ol>
<p>There are two different methods of compiling LAMMPS using <strong>make</strong> (support CPUs) or <strong>cmake</strong> (support GPUs). We introduce both methods here:</p>
<ol>
<li>
<p>Compile LAMMPS using <strong>make</strong> with <strong>KOKKOS</strong>  support</p>
<p>~~~
$ cd src
$ make yes-kokkos 
$ make mpi -j8 
~~~</p>
<p>A new executable file <strong>lmp_mpi</strong> will be produced in the <em>src</em> folder. You can copy this executable over to the working directory of your jobs. Note that the size of the executable may be a hundred MB</p>
<p>Note: there are optional packages which can be installed together with the above package using <em>make yes-</em> prior to running <em>make mpi</em>. For example: </p>
<p>~~~
$ make yes-USER-MISC
$ make yes-KSPACE
$ make yes-MOLECULE
$ make yes-MISC
$ make yes-manybody
$ make yes-dipole
$ make yes-class2
$ make yes-asphere
$ make yes-replica
$ make yes-granular
$ make yes-rigid
~~~</p>
</li>
<li>
<p>Compile LAMMPS using <strong>cmake</strong> with <strong>GPU</strong> support:
    The detailed manual can be found <a href="https://github.com/lammps/lammps/blob/master/cmake/README.md">here</a> with more information on the options to be included. In this instruction, we only cover a certain number of options that are enough install cmake with GPU:</p>
<p>~~~
$ mkdir build
$ cd build
$ cmake ../cmake/ -DPKG_GPU=on -DGPU_API=cuda -DGPU_PREC=double -DGPU_ARCH=sm_70 -DCUDA_CUDA_LIBRARY=/usr/lib64/libcuda.so -DFFMPEG_EXECUTABLE=/software/ffmpeg/3.3.2/bin/ffmpeg -DPKG_MC=on -DPKG_REPLICA=on -DPKG_SNAP=on -DPKG-MANYBODY=on -DPKG_USER-MEAMC=on -DPKG_USER-MISC=on -DPKG_MISC=on
$ make -j8
~~~</p>
<p>In the above script; the <em>-DGPU_ARCH=sm_70</em> was used because the gpu_model was set at Volta (v100). Please refer to the above <a href="https://github.com/lammps/lammps/blob/master/cmake/README.md">guideline</a> for setting particular gpu architecture</p>
<p>Note: there are optional packages which can be installed together with the above package using <em>-D[Options]=on</em>. For example: </p>
<p>~~~
$ cmake ../cmake/ -DPKG_MISC=on -DPKG_KSPACE=on -DPKG_MOLECULE=on -DPKG_MANYBODY=on -DPKG_DIPOLE=on -DPKG_CLASS2=on -DPKG_ASPHERE=on -DPKG_REPLICA=on -DPKG_GRANULAR=on -DPKG_RIGID=on
~~~</p>
</li>
</ol>
<p>The executable file <strong>lmp</strong> will be produced. You can copy this executable over to the working directory of your jobs.
Note that the size of the executable may be a hundred MB.</p>
<h3 id="running-lammps-an-example">Running LAMMPS - an example</h3>
<p>Several existing examples are in the installed folder: <em>lammps-7Aug19/examples/</em>
Detailes description of all examples are <a href="https://lammps.sandia.gov/doc/Examples.html#">here</a>.
In order to run the example, simply copy the executable files created from step 5 and 6 to particular example folder and follow the <strong>README</strong> for detailed description on how to run.</p>
<p>For instance, in order to run an example <em>accelerate</em> using GPU package. Copy <strong>lmp</strong> to that folder.
Here is a sample batch script <code>job.sh</code> for this example:</p>
<pre><code>#PBS -N accelerate 
#PBS -l select=1:ncpus=8:mpiprocs=8:ngpus=1:gpu_model=v100:mem=64gb,walltime=1:00:00
#PBS -j oe

module purge
module load openmpi/3.1.4-gcc71-ucx

mpirun -np 8 lmp -sf gpu &lt; in.lj        # 8 MPI, 8 MPI/GPU
</code></pre>

<h2 id="matlab">MATLAB</h2>
<h3 id="checking-license-usage-for-matlab">Checking license usage for MATLAB</h3>
<p>You can check the availability of MATLAB licenses
using the <code>lmstat</code> command:</p>
<pre><code>$ /software/USR_LOCAL/flexlm/lmstat -a -c /software/USR_LOCAL/flexlm/licenses/matlab.dat 
</code></pre>

<h3 id="running-the-matlab-graphical-interface">Running the MATLAB graphical interface</h3>
<p>To launch the MATLAB graphical interface, you must first
you must <a href="{{site.baseurl}}/userguide_howto_run_graphical_applications.html">log-in with tunneling enabled</a>,
and then ask for an interactive session:</p>
<pre><code>$ qsub -I -X -l select=1:ncpus=2:mem=24gb,walltime=1:00:00
</code></pre>

<p>Once logged-in, you must load one of the MATLAB modules:</p>
<pre><code>$ module add matlab/2018b
</code></pre>

<p>And then launch the MATLAB program:</p>
<pre><code>$ matlab
</code></pre>

<p><strong>Warning</strong>: DO NOT attempt to run MATLAB right after
logging-in (i.e., on the <code>login001</code> node).
Always ask for an interactive job first.
MATLAB sessions are automatically killed on the login node.</p>
<h3 id="running-the-matlab-command-line-without-graphics">Running the MATLAB command line without graphics</h3>
<p>To use the MATLAB command-line interface without graphics,
you can additionally use the <code>-nodisplay</code> and <code>-nosplash</code> options:</p>
<pre><code>$ matlab -nodisplay -nosplash
</code></pre>

<p>To quit matlab command-line interface, type:</p>
<pre><code>$ exit
</code></pre>

<h3 id="matlab-in-batch-jobs">MATLAB in batch jobs</h3>
<p>To use MATLAB in your batch jobs,
you can use the <code>-r</code> switch provided by MATLAB,
which lets you <strong>r</strong>un commands specified on the command-line.
For example:</p>
<pre><code>$ matlab -nodisplay -nosplash -r myscript
</code></pre>

<p>will run the MATLAB script <code>myscript.m</code>, 
Or:</p>
<pre><code>$ matlab -nodisplay -nosplash &lt; myscript.m &gt; myscript_results.txt
</code></pre>

<p>will run the MATLAB script <code>myscript.m</code> and write the output to <em>myscript_results.txt</em> file.
Thus, an example batch job using MATLAB could have
a batch script as follows:</p>
<pre><code>#!/bin/bash
#
#PBS -N test_matlab
#PBS -l select=1:ncpus=1:mem=5gb
#PBS -l walltime=1:00:00

module add matlab/2018b

cd $PBS_O_WORKDIR

taskset -c 0-$(($OMP_NUM_THREADS-1)) matlab -nodisplay -nosplash &lt; myscript.m &gt; myscript_results.txt
</code></pre>

<p><strong>Note</strong>: MATLAB will sometimes attemps to use all available
CPU cores on the node it is running on.
If you haven't reserved all cores on the node,
your job may be killed if this happens.
To avoid this, you can use the <code>taskset</code> utility to
set the "core affinity" (as shown above).
As an example:</p>
<pre><code>$ taskset 0-2 &lt;application&gt;
</code></pre>

<p>will limit <code>application</code> to using 3 CPU cores.
On Palmetto,
the variable <code>OMP_NUM_THREADS</code> is automatically set to be the
number of cores requested for a job.
Thus, you can use <code>0-$((OMP_NUM_THREADS-1))</code> as shown
in the above batch script to use all the cores you requested.</p>
<h3 id="compiling-matlab-code-to-create-an-executable">Compiling MATLAB code to create an executable</h3>
<p>Often, you need to run a large number of MATLAB jobs
concurrently (e.g,m each job operating on different data).
In such cases, you can avoid over-utilizing MATLAB licenses
by <em>compiling</em> your MATLAB code into an executable.
This can be done from within the MATLAB command-line as follows:</p>
<pre><code>$ matlab -nodisplay -nosplash
&gt;&gt; mcc -R -nodisplay -R -singleCompThread -R -nojvm -m mycode.m
</code></pre>

<p>Note: MATLAB will try to use all the available CPU cores
on the system where it is running, and this presents a problem
when your compiled executable on the cluster where available
cores on a single node might be shared amongst mulitple users.
You can disable this "feature" when you compile your code by
adding the <code>-R -singleCompThread</code> option, as shown above.</p>
<p>The above command will produce the executable <code>mycode</code>, corresponding
to the M-file <code>mycode.m</code>. If you have multiple M-files in your project
and want to create a single excutable, you can use
a command like the following:</p>
<pre><code>&gt;&gt; mcc -R -nodisplay -R -singleCompThread -R -nojvm -m my_main_code.m myfunction1.m myfunction2.m myfunction3.m
</code></pre>

<p>Once the executable is produced,
you can run it like any other executable in your batch jobs.
Of course, you'll also need the same <code>matlab</code> and
(optional) GCC module loaded for your job's runtime environment.</p>
<h2 id="mothur">mothur</h2>
<p>This page instructs how to install mothur software to Palmetto
The code and the issue tracker can be found <a href="https://github.com/mothur/mothur/blob/master/INSTALL.md">here</a></p>
<ol>
<li>Request an interactive session. For example:</li>
</ol>
<p><code>$ qsub -I -l select=1:ncpus=6:mem=24gb:interconnect=fdr,walltime=3:00:00</code></p>
<ol>
<li>Load the required modules</li>
</ol>
<p><code>$ module load boost/1.65.1 hdf5/1.10.1 openmpi/1.10.3 gcc/8.2.0</code></p>
<ol>
<li>Download mothur from <a href="https://github.com/mothur/mothur/releases/tag/v1.41.3">source</a>. Here we download the latest version 1.41.3</li>
</ol>
<p><code>$ wget https://github.com/mothur/mothur/archive/v1.41.3.tar.gz</code></p>
<ol>
<li>Unpack the downloaded file and go into mothur source folder</li>
</ol>
<p><code>$ tar -xvf v1.41.3.tar.gz
   $ cd mothur-1.41.3</code></p>
<ol>
<li>Modify the Makefile</li>
</ol>
<p><code>$ nano Makefile
   #Replace the following lines 22-32 with information: (Note: username should be replaced)
    OPTIMIZE ?= yes
    USEREADLINE ?= yes
    USEBOOST ?= yes
    USEHDF5 ?= yes
    LOGFILE_NAME ?= no
    BOOST_LIBRARY_DIR ?= "/software/boost/1.65.1/lib"
    BOOST_INCLUDE_DIR ?= "/software/boost/1.65.1/include"
    HDF5_LIBRARY_DIR ?= "/software/hdf5/1.10.1/lib"
    HDF5_INCLUDE_DIR ?= "/software/hdf5/1.10.1/include"
    MOTHUR_FILES ?= "\"home/username/application/bin/mothur\""
    VERSION = "\"1.41.3\""</code></p>
<ol>
<li>Run make file</li>
</ol>
<p><code>$ make</code></p>
<ol>
<li>The <em>mothur</em> executable file will be created in the installation folder: <strong>/home/username/applications/bin</strong>.
Make sure you set the correct environment PATH in ~/.bashrc file</li>
</ol>
<p><code>export PATH=$PATH:$HOME/applications/bin</code></p>
<h2 id="mrbayes">MrBayes</h2>
<p><em>MrBayes</em>  is a program for Bayesian inference and model choice across a wide range of phylogenetic and evolutionary models. MrBayes uses Markov chain Monte Carlo (MCMC) methods to estimate the posterior distribution of model parameters.</p>
<h3 id="installing-beagle-library">Installing BEAGLE Library</h3>
<p><strong>BEAGLE</strong> library is supported to dramatic speedups for codon and amino acid models on compatible hardware (NVIDIA graphics cards);
To install MrBayes in your <code>/home</code> directory on Palmetto, you'll need to begin by installing 
the BEAGLE library.  Here are the steps, starting with checking-out the 
source code using Subversion:</p>
<pre><code>[galen@login001 ~]$ cd
[galen@login001 ~]$ svn checkout http://beagle-lib.googlecode.com/svn/trunk/ beagle-setup
[galen@login001 ~]$ cd beagle-setup
[galen@login001 beagle-setup]$ ./autogen.sh
[galen@login001 beagle-setup]$ ./configure --prefix=/home/galen/beagle-lib
[galen@login001 beagle-setup]$ make install
</code></pre>
<p>Once installed, I also needed to add the location of these new libraries to my <code>LD_LIBRARY_PATH</code> 
(this can be done in your <code>~/.bashrc</code> file, or in your PBS job script as I have done at the bottom 
of this section):</p>
<pre><code>[galen@login001 beagle-setup]$ export LD_LIBRARY_PATH=/home/galen/beagle-lib/lib:$LD_LIBRARY_PATH
</code></pre>
<p>Finally, verify that everything is setup properly:</p>
<pre><code>[galen@login001 beagle-setup]$ make check
</code></pre>
<h3 id="installing-mrbayes">Installing MrBayes</h3>
<p>You can build a parallel version of MrBayes using OpenMPI or MPICH2.  I used MPICH2 and I 
included the BEAGLE libraries when compiling MrBayes, so those libraries will also be needed 
whenever I run MrBayes.</p>
<pre><code>[galen@login001 src]$ module add gcc/4.4 mpich2/1.4

[galen@login001 ~]$ wget http://downloads.sourceforge.net/project/mrbayes/mrbayes/3.2.1/mrbayes-3.2.1.tar.gz
[galen@login001 ~]$ tar -zxf mrbayes-3.2.1.tar.gz
[galen@login001 ~]$ cd mrbayes_3.2.1/src
[galen@login001 src]$ export PKG_CONFIG_PATH=/home/galen/beagle-lib/lib/pkgconfig:$PKG_CONFIG_PATH
[galen@login001 src]$ autoconf
[galen@login001 src]$ ./configure --enable-mpi=yes --with-beagle=/home/galen/beagle-lib
[galen@login001 src]$ make
</code></pre>
<p>Here, the <code>mb</code> executable was created in my <code>/home/galen/mrbayes_3.2.1/src</code> directory.</p>
<h3 id="running-mrbayes">Running MrBayes</h3>
<p>When running MrBayes in parallel, you'll need to use 1 processor core for each Markov chain, 
and the default number of chains is 4 (3 heated and 1 that's not heated).</p>
<p>Below is an example MrBayes job that uses one of the example Nexus files included with my 
installation package <code>/home/galen/mrbayes_3.2.1/examples/primates.nex</code>.</p>
<p>My MrBayes input file (mb_input) contains these commands:</p>
<pre><code>begin mrbayes;
  set autoclose=yes nowarn=yes;
  execute primates.nex;
  lset nst=6 rates=gamma;
  mcmc nruns=1 ngen=10000 samplefreq=10 file=primates.nex;
  mcmc file=primates.nex2;
  mcmc file=primates.nex3;
end;
</code></pre>
<p>My PBS job script for running this job in parallel looks like this:</p>
<pre><code>#!/bin/bash
#PBS -N MrBayes
#PBS -l select=1:ncpus=4:mpiprocs=4:mem=6gb:interconnect=1g,walltime=02:00:00
#PBS -j oe

source /etc/profile.d/modules.sh
module purge
module add gcc/4.4 mpich2/1.4

export LD_LIBRARY_PATH=/home/galen/beagle-lib/lib:$LD_LIBRARY_PATH
NCORES=`qstat -xf $PBS_JOBID | grep List.ncpus | sed 's/^.\{26\}//'`
cd $PBS_O_WORKDIR

mpiexec -n $NCORES /home/galen/mrbayes_3.2.1/src/mb mb_input &gt; mb.log
</code></pre>

<h2 id="paraview">Paraview</h2>
<h3 id="using-paraviewgpus-to-visualize-very-large-datasets">Using Paraview+GPUs to visualize very large datasets</h3>
<p>Paraview can also use multiple GPUs on Palmetto cluster
to visualize very large datasets.
For this, Paraview must be run in client-server mode.
The "client" is your local machine on which Paraview must be installed,
and the "server" is the Palmetto cluster on which the computations/rendering is done.</p>
<ol>
<li>
<p>The version of Paraview on the client needs to match
exactly the version of Paraview on the server.
The client must be running Linux.
You can obtain the source code used for installation of Paraview 5.0.1
on Palmetto from <code>/software/paraview/ParaView-v5.0.1-source.tar.gz</code>.
Copy this file to the client, extract it and compile Paraview.
Compilation instructions can be found in the Paraview
<a href="http://www.paraview.org/Wiki/ParaView:Build_And_Install">documentation</a>.</p>
</li>
<li>
<p>You will need to run the Paraview server on Palmetto cluster.
First, log-in with X11 tunneling enabled, and request an interactive session:</p>
</li>
</ol>
<pre><code>$ qsub -I -X -l select=4:ncpus=2:mpiprocs=2:ngpus=2:mem=32gb,walltime=1:00:00
</code></pre>

<p>In the above example, we request 4 nodes with 2 GPUs each.</p>
<ol>
<li>Next, launch the Paraview server:</li>
</ol>
<pre><code>$ module add paraview/5.0
$ export DISPLAY=:0
$ mpiexec -n 8 pvserver -display :0
</code></pre>

<p>The server will be serving on a specific port number (like 11111)
on this node. Note this number down.</p>
<ol>
<li>Next, you will need to set up "port-forwarding" from the lead node
(the node your interactive session is running one) to your local machine.
This can be done by opening a terminal running on the local machine,
and typing the following:</li>
</ol>
<pre><code>$ ssh -L 11111:nodeXYZ:11111 username@login.palmetto.clemson.edu
</code></pre>

<ol>
<li>Once port-forwarding is set up,
you can launch Paraview on your local machine,</li>
</ol>
<h2 id="rclone">rclone</h2>
<p><code>rclone</code> is a command-line program that can be used
to sync files and folders to and from cloud services
such as Google Drive, Amazon S3, Dropbox, and <a href="http://rclone.org/">many others</a>.</p>
<p>In this example,
we will show how to use <code>rclone</code> to sync files to a Google Drive account,
but the official documentation has specific instructions for other services.</p>
<h3 id="setting-up-rclone-for-use-with-google-drive-on-palmetto">Setting up rclone for use with Google Drive on Palmetto</h3>
<p>To use <code>rclone</code> with any of the above cloud storage services,
you must perform a one-time configuration.
You can configure <code>rclone</code> to work with as many services as you like.</p>
<p>For the one-time configuration, you will need to
<a href="{{site.baseurl}}/userguide_howto_run_graphical_applications.html">log-in with tunneling enabled</a>.
Once logged-in, ask for an interactive job:</p>
<pre><code>$ qsub -I -X
</code></pre>

<p>Once the job starts, load the <code>rclone</code> module:</p>
<pre><code>$ module add rclone/1.23
</code></pre>

<p>After <code>rclone</code> is loaded, you must set up a "remote". In this case,
we will configure a remote for Google Drive. You can create and manage a separate
remote for each cloud storage service you want to use.
Start by entering the following command:</p>
<pre><code>$ rclone config

n) New remote
q) Quit config
n/q&gt;
</code></pre>

<p>Hit <strong>n</strong> then Enter to create a new remote host</p>
<pre><code>name&gt;
</code></pre>

<p>Provide any name for this remote host. For example: <strong>gmaildrive</strong></p>
<pre><code>What type of source is it?
Choose a number from below
 1) amazon cloud drive
 2) drive
 3) dropbox
 4) google cloud storage
 5) local
 6) s3
 7) swift
type&gt;
</code></pre>

<p>Provide any number for the remote source. For example choose number <strong>2</strong> for goolge drive.</p>
<pre><code>Google Application Client Id - leave blank normally.
client_id&gt; # Enter to leave blank
Google Application Client Secret - leave blank normally.
client_secret&gt; # Enter to leave blank

Remote config
Use auto config?
 * Say Y if not sure
 * Say N if you are working on a remote or headless machine or Y didn't work
y) Yes
n) No
y/n&gt;
</code></pre>

<p>Use <strong>y</strong> if you are not sure</p>
<pre><code>If your browser doesn't open automatically go to the following link: http://127.0.0.1:53682/auth
Log in and authorize rclone for access
Waiting for code...
</code></pre>

<p>This will open the Firefox web browser, allowing you to log-in to your Google account.
Enter your username and password then accept to let <strong>rclone</strong> access your Goolge drive.
Once this is done, the browser will ask you to go back to rclone to continue.</p>
<pre><code>Got code
--------------------
[gmaildrive]
client_id =
client_secret =
token = {&quot;access_token&quot;:&quot;xyz&quot;,&quot;token_type&quot;:&quot;Bearer&quot;,&quot;refresh_token&quot;:&quot;xyz&quot;,&quot;expiry&quot;:&quot;yyyy-mm-ddThh:mm:ss&quot;}
--------------------
y) Yes this is OK
e) Edit this remote
d) Delete this remote
y/e/d&gt;
</code></pre>

<p>Select <strong>y</strong> to finish configure this remote host.
The <strong>gmaildrive</strong> host will then be created.</p>
<pre><code>Current remotes:

Name                 Type
====                 ====
gmaildrive           drive

e) Edit existing remote
n) New remote
d) Delete remote
q) Quit config
e/n/d/q&gt;
</code></pre>

<p>After this, you can quit the config using <strong>q</strong>, kill the job and exit this <code>ssh</code> session:</p>
<pre><code>$ exit
</code></pre>

<h3 id="using-rclone">Using rclone</h3>
<p>Whenever transfering files (using <code>rclone</code> or otherwise), login to the transfer node <code>xfer01-ext.palmetto.clemson.edu</code>.</p>
<ul>
<li>In MobaXterm, create a new ssh session with <strong>xfer01-ext.palmetto.clemson.edu</strong> as the Remote host</li>
<li>In MacOS, open new terminal and <code>ssh user@xfer01-ext.palmetto.clemson.edu</code></li>
</ul>
<p>Once logged-in, load the <code>rclone</code> module:</p>
<pre><code>$ module add rclone/1.23
</code></pre>

<p>You can check the content of the remote host <strong>gmaildrive</strong>:</p>
<pre><code>$ rclone ls gmaildrive:
$ rclone lsd gmaildrive: 
</code></pre>

<p>You can use <code>rclone</code> to (for example) copy a file from Palmetto to any folder in your Google Drive:</p>
<pre><code>$ rclone copy /path/to/file/on/palmetto gmaildrive:/path/to/folder/on/drive
</code></pre>

<p>Or if you want to copy to a specific destination on Google Drive back to Palmetto:</p>
<pre><code>$ rclone copy gmaildrive:/path/to/folder/on/drive /path/to/file/on/palmetto
</code></pre>

<p>Additional <code>rclone</code> commands can be found <a href="http://rclone.org/docs/">here</a>.</p>
<h2 id="singularity">Singularity</h2>
<p><a href="https://www.sylabs.io/">Singularity</a>
is a tool for creating and running
<a href="https://en.wikipedia.org/wiki/Operating-system-level_virtualization">containers</a>
on HPC systems,
similar to <a href="https://www.docker.com/">Docker</a>.</p>
<p>For further information on Singularity,
and on downloading, building and running containers with Singularity,
please refer to the <a href="https://www.sylabs.io/docs/">Singularity documentation</a>.
This page provides information about singularity specific to the Palmetto cluster.</p>
<h3 id="running-singularity">Running Singularity</h3>
<p>Singularity is installed on all of the Palmetto compute nodes and the Palmetto LoginVMs, but it <strong>IS NOT</strong> present on the login.palmetto.clemson.edu node.</p>
<p>To run singularity, you may simply run <code>singularity</code> or more specifically <code>/bin/singularity</code>.</p>
<p>e.g.</p>
<pre><code>$ singularity --version
singularity version 3.5.3-1.el7
</code></pre>

<h3 id="an-important-change-for-existing-singularity-users">An important change for existing singularity users</h3>
<p>Formerly, Palmetto administrators had installed singularity as a "software module" on Palmetto, but that is no longer the case. If your job scripts have any statements that use the singularity module, then those statements will need to be completely removed; otherwise, your job script may error.</p>
<p>Remove any statements from your job scripts that resemble the following lines:</p>
<pre><code>module &lt;some_command&gt; singularity
</code></pre>

<h3 id="where-to-download-containers">Where to download containers</h3>
<p>Containers can be downloaded from DockerHub</p>
<ul>
<li>
<p><a href="https://hub.docker.com/">DockerHub</a>
contains containers for various software packages,
and Singularity is
<a href="https://sylabs.io/guides/3.5/user-guide/singularity_and_docker.html">compatible with Docker images</a>.</p>
</li>
<li>
<p><a href="https://singularity-hub.org/">SingularityHub</a></p>
</li>
<li>
<p>The <a href="https://ngc.nvidia.com/signin/email">NVIDIA GPU Cloud</a> for GPU-optimized images.</p>
</li>
<li>
<p>Many individual projects contain specific instructions for installation via
Docker and/or Singularity, and may host pre-built images in other locations.</p>
</li>
</ul>
<h3 id="example-running-openfoam-using-singularity">Example: Running OpenFOAM using Singularity</h3>
<p>As an example, we consider installing and running the OpenFOAM CFD solver using Singularity.
OpenFOAM can be quite difficult to install manually,
but singularity makes it very easy.
This example shows how to use singularity interactively,
but singularity containers can be run in batch jobs as well.</p>
<p>Start by requesting an interactive job.</p>
<p>NOTE: Singularity can only be run on the compute nodes and Palmetto Login VMs:</p>
<pre><code>$ qsub -I -l select=1:ngpus=2:ncpus=16:mpiprocs=16:mem=120gb,walltime=5:00:00
</code></pre>

<p>We recommend that all users store built singularity images
in their <code>/home</code> directories.
Singularity images can be quite large,
so be sure to delete unused or old images:</p>
<pre><code>$ mkdir ~/singularity-images
$ cd ~/singularity-images
</code></pre>

<p>Next, we download the singularity image for OpenFOAM from DockerHub.
This takes a few seconds to complete:</p>
<pre><code>$ singularity pull docker://openfoam/openfoam6-paraview54
</code></pre>

<p>Once the image is downloaded,
we are ready to run OpenFOAM.
We use <code>singularity shell</code> to start a container,
and run a shell in the container.</p>
<p>The <code>-B</code> option is used to "bind" the <code>/scratch2/$USER</code> directory
to a directory named <code>/scratch</code> in the container.</p>
<p>We also the <code>--pwd</code> option to specify the working directory in the running container
(in this case <code>/scratch</code>).
This is <strong>always</strong> recommended.</p>
<p>Typically, the working directory may be the $TMPDIR directory or
one of the scratch directories.</p>
<pre><code>$ singularity shell -B /scratch2/atrikut:/scratch --pwd /scratch openfoam6-paraview54.simg
</code></pre>

<p>Before running OpenFOAM commands, we need to source a few environment variables
(this step is specific to OpenFOAM):</p>
<pre><code>$ source /opt/openfoam6/etc/bashrc
</code></pre>

<p>Now, we are ready to run a simple example using OpenFOAM:</p>
<pre><code>$ cp -r $FOAM_TUTORIALS/incompressible/simpleFoam/pitzDaily .
$ cd pitzDaily
$ blockMesh
$ simpleFoam
</code></pre>

<p>The simulation takes a few seconds to complete,
and should finish with the following output:</p>
<pre><code>smoothSolver:  Solving for Ux, Initial residual = 0.00012056, Final residual = 7.8056e-06, No Iterations 6
smoothSolver:  Solving for Uy, Initial residual = 0.000959834, Final residual = 6.43909e-05, No Iterations 6
GAMG:  Solving for p, Initial residual = 0.00191644, Final residual = 0.000161493, No Iterations 3
time step continuity errors : sum local = 0.00681813, global = -0.000731564, cumulative = 0.941842
smoothSolver:  Solving for epsilon, Initial residual = 0.000137225, Final residual = 8.98917e-06, No Iterations 3
smoothSolver:  Solving for k, Initial residual = 0.000215144, Final residual = 1.30281e-05, No Iterations 4
ExecutionTime = 10.77 s  ClockTime = 11 s


SIMPLE solution converged in 288 iterations

streamLine streamlines write:
    seeded 10 particles
    Tracks:10
    Total samples:11980
    Writing data to &quot;/scratch/pitzDaily/postProcessing/sets/streamlines/288&quot;
End
</code></pre>

<p>We are now ready to exit the container:</p>
<pre><code>$ exit
</code></pre>

<p>Because the directory <code>/scratch</code> was bound to <code>/scratch2/$USER</code>, the simulation output is available in
the directory <code>/scratch2/$USER/pitzDaily/postProcessing/</code>:</p>
<pre><code>$ ls /scratch2/$USER/pitzDaily/postProcessing/
sets
</code></pre>

<h3 id="gpu-enabled-software-using-singularity-containers-nvidia-gpu-cloud">GPU-enabled software using Singularity containers (NVIDIA GPU Cloud)</h3>
<p>Palmetto also supports use of images provided by the <a href="https://www.nvidia.com/en-us/gpu-cloud/">NVIDIA GPU Cloud (NGC)</a>.</p>
<p>The provides GPU-accelerated HPC and deep learning containers for scientific computing.
NVIDIA tests HPC container compatibility with the Singularity runtime through a rigorous QA process.</p>
<h3 id="pulling-ngc-images">Pulling NGC images</h3>
<p>Singularity images may be pulled directly from the Palmetto GPU compute nodes,
an interactive job is most convenient for this.
Singularity uses multiple CPU cores when building the image
and so it is recommended that a minimum of 4 CPU cores are reserved.
For instance to reserve 4 CPU cores, 2 NVIDIA Pascal GPUs, for 20 minutes the following could be used:</p>
<pre><code>$ qsub -I -lselect=1:ncpus=4:mem=2gb:ngpus=2:gpu_model=p100,walltime=00:20:00
</code></pre>

<p>Wait for the interactive job to give you control over the shell.</p>
<p>Before pulling an NGC image, authentication credentials must be set.
This is most easily accomplished by setting the following variables in the build environment.</p>
<pre><code>$ export SINGULARITY_DOCKER_USERNAME='$oauthtoken'
$ export SINGULARITY_DOCKER_PASSWORD=&lt;NVIDIA NGC API key&gt;
</code></pre>

<p>More information describing how to obtain and use your NVIDIA NGC API key can be found
<a href="https://docs.nvidia.com/ngc/ngc-getting-started-guide/index.html">here</a>.</p>
<p>Once credentials are set in the environment,
we’re ready to pull and convert the NGC image to a local Singularity image file.
The general form of this command for NGC HPC images is:</p>
<pre><code>$ singularity build &lt;local_image&gt; docker://nvcr.io/&lt;registry&gt;/&lt;app:tag&gt;
</code></pre>

<p>This singularity build command will download the <code>app:tag</code> NGC Docker image,
convert it to Singularity format,
and save it to the local file named local_image.</p>
<p>For example to pull the namd NGC container tagged with version 2.12-171025
to a local file named <code>namd.simg</code> we can run:</p>
<pre><code>$ singularity build ~/namd.simg docker://nvcr.io/hpc/namd:2.12-171025
</code></pre>

<p>After this command has finished we'll have a Singularity image file, <code>namd.simg</code>:</p>
<h3 id="running-ngc-containers">Running NGC containers</h3>
<p>Running NGC containers on Palmetto presents few differences from the run instructions provided on NGC for each application.
Application-specific information may vary so it is recommended that you follow the
container specific documentation before running with Singularity.
If the container documentation does not include Singularity information,
then the container has not yet been tested under Singularity.</p>
<p>As all NGC containers are optimized for NVIDIA GPU acceleration we will always want to add the <code>--nv</code> flag
to enable NVIDIA GPU support  within the container.</p>
<p>The Singularity command below represents the standard form of the Singularity command used on the Palmetto cluster.
It will mount the present working directory on the host to <code>/host_pwd</code> in the container process and
set the present working directory of the container process to <code>/host_pwd</code>
This means that when our process starts it will be effectively running in the host directory
the singularity command was launched from.</p>
<pre><code>$ singularity exec --nv -B $(pwd):/host_pwd --pwd /host_pwd &lt;image.simg&gt; &lt;cmd&gt;
</code></pre>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../spack/" class="btn btn-neutral float-right" title="User Software Installation">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../../basic/jupyter/" class="btn btn-neutral" title="JupyterLab"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../../basic/jupyter/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../spack/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
