<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Login - Palmetto Documentation</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../../css/theme.css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  <link href="../../css/extra.css" rel="stylesheet" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Login";
    var mkdocs_page_input_path = "basic/login.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Palmetto Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../about/">About</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../news/">News</a>
                    </li>
                </ul>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../owner/">Owner</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Basic</span></p>
                <ul class="current">
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">Login</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#mac-os-x-and-linux-users">Mac OS X and Linux users</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#windows">Windows</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#two-factor-authentication-2fa">Two-Factor Authentication (2FA)</a>
    </li>
    </ul>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../jupyter/">JupyterLab</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Software</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../software/software/">Software on Palmetto</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../../software/spack/">User Software Installation</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../../software/programming/">Software Development</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Training and Outreach</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../training/schedule/">Training Schedule</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../../training/workshop/">Workshop Descriptions</a>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Misc</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../faq/common/">Common Issues</a>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../../faq/faq/">FAQ</a>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Palmetto Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Basic &raquo;</li>
        
      
    
    <li>Login</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h2 id="logging-in">Logging in</h2>
<p>Any user with a Palmetto Cluster account can log-in using
<a href="https://en.wikipedia.org/wiki/Comparison_of_SSH_clients">SSH (Secure Shell)</a>.
Mac OS X and Linux systems come with an SSH client installed,
while Windows users will need to download one.</p>
<h3 id="mac-os-x-and-linux-users">Mac OS X and Linux users</h3>
<p>Mac OS X or Linux users may open a Terminal, and
type in the following command:</p>
<pre><code>$ ssh username@login.palmetto.clemson.edu 
</code></pre>

<p>where <code>username</code> is your Clemson user ID.
You will be prompted for both your password and DUO authentication.</p>
<h3 id="windows">Windows</h3>
<p>MobaXterm is the recommended SSH client for Windows and can be downloaded
from <a href="http://mobaxterm.mobatek.net/download.html">MobaXterm's download page</a>.
This software is recommended because it is free and comes with:</p>
<ul>
<li>A built-in file transfer client, which allows you to exchange files and folders 
between your own computer and Palmetto in a convenient manner.</li>
<li>An X11 server which allows you to run graphical programs on Palmetto cluster</li>
<li>A graphical port-forwarding interface to support easy access to web-based 
programs launched inside Palmetto</li>
</ul>
<p><em>If you select the installation version of MobaXterm, you will need to unzip the downloaded
file before running the installation program. Windows sometimes allow users to run the 
installation from inside the zipped file, resulting in missing an additional utility file (still
inside the zipped file).</em></p>
<p>After downloading and installing MobaXterm, users can log-in by following these steps:</p>
<ol>
<li>
<p>Launch the MobaXterm program</p>
<p><img src="../../image/login/mobaxterm_01.PNG" style="width:1000px"></p>
</li>
<li>
<p>On the top-left corner of MobaXterm, click the <strong>Session</strong> button. Confirm that the 
following settings are set for <strong>Basic SSH settings</strong> and <strong>Advanced SSH settings</strong>:</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>Remote host</td>
<td><code>login.palmetto.clemson.edu</code></td>
</tr>
<tr>
<td>Port</td>
<td>22</td>
</tr>
<tr>
<td>X11-Forwarding</td>
<td>enabled</td>
</tr>
<tr>
<td>Compression</td>
<td>enabled</td>
</tr>
<tr>
<td>Remote environment</td>
<td>Interactive shell</td>
</tr>
<tr>
<td>SSH-browser type</td>
<td><strong>SCP (enhanced speed)</strong></td>
</tr>
</tbody>
</table>
<p><img src="{{site.baseurl}}/images/mobaxterm_02.png" style="width:1000px"></p>
</li>
<li>
<p>Click <strong>OK</strong> and a new session window will be opened, where you will be
    prompted for your Palmetto password and the DUO authentication.</p>
<p><img src="{{site.baseurl}}/images/mobaxterm_03.png" style="width:1000px"></p>
</li>
<li>
<p>After being authenticated, you will login to the <strong>login001</strong> node.</p>
<p><img src="{{site.baseurl}}/images/mobaxterm_04.png" style="width:1000px"></p>
<p>All settings for this session are saved, and for future logins, you
can select this session from the <strong>Recent sessions</strong> form of the main MobaXterm
window as well as the <strong>Saved sessions</strong> tab of the side window. The side
window can be displayed or hidden by clicking on the blue double-arrow sign
on the top left of MobaXterm.</p>
<p><img src="{{site.baseurl}}/images/mobaxterm_05.png" style="width:1000px"></p>
<p><img src="{{site.baseurl}}/images/mobaxterm_06.png" style="width:1000px"></p>
<p>MobaXterm also comes with a built-in file browser and transfer GUI
(SSH-browser). This GUI is accessible via the <strong>SCP</strong> tab of the side
window. Using the Upload (green arrow pointing up) and
and Download (blue arrow pointing down) buttons at the top of the SCP tab,
you can easily transfer files between Palmetto and your local computer.</p>
<p><img src="{{site.baseurl}}/images/mobaxterm_07.png" style="width:1000px"></p>
</li>
</ol>
<h3 id="two-factor-authentication-2fa">Two-Factor Authentication (2FA)</h3>
<p>All connections to Palmetto require 2FA. If you are not enrolled in 2FA yet,
you may enroll using the link <a href="https://2fa.clemson.edu/">https://2fa.clemson.edu/</a>.</p>
<p>After you enter your login name and password, Palmetto will ask you to provide 
additional authentication for 2FA via one of the following three options for registerd devices 
(smart phone or tablet):</p>
<pre><code>Using keyboard-interactive authentication.
Duo two-factor login for $user

Enter a passcode or select one of the following options:

 1. Duo Push to XXX-XXX-XXXX
 2. Phone call to XXX-XXX-XXXX
 3. SMS passcodes to XXX-XXX-XXXX

Passcode or option (1-3):
</code></pre>

<ul>
<li>Option 1: response to Duo Push to your device by clicking <strong>Approve</strong></li>
<li>Option 2: listen to the automatic call from system and select any key on your device.</li>
<li>Option 3: enter a passcode that is shown in your DUO app. </li>
</ul>
<pre><code>Passcode or option (1-3): 1234567
</code></pre>

<p>More information can be found at <a href="https://ccit.clemson.edu/support/current-students/two-factor-authentication-2fa/">here</a></p>
<h2 id="basic-tasks">Basic tasks</h2>
<h3 id="storing-files-and-folders">Storing files and folders</h3>
<h4 id="home-and-scratch-directories">Home and scratch directories</h4>
<p>Various filesystems are available for users to store data.
These differ in capacity, data-persistence, and efficiency,
and it is important that users understand which filesystem
to use under which circumstances.</p>
<table>
<thead>
<tr>
<th>Location</th>
<th>Available space</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>/home/username</code></td>
<td>100 GB per user</td>
<td>Backed-up nightly, permanent storage space accessible from all nodes</td>
</tr>
<tr>
<td><code>/scratch1/username</code></td>
<td>233 TB shared by all users</td>
<td>Not backed up, temporary work space accessible from all nodes, OrangeFS Parallel File System</td>
</tr>
<tr>
<td><code>/scratch2/username</code></td>
<td>160 TB shared by all users</td>
<td>Not backed up, temporary work space accessible from all nodes, XFS</td>
</tr>
<tr>
<td><code>/scratch3/username</code></td>
<td>129 TB shared by all users</td>
<td>Not backed up, temporary work space accessible from all nodes, ZFS</td>
</tr>
<tr>
<td><code>/scratch4/username</code></td>
<td>175 TB shared by all users</td>
<td>Not backed up, temporary work space accessible from all nodes, BeeGFS Parallel File System</td>
</tr>
<tr>
<td><code>/local_scratch</code></td>
<td>Varies between nodes (99GB-800GB)</td>
<td>Per-node temporary work space, accessible only for the lifetime of job</td>
</tr>
</tbody>
</table>
<p>The <code>/home</code> and <code>/scratch</code> directories are shared by all nodes.
In contrast, each node has its own <code>/local_scratch</code> directory.</p>
<p>All data in the <code>/home</code> directory is permanent
(not automatically deleted) and backed-up on a nightly basis.
If you lose data in the <code>/home</code> directory,
it may be possible to recover it if it was previously backed up.</p>
<p>Data in the <code>/scratch</code> directories is <strong>not</strong> backed up,
and any data that is untouched for 30 days is automatically
removed from the <code>/scratch</code> directories.
Data that cannot easily be reproduced should <strong>not</strong> be stored
in the <code>/scratch</code> directories,
and any data that is not required should be
removed as soon as possible.</p>
<p>See <a href="{{site.baseurl}}/userguide_howto_choose_right_filesystem.html">this guide</a>
on how to choose the appropriate filesystem for your work.</p>
<h4 id="temporary-reservations">Temporary reservations</h4>
<p>All users may apply for temporary reservation
of the following resources:</p>
<ol>
<li>Up to 150 TB of long-term storage</li>
<li>Up to 8.5 TB of fast SSD scratch space</li>
</ol>
<p>All requests will be reviewed by Clemson University
Computational Advisory Team (CU-CAT).
Reservation requests can be made <a href="https://citi.sites.clemson.edu/new-reservation/">here</a>.</p>
<h4 id="purchased-storage">Purchased storage</h4>
<p>Users or groups may also
purchase storage in 1 TB increments.
For details about purchased storage, please contact the Palmetto support staff
(<a href="&#109;&#97;&#105;&#108;&#116;&#111;&#58;&#105;&#116;&#104;&#101;&#108;&#112;&#64;&#99;&#108;&#101;&#109;&#115;&#111;&#110;&#46;&#101;&#100;&#117;">&#105;&#116;&#104;&#101;&#108;&#112;&#64;&#99;&#108;&#101;&#109;&#115;&#111;&#110;&#46;&#101;&#100;&#117;</a>, and include the word "Palmetto" in the subject line).</p>
<h3 id="moving-data-in-and-out-of-the-cluster">Moving data in and out of the cluster</h3>
<h4 id="small-files-kilobytes-or-a-few-megabytes">Small files (kilobytes or a few megabytes)</h4>
<p>On Windows machines, using the MobaXterm SSH client,
the built-in file browser can be used (<strong>SCP</strong> tab of the side window).
Using the Upload (green arrow pointing up) and
and Dowload (blue arrow pointing down) buttons at the top of the SCP tab,
you can easily transfer small files between Palmetto and your local computer.</p>
<p><img src="{{site.baseurl}}/images/mobaxterm_07.png" style="width:1000px"></p>
<p>On Unix systems, you can use the <code>scp</code> (secure copy) command to
perform file transfers. The general form of the <code>scp</code> command is:</p>
<pre><code>$ scp &lt;path_to_source&gt; username@xfer01-ext.palmetto.clemson.edu:&lt;path_to_destination&gt;
</code></pre>

<p>For example, here is the <code>scp</code> command to copy a file from the
current directory on your local machine to your
<code>/home/username</code> directory on Palmetto
(this command is entered into a terminal
when <strong>not logged-in to Palmetto</strong>):</p>
<pre><code>$ scp myfile.txt username@xfer01-ext.palmetto.clemson.edu:/home/username
</code></pre>

<p>... and to do the same in reverse,
i.e., copy from Palmetto to your local machine.
(again, from a terminal running on your local machine,
<strong>not</strong> on Palmetto):</p>
<pre><code>$ scp username@xfer01-ext.palmetto.clemson.edu:/home/username/myfile.txt .
</code></pre>

<p>The <code>.</code> represents the working directory on the local machine.</p>
<p>For folders, include the <code>-r</code> switch:</p>
<pre><code>$ scp -r myfolder username@xfer01-ext.palmetto.clemson.edu:/home/username
</code></pre>

<h4 id="transfering-larger-files-more-than-a-few-megabytes">Transfering larger files (more than a few megabytes)</h4>
<p>For larger files, we recommend using the <a href="https://www.globus.org/">Globus</a>
file transfer application. Here, we demonstrate how to use Globus Online
to transfer files between Palmetto and a local machine (laptop).
However, Globus can be used for file transfers to/from other locations as well.</p>
<ol>
<li>
<p>You will need to have a Globus account set up to begin.
    Visit <a href="https://www.globus.org/">https://www.globus.org/</a>
    and set up a Globus account.</p>
</li>
<li>
<p>To begin transfering files,
    navigate to the Globus Online transfer utility
    here:
    <a href="https://www.globus.org/app/transfer">https://www.globus.org/app/transfer</a>.</p>
</li>
<li>
<p>The transfer utility allows you to transfer
    files between "endpoints".
    You will need to set your local machine
    as a Globus Connect Personal Endpoint for the file transfer.
    As a part of this step, you must install the
    Globus Connect Personal application
    (see here:
    <a href="https://www.globus.org/app/endpoints/create-gcp">https://www.globus.org/app/endpoints/create-gcp</a>
    ).
    After installing,
    ensure that the application is running.
    You should then be able to set your local machine as one endpoint.
    In the figure below, the endpoint is named <code>My Personal Mac</code>.</p>
</li>
<li>
<p>As the second endpoint,
    choose <code>clemson#xfer01-ext.clemson.edu</code>.</p>
</li>
<li>
<p>You can now transfer files between any locations on your
    local machine and the Palmetto cluster.</p>
</li>
</ol>
<h3 id="checking-available-compute-hardware">Checking available compute hardware</h3>
<p>The login node <code>login001</code> (the node that users first log-in to),
is not meant for running computationally intensive tasks.
Instead, users must reserve hardware from the <strong>compute nodes</strong>
of the cluster. Currently, Palmetto cluster has over 2020 compute
nodes. The hardware configuration of the different nodes is available
in the file <code>/etc/hardware-table</code>:</p>
<pre><code>[atrikut@login001 ~]$ cat /etc/hardware-table

PALMETTO HARDWARE TABLE      Last updated:  Mar 04 2019

PHASE COUNT  MAKE   MODEL    CHIP(0)                CORES  RAM(1)    /local_scratch   Interconnect     GPUs  PHIs SSD
 0      6    HP     DL580    Intel Xeon    7542       24   505 GB(2)    99 GB         1ge               0     0    0
 0      1    HP     DL980    Intel Xeon    7560       64     2 TB(2)    99 GB         1ge               0     0    0
 0      1    HP     DL560    Intel Xeon    E5-4627v4  40   1.5 TB(2)   881 GB         10ge              0     0    0
 0      1    Dell   R830     Intel Xeon    E5-4627v4  40   1.0 TB(2)   880 GB         10ge              0     0    0
 0      2    HP     DL560    Intel Xeon    6138G      80   1.5 TB(2)   3.6 TB         10ge              0     0    0
 1     75    Dell   PE1950   Intel Xeon    E5345       8    12 GB       37 GB         1g                0     0    0
 2a   158    Dell   PE1950   Intel Xeon    E5410       8    12 GB       37 GB         1g                0     0    0
 2b    84    Dell   PE1950   Intel Xeon    E5410       8    16 GB       37 GB         1g                0     0    0
 3    225    Sun    X2200    AMD   Opteron 2356        8    16 GB      193 GB         1g                0     0    0
 4    326    IBM    DX340    Intel Xeon    E5410       8    16 GB      111 GB         1g                0     0    0
 5a   320    Sun    X6250    Intel Xeon    L5420       8    32 GB       31 GB         1g                0     0    0
 5b     9    Sun    X4150    Intel Xeon    E5410       8    32 GB       99 GB         1g                0     0    0
 6     67    HP     DL165    AMD   Opteron 6176       24    48 GB      193 GB         1g                0     0    0
 7a    42    HP     SL230    Intel Xeon    E5-2665    16    64 GB      240 GB         56g, fdr          0     0    0
 7b    12    HP     SL250s   Intel Xeon    E5-2665    16    64 GB      240 GB         56g, fdr          2(3)  0    0
 8a    71    HP     SL250s   Intel Xeon    E5-2665    16    64 GB      900 GB         56g, fdr          2(4)  0    300 GB(7)
 8b    57    HP     SL250s   Intel Xeon    E5-2665    16    64 GB      420 GB         56g, fdr          2(4)  0    0
 8c    88    Dell   PEC6220  Intel Xeon    E5-2665    16    64 GB      350 GB         10ge              0     0    0
 9     72    HP     SL250s   Intel Xeon    E5-2665    16   128 GB      420 GB         56g, fdr, 10ge    2(4)  0    0
10     80    HP     SL250s   Intel Xeon    E5-2670v2  20   128 GB      800 GB         56g, fdr, 10ge    2(4)  0    0
11a    40    HP     SL250s   Intel Xeon    E5-2670v2  20   128 GB      800 GB         56g, fdr, 10ge    2(6)  0    0
11b     4    HP     SL250s   Intel Xeon    E5-2670v2  20   128 GB      800 GB         56g, fdr, 10ge    0     2(8) 0
12     30    Lenovo NX360M5  Intel Xeon    E5-2680v3  24   128 GB      800 GB         56g, fdr, 10ge    2(6)  0    0
13     24    Dell   C4130    Intel Xeon    E5-2680v3  24   128 GB      1.8 TB         56g, fdr, 10ge    2(6)  0    0
14     12    HP     XL1X0R   Intel Xeon    E5-2680v3  24   128 GB      880 GB         56g, fdr, 10ge    2(6)  0    0
15     32    Dell   C4130    Intel Xeon    E5-2680v3  24   128 GB      1.8 TB         56g, fdr, 10ge    2(6)  0    0
16     40    Dell   C4130    Intel Xeon    E5-2680v4  28   128 GB      1.8 TB         56g, fdr, 10ge    2(9)  0    0
17     20    Dell   C4130    Intel Xeon    E5-2680v4  28   128 GB      1.8 TB         56g, fdr, 10ge    2(9)  0    0
18a     2    Dell   C4140    Intel Xeon    6148G      40   372 GB      1.9 TB(12)     56g, fdr, 25ge    4(10) 0    0
18b    65    Dell   R740     Intel Xeon    6148G      40   372 GB      1.8 TB         56g, fdr, 25ge    2(11) 0    0
18c    10    Dell   R740     Intel Xeon    6148G      40   748 GB      1.8 TB         56g, fdr, 25ge    2(11) 0    0

  *** PBS resource requests are always lowercase ***

(0) CHIP has 3 resources:   chip_manufacturer, chip_model, chip_type
(1) Leave 2 or 3GB for the operating system when requesting memory in PBS jobs
(2) Specify queue &quot;bigmem&quot; to access the large memory machines, only ncpus and mem are valid PBS resource requests
(3) 2 NVIDIA Tesla M2075 cards per node, use resource request &quot;ngpus=[1|2]&quot; and &quot;gpu_model=m2075&quot;
(4) 2 NVIDIA Tesla K20m cards per node, use resource request &quot;ngpus=[1|2]&quot; and &quot;gpu_model=k20&quot;
(5) 2 NVIDIA Tesla M2070-Q cards per node, use resource request &quot;ngpus=[1|2]&quot; and &quot;gpu_model=m2070q&quot;
(6) 2 NVIDIA Tesla K40m cards per node, use resource request &quot;ngpus=[1|2]&quot; and &quot;gpu_model=k40&quot;
(7) Use resource request &quot;ssd=true&quot; to request a chunk with SSD in location /ssd1, /ssd2, and /ssd3 (100GB max each)
(8) Use resource request &quot;nphis=[1|2]&quot; to request phi nodes, the model is Xeon 7120p
(9) 2 NVIDIA Tesla P100 cards per node, use resource request &quot;ngpus=[1|2]&quot; and &quot;gpu_model=p100&quot;
(10)4 NVIDIA Tesla V100 cards per node with NVLINK2, use resource request &quot;ngpus=[1|2|3|4]&quot; and &quot;gpu_model=v100nv&quot;
(11)2 NVIDIA Tesla V100 cards per node, use resource request &quot;ngpus=[1|2]&quot; and &quot;gpu_model=v100&quot;
(12)Phase18a nodes contain only NVMe storage for local_scratch.
</code></pre>

<p>The compute nodes are divided into "phases" (currently phases 0-18).
Each phase is composed of several nodes with identical configuration,
e.g., each node in phase 5a has 8 cores, 32 GB ram, 31 GB local disk space,
and 10 Gbps Myrinet interconnect.</p>
<p>A useful command on the login node is <code>whatsfree</code>, which gives information
about how many nodes from each phase are currently in use, free, or offlined
for maintenance.</p>
<p>Later sections of this guide will describe how to submit <strong>jobs</strong> to
the cluster, i.e., reserve compute nodes for running computational tasks.</p>
<h3 id="checking-and-using-available-software">Checking and using available software</h3>
<p>The Palmetto cluster provides a limited number of packages
(including site-licensed packages),
that can be used by all Palmetto users.
These packages are available as <strong>modules</strong>,
and must be activated/deactivated using the <code>module</code> command:</p>
<table>
<thead>
<tr>
<th>Command</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>module avail</code></td>
<td>List all packages available (on current system)</td>
</tr>
<tr>
<td><code>module add package/version</code></td>
<td>Add a package to your current shell environment</td>
</tr>
<tr>
<td><code>module list</code></td>
<td>List packages you have loaded</td>
</tr>
<tr>
<td><code>module rm package/version</code></td>
<td>Remove a currently loaded package</td>
</tr>
<tr>
<td><code>module purge</code></td>
<td>Remove <em>all</em> currently loaded packages</td>
</tr>
</tbody>
</table>
<p>For example, to load the GCC (v4.8.1), CUDA Toolkit (v6.5.14)
and OpenMPI (v1.8.4) modules, you can use the command:</p>
<pre><code>$ module add gcc/4.8.1 cuda-toolkit/6.5.14 openmpi/1.8.4
</code></pre>

<p>Then, check the version of <code>gcc</code>:</p>
<pre><code>$ gcc --version
</code></pre>

<pre><code>gcc (GCC) 4.8.1
Copyright (C) 2013 Free Software Foundation, Inc.
This is free software; see the source for copying conditions.  There is NO
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
</code></pre>

<p>Some modules when loaded, implicitly load other modules as well.
If you use some modules to compile/install some software,
then you will probably have to load them
when running that software as well, otherwise you may see
errors about missing libraries/headers.
Modules do not remain loaded when you log out and log back in,
i.e., they are active only for the current session -
so you will need to load them for every session.</p>
<p>As an exercise,
examine the environment variables PATH, LIBRARY_PATH, etc.,
before and after loading some module:</p>
<pre><code>$ echo $PATH
$ module add python/2.7.13
$ echo $PATH
</code></pre>

<p>You can also look at the modulefiles in <code>/software/modulefiles</code>
to understand what happens when you add a module.</p>
<h3 id="start-an-interactive-job">Start an interactive job</h3>
<p>An interactive job can be started using the <code>qsub</code> command.
Here is an example of an interactive job:</p>
<pre><code>[username@login001 ~]$ qsub -I -l select=1:ncpus=2:mem=4gb,walltime=4:00:00
qsub (Warning): Interactive jobs will be treated as not rerunnable
qsub: waiting for job 8730.pbs02 to start
qsub: job 8730.pbs02 ready

[username@node0021 ~]$ module add python/3.4
[username@node0021 ~]$ python runsim.py
.
.
.
[username@node0021 ~]$ exit
[username@login001 ~]$
</code></pre>

<p>Above, we request an interactive job using 1 "chunk" of hardware (<code>select=1</code>),
2 CPU cores per "chunk", and 4gb of RAM per "chunk", for a wall time of 4 hours.
Once these resources are available, we receive a Job ID (<code>8730.pbs02</code>),
and a command-line session running on <code>node0021</code>.</p>
<h3 id="submit-a-batch-job">Submit a batch job</h3>
<p>Interactive jobs require you to be logged-in while your tasks are running.
In contrast, you may logout after submitting a batch job,
and examine the results at a later time. This is useful when you need to
run several computational tasks to the cluster, and/or when your
computational tasks are expected to run for a long time.</p>
<p>To submit a batch job, you must prepare a <strong>batch script</strong>
(you can do this using an editor like <code>vim</code> or <code>nano</code>).
Following is an example of a batch script (call it <code>example.pbs</code>).
In the batch job below, we really don't do anything useful
(just sleep or "do nothing" for 60 seconds),</p>
<pre><code>#PBS -N example
#PBS -l select=1:ncpus=1:mem=2gb,walltime=00:10:00

module add gcc/4.8.1

cd /home/username
echo Hello World from `hostname`
sleep 60
</code></pre>

<p>After saving the above file, you can submit the batch job using <code>qsub</code>:</p>
<pre><code>[username@login001 ~]$ qsub example.pbs
8738.pbs02
</code></pre>

<p>The returned job ID can be used to query the status of the job (using <code>qstat</code>)
(or delete it using <code>qdel</code>):</p>
<pre><code>[username@login001 ~]$ qstat 8738.pbs02
Job id            Name             User              Time Use S Queue
----------------  ---------------- ----------------  -------- - -----
8738.pbs02        example          username           00:00:00 R c1_solo
</code></pre>

<p>Once the job is completed, you will see the files <code>example.o8738</code> (containing output if any)
and <code>example.e8738</code> (containing errors if any) from your job.</p>
<pre><code>[username@login001 ~]$ cat example.o8738
Hello World from node0230.palmetto.clemson.edu
</code></pre>

<h2 id="job-submission-and-control-on-palmetto">Job submission and control on Palmetto</h2>
<p>The Palmetto cluster uses the Portable Batch Scheduling system (PBS)
to manage jobs. Here are some basic PBS commands
for submitting, querying and deleting jobs:</p>
<table>
<thead>
<tr>
<th>Command</th>
<th>Action</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>qsub -I</code></td>
<td>Submit an interactive job (reserves 1 core, 1gb RAM, 30 minutes walltime)</td>
</tr>
<tr>
<td><code>qsub xyz.pbs</code></td>
<td>Submit the job script <code>xyz.pbs</code></td>
</tr>
<tr>
<td><code>qstat &lt;job id&gt;</code></td>
<td>Check the status of the job with given job ID</td>
</tr>
<tr>
<td><code>qstat -u &lt;username&gt;</code></td>
<td>Check the status of all jobs submitted by given username</td>
</tr>
<tr>
<td><code>qstat -xf &lt;job id&gt;</code></td>
<td>Check detailed information for job with given job ID</td>
</tr>
<tr>
<td><code>qsub -q &lt;queuename&gt; xyz.pbs</code></td>
<td>Submit to queue <code>queuename</code></td>
</tr>
<tr>
<td><code>qdel &lt;job id&gt;</code></td>
<td>Delete the job (queued or running) with given job ID</td>
</tr>
<tr>
<td><code>qpeek &lt;job id&gt;</code></td>
<td>"Peek" at the standard output from a running job</td>
</tr>
<tr>
<td><code>qdel -Wforce &lt;job id&gt;</code></td>
<td>Use when job not responding to just <code>qdel</code></td>
</tr>
</tbody>
</table>
<p>For more details and more advanced commands for submitting and controlling jobs,
please refer to the <a href="http://www.pbsworks.com/pdfs/PBSUserGuide14.2.pdf">PBS Professional User's Guide</a>.</p>
<h3 id="pbs-job-options">PBS job options</h3>
<p>The following switches can be used either
with <code>qsub</code> on the command line,
or with a <code>#PBS</code> directive in a batch script.</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Purpose</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>-N</code></td>
<td>Job name (7 characters)</td>
<td><code>-N maxrun1</code></td>
</tr>
<tr>
<td><code>-l</code></td>
<td>Job limits (lowercase L), hardware &amp; other requirements for job.</td>
<td><code>-l select=1:ncpus=8:mem=1gb</code></td>
</tr>
<tr>
<td><code>-q</code></td>
<td>Queue to direct this job to (<code>workq</code> is the default, <code>supabad</code> is an example of specific research group's job queue)</td>
<td><code>-q supabad</code></td>
</tr>
<tr>
<td><code>-o</code></td>
<td>Path to stdout file for this job (environment variables are not accepted here)</td>
<td><code>-o stdout.txt</code></td>
</tr>
<tr>
<td><code>-e</code></td>
<td>Path to stderr file for this job (environment variables are not accepted here)</td>
<td><code>-e stderr.txt</code></td>
</tr>
<tr>
<td><code>-m</code></td>
<td>mail event: Email from the PBS server with flag <strong>a</strong>bort\ <strong>b</strong>egin\ <strong>e</strong>nd \ or <strong>n</strong>o mail for job's notification.</td>
<td><code>-m abe</code></td>
</tr>
<tr>
<td><code>-M</code></td>
<td>Specify list of user to whom mail about the job is sent. The user list argument is of the form: [user[@host],user[@host],...]. If <strong>-M</strong> is not used and <strong>-m</strong> is specified, PBS will send email to userid@clemson.edu</td>
<td><code>-M user1@domain1.com,user2@domain2.com</code></td>
</tr>
<tr>
<td><code>-j oe</code></td>
<td>Join the output and error streams and write to a single file</td>
<td><code>-j oe</code></td>
</tr>
<tr>
<td><code>-r n</code></td>
<td>Ask PBS <strong>not</strong> to restart the job if it's failed</td>
<td><code>-r n</code></td>
</tr>
</tbody>
</table>
<p>For example, in a batch script:</p>
<pre><code>#PBS -N hydrogen
#PBS -l select=1:ncpus=24:mem=200gb,walltime=4:00:00
#PBS -q bigmem
#PBS -m abe
#PBS -M userid@domain.com
#PBS -j oe
</code></pre>

<p>And in an interactive job request on the command line:</p>
<pre><code>$ qsub -I -N hydrogen -q bigmem -j oe -l select=1:ncpus=24:mem=200gb,walltime=4:00:00
</code></pre>

<p>For more detailed information, please take a look at:</p>
<pre><code>$man qsub
</code></pre>

<h3 id="resource-limits-specification">Resource limits specification</h3>
<p>The <code>-l</code> switch provided to <code>qsub</code> or along with the <code>#PBS</code> directive
can be used to specify the amount and kind of compute hardware (cores, memory, GPUs, interconnect, etc.,),
its location, i.e., the node(s) and phase from which to request hardware,</p>
<table>
<thead>
<tr>
<th>Option</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>select</code></td>
<td>Number of chunks and resources per chunk. Two or more "chunks" can be placed on a single node, but a single "chunk" cannot span more than one node.</td>
</tr>
<tr>
<td><code>walltime</code></td>
<td>Expected wall time of job (job is terminated after this time)</td>
</tr>
<tr>
<td><code>place</code></td>
<td>Controls the placement of the different chunks</td>
</tr>
</tbody>
</table>
<p>Here are some examples of resource limits specification:</p>
<pre><code>-l select=1:ncpus=8:chip_model=opteron:interconnect=10g
-l select=1:ncpus=16:chip_type=e5-2665:interconnect=56g:mem=62gb,walltime=16:00:00
-l select=1:ncpus=8:chip_type=2356:interconnect=10g:mem=15gb
-l select=1:ncpus=1:node_manufacturer=ibm:mem=15gb,walltime=00:20:00
-l select=1:ncpus=4:mem=15gb:ngpus=2,walltime=00:20:00
-l select=1:ncpus=4:mem=15gb:ngpus=1:gpu_model=k40,walltime=00:20:00
-l select=1:ncpus=2:mem=15gb:host=node1479,walltime=00:20:00
-l select=2:ncpus=2:mem=15gb,walltime=00:20:00,place=scatter    # force each chunk to be on a different node
-l select=2:ncpus=2:mem=15gb,walltime=00:20:00,place=pack       # force each chunk to be on the same node
</code></pre>

<p>and examples of options you can use in the job limit specification:</p>
<pre><code>chip_manufacturer=amd
chip_manufacturer=intel
chip_model=opteron
chip_model=xeon
chip_type=e5345
chip_type=e5410
chip_type=l5420
chip_type=x7542
chip_type=2356
chip_type=6172
chip_type=e5-2665
node_manufacturer=dell
node_manufacturer=hp
node_manufacturer=ibm
node_manufacturer=sun
gpu_model=k20
gpu_model=k40
interconnect=1g     (1 Gbps Ethernet)
interconnect=10ge   (10 Gbps Ethernet)
interconnect=56g    (56 Gbps FDR InfiniBand, same as fdr)
interconnect=fdr    (56 Gbps FDR InfiniBand, same as 56g)
ssd=true            (Use a node with an SSD hard drive)
</code></pre>

<h3 id="querying-job-information-and-deleting-jobs">Querying job information and deleting jobs</h3>
<p>The <code>qstat</code> command can be used
to query the status of a particular job:</p>
<pre><code>$ qstat 7600424.pbs02
Job id            Name             User              Time Use S Queue
----------------  ---------------- ----------------  -------- - -----
7600424.pbs02     pi-mpi           username           00:00:00 R c1_single
</code></pre>

<p>To list the job IDs and status of all your jobs,
you can use the <code>-u</code> switch:</p>
<pre><code>$ qstat -u username

pbs02:
                                                            Req'd  Req'd   Elap
Job ID          Username Queue    Jobname    SessID NDS TSK Memory Time  S Time
--------------- -------- -------- ---------- ------ --- --- ------ ----- - -----
7600567.pbs02   username  c1_singl pi-mpi-1     1382   4   8    4gb 00:05 R 00:00
7600569.pbs02   username  c1_singl pi-mpi-2    20258   4   8    4gb 00:05 R 00:00
7600570.pbs02   username  c1_singl pi-mpi-3     2457   4   8    4gb 00:05 R 00:00
</code></pre>

<p>Once a job has finished running,
<code>qstat -xf</code> can be used to obtain detailed job information:</p>
<pre><code>$ qstat -xf 7600424.pbs02

Job Id: 7600424.pbs02
    Job_Name = pi-mpi
    Job_Owner = username@login001.palmetto.clemson.edu
    resources_used.cpupercent = 103
    resources_used.cput = 00:00:04
    resources_used.mem = 45460kb
    resources_used.ncpus = 8
    resources_used.vmem = 785708kb
    resources_used.walltime = 00:02:08
    job_state = F
    queue = c1_single
    server = pbs02
    Checkpoint = u
    ctime = Tue Dec 13 14:09:32 2016
    Error_Path = login001.palmetto.clemson.edu:/home/username/MPI/pi-mpi.e7600424

    exec_host = node0088/1*2+node0094/1*2+node0094/2*2+node0085/0*2
    exec_vnode = (node0088:ncpus=2:mem=1048576kb:ngpus=0:nphis=0)+(node0094:ncp
    us=2:mem=1048576kb:ngpus=0:nphis=0)+(node0094:ncpus=2:mem=1048576kb:ngp
    us=0:nphis=0)+(node0085:ncpus=2:mem=1048576kb:ngpus=0:nphis=0)
    Hold_Types = n
    Join_Path = oe
    Keep_Files = n
    Mail_Points = a
    Mail_Users = username@clemson.edu
    mtime = Tue Dec 13 14:11:42 2016
    Output_Path = login001.palmetto.clemson.edu:/home/username/MPI/pi-mpi.o760042
    4
    Priority = 0
    qtime = Tue Dec 13 14:09:32 2016
    Rerunable = True
    Resource_List.mem = 4gb
    Resource_List.mpiprocs = 8
    Resource_List.ncpus = 8
    Resource_List.ngpus = 0
    Resource_List.nodect = 4
    Resource_List.nphis = 0
    Resource_List.place = free:shared
    Resource_List.qcat = c1_workq_qcat
    Resource_List.select = 4:ncpus=2:mem=1gb:interconnect=1g:mpiprocs=2
    Resource_List.walltime = 00:05:00
    stime = Tue Dec 13 14:09:33 2016
    session_id = 2708
    jobdir = /home/username
    substate = 92
    Variable_List = PBS_O_SYSTEM=Linux,PBS_O_SHELL=/bin/bash,
    PBS_O_HOME=/home/username,PBS_O_LOGNAME=username,
    PBS_O_WORKDIR=/home/username/MPI,PBS_O_LANG=en_US.UTF-8,
    PBS_O_PATH=/software/examples/:/home/username/local/bin:/usr/lib64/qt-3
    .3/bin:/opt/pbs/default/bin:/opt/gold/bin:/usr/local/bin:/bin:/usr/bin:
    /usr/local/sbin:/usr/sbin:/sbin:/opt/mx/bin:/home/username/bin,
    PBS_O_MAIL=/var/spool/mail/username,PBS_O_QUEUE=c1_workq,
    PBS_O_HOST=login001.palmetto.clemson.edu
    comment = Job run at Tue Dec 13 at 14:09 on (node0088:ncpus=2:mem=1048576kb
    :ngpus=0:nphis=0)+(node0094:ncpus=2:mem=1048576kb:ngpus=0:nphis=0)+(nod
    e0094:ncpus=2:mem=1048576kb:ngpus=0:nphis=0)+(node0085:ncpus=2:mem=1048
    576kb:ngpus=0:nphis=0) and finished
    etime = Tue Dec 13 14:09:32 2016
    run_count = 1
    Stageout_status = 1
    Exit_status = 0
    Submit_arguments = job.sh
    history_timestamp = 1481656302
    project = _pbs_project_default
</code></pre>

<p>Similarly, to get detailed information about
a running job, you can use <code>qstat -f</code>.</p>
<p>To delete a job (whether in queued, running or error status),
you can use the <code>qdel</code> command.</p>
<pre><code>$ qdel 7600424.pbs02
</code></pre>

<h3 id="job-limits-on-palmetto">Job limits on Palmetto</h3>
<h4 id="walltime">Walltime</h4>
<p>Jobs running in phases 1-6 of the cluster (nodes with interconnect <code>1g</code>)
can run for a maximum walltime of 168 hours (7 days).</p>
<p>Job running in phases 7 and higher of the cluster
can run for a maximum walltime of 72 hours (3 days).</p>
<h4 id="number-of-jobs">Number of jobs</h4>
<p>When you submit a job,
it is forwarded to a specific <strong>execution queue</strong>
based on job critera
(how many cores, RAM, etc.).
There are three classes of execution queues:</p>
<ol>
<li>
<p>MX queues (<code>c1_</code> queues): jobs submitted to run on the older hardware (phases 1-6)
will be forwarded to theses queues.</p>
</li>
<li>
<p>IB queues (<code>c2_</code> queues): jobs submitted to run the newer hardware (phases 7 and up)
will be forwarded to these queues.</p>
</li>
<li>
<p>GPU queues (<code>gpu_</code> queues): jobs that
<a href="{{site.baseurl}}/userguide_howto_use_gpus.html">request GPUs</a>
will be forwarded to these queues.</p>
</li>
<li>
<p>bigmem queue: jobs submitted to the large-memory machines (phase 0).</p>
</li>
</ol>
<p>Each execution queue has its own limits for
how many jobs can be running at one time,
and how many jobs can 
be waiting in that execution queue.
The maximum number of running jobs per user in
execution queues may vary
throughout the day depending on cluster load.
Users can see what the current limits
are using the <code>checkqueuecfg</code> command:</p>
<pre><code>$ checkqueuecfg

MX QUEUES     min_cores_per_job  max_cores_per_job   max_mem_per_queue  max_jobs_per_queue   max_walltime
c1_solo                       1                  1              4000gb                2000      168:00:00
c1_single                     2                 24             90000gb                 750      168:00:00
c1_tiny                      25                128             25600gb                  25      168:00:00
c1_small                    129                512             24576gb                   6      168:00:00
c1_medium                   513               2048             81920gb                   5      168:00:00
c1_large                   2049               4096             32768gb                   1      168:00:00

IB QUEUES     min_cores_per_job  max_cores_per_job   max_mem_per_queue  max_jobs_per_queue   max_walltime
c2_single                     1                 24               600gb                   5       72:00:00
c2_tiny                      25                128              4096gb                   2       72:00:00
c2_small                    129                512              6144gb                   1       72:00:00
c2_medium                   513               2048             16384gb                   1       72:00:00
c2_large                   2049               4096                 0gb                   0       72:00:00

GPU QUEUES     min_gpus_per_job   max_gpus_per_job  min_cores_per_job  max_cores_per_job   max_mem_per_queue  max_jobs_per_queue   max_walltime
gpu_small                     1                  4                  1                 96              3840gb                  20       72:00:00
gpu_medium                    5                 16                  1                256              5120gb                   5       72:00:00
gpu_large                    17                128                  1               1024             20480gb                   5       72:00:00

SMP QUEUE     min_cores  max_cores   max_jobs   max_walltime
bigmem                1         64          3       72:00:00


   'max_mem' is the maximum amount of memory all your jobs in this queue can
   consume at any one time.  For example, if the max_mem for the solo queue
   is 4000gb, and your solo jobs each need 10gb, then you can run a
   maximum number of 4000/10 = 400 jobs in the solo queue, even though the
   current max_jobs setting for the solo queue may be set higher than 400.
</code></pre>

<p>The <code>qstat</code> command tells you which of the execution queues
your job is forwarded to. For example, here is an interactive
job requesting 8 CPU cores, a K40 GPU, and 32gb RAM:</p>
<pre><code>$ qsub -I -l select=1:ncpus=8:ngpus=1:gpu_model=k40:mem=32gb,walltime=2:00:00
qsub (Warning): Interactive jobs will be treated as not rerunnable
qsub: waiting for job 9567792.pbs02 to start
</code></pre>

<p>We see from <code>qstat</code> that the job request is forward to
the <code>c2_single</code> queue:</p>
<pre><code>[username@login001 ~]$ qstat 9567792.pbs02
Job id            Name             User              Time Use S Queue
----------------  ---------------- ----------------  -------- - -----
9567792.pbs02     STDIN            username                  0 Q c2_single
</code></pre>

<p>From the output of <code>checkqueuecfg</code> above,
we see that each user can have a maximum of <strong>5</strong> running jobs in this queue.</p>
<h3 id="example-pbs-scripts">Example PBS scripts</h3>
<p>A list of example PBS scripts for submitting
jobs to the Palmetto cluster can be found
<a href="https://github.com/clemsonciti/palmetto-examples">here</a>.</p>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../jupyter/" class="btn btn-neutral float-right" title="JupyterLab">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../../owner/" class="btn btn-neutral" title="Owner"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../../owner/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../jupyter/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../search/main.js" defer></script>
    <script defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(true);
        };
    </script>

</body>
</html>
